<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>How and Why Does Dropout Work? &#8212; Data Science Blog</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="icon" href="_static/starburst (200 x 200 px).png"/>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="how-and-why-does-dropout-work">
<h1>How and Why Does Dropout Work?<a class="headerlink" href="#how-and-why-does-dropout-work" title="Link to this heading">¶</a></h1>
<section id="what-is-dropout">
<h2>What is Dropout?<a class="headerlink" href="#what-is-dropout" title="Link to this heading">¶</a></h2>
<p>The idea behind dropout is quite simple - during the training phase, a randomly selected subset of neurons is “dropped out” of the computation. This is achieved by setting their activations to 0. A fixed proportion <span class="math notranslate nohighlight">\(p\)</span>, usually between 10-50%, are “dropped out” during each training step. This value is often different for each layer.</p>
<p>During the testing phase, dropout is not applied, but instead, the outputs of each layer are scaled down by a factor of <span class="math notranslate nohighlight">\(1-p\)</span>.</p>
<p>Empirically, this results in improved generalization performance (i.e. on the validation or test set).</p>
<p>In this experiment, we will compare two deep neural networks, one without dropout and one with. Then, we will try to understand why dropout results in better performance.</p>
</section>
<section id="let-s-start-by-loading-the-cifar-10-dataset">
<h2>Let’s Start by Loading the CIFAR-10 Dataset<a class="headerlink" href="#let-s-start-by-loading-the-cifar-10-dataset" title="Link to this heading">¶</a></h2>
<p>The CIFAR-10 dataset is a popular benchmark for machine learning and computer vision tasks, consisting of 60,000 32x32 color images across 10 distinct classes. Each image is labeled as one of the following categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, or truck. The dataset is divided into 50,000 training images and 10,000 test images, with an even distribution of 6,000 images per class.</p>
<p>Despite its simplicity, the dataset contains a variety of object poses, lighting conditions, and backgrounds, making it non-trivial to achieve high accuracy.</p>
<p>Convolutional Neural Networks and other image-specific architectures are typically trained on this dataset. However, for this experiment, we will use a feedforward network in order to demonstrate the benefits of using dropout.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="n">train_tf</span><span class="p">,</span> <span class="n">test_tf</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;cifar10&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">raw_train_images</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">train_tf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">raw_train_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">raw_train_images</span><span class="p">)</span>
<span class="n">raw_train_images</span> <span class="o">=</span> <span class="n">raw_train_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">raw_train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>

<span class="n">raw_test_images</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">test_tf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">test_tf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">raw_test_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">raw_test_images</span><span class="p">)</span>
<span class="n">raw_test_images</span> <span class="o">=</span> <span class="n">raw_test_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">raw_test_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">test_labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training Set Size </span><span class="si">{</span><span class="n">raw_train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test Set Size </span><span class="si">{</span><span class="n">raw_test_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ERROR:absl:Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc &gt;= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Traceback (most recent call last):
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/__init__.py&quot;, line 79, in &lt;module&gt;
    from tensorflow_datasets import rlds  # pylint: disable=g-bad-import-order
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/rlds/__init__.py&quot;, line 21, in &lt;module&gt;
    from tensorflow_datasets.rlds import envlogger_reader
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/rlds/envlogger_reader.py&quot;, line 21, in &lt;module&gt;
    from tensorflow_datasets.core.utils.lazy_imports_utils import tree
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/__init__.py&quot;, line 22, in &lt;module&gt;
    from tensorflow_datasets.core import community
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/community/__init__.py&quot;, line 19, in &lt;module&gt;
    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/community/huggingface_wrapper.py&quot;, line 31, in &lt;module&gt;
    from tensorflow_datasets.core import dataset_builder
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py&quot;, line 35, in &lt;module&gt;
    from tensorflow_datasets.core import dataset_info
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_info.py&quot;, line 52, in &lt;module&gt;
    from tensorflow_datasets.core import splits as splits_lib
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/splits.py&quot;, line 34, in &lt;module&gt;
    from tensorflow_datasets.core import proto as proto_lib
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/proto/__init__.py&quot;, line 18, in &lt;module&gt;
    from tensorflow_datasets.core.proto import dataset_info_generated_pb2 as dataset_info_pb2  # pylint: disable=line-too-long
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/proto/dataset_info_generated_pb2.py&quot;, line 32, in &lt;module&gt;
    from tensorflow_metadata.proto.v0 import schema_pb2 as tensorflow__metadata_dot_proto_dot_v0_dot_schema__pb2
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_metadata/proto/v0/schema_pb2.py&quot;, line 32, in &lt;module&gt;
    _descriptor.EnumValueDescriptor(
  File &quot;/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/google/protobuf/descriptor.py&quot;, line 789, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc &gt;= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">jax</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="n">train_tf</span><span class="p">,</span> <span class="n">test_tf</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;cifar10&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">raw_train_images</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_tf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">train_tf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">raw_train_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">raw_train_images</span><span class="p">)</span>

<span class="ne">AttributeError</span>: module &#39;tensorflow_datasets&#39; has no attribute &#39;load&#39;
</pre></div>
</div>
</div>
</div>
<section id="what-does-our-dataset-look-like">
<h3>What Does Our Dataset Look Like?<a class="headerlink" href="#what-does-our-dataset-look-like" title="Link to this heading">¶</a></h3>
<p>Let’s look at a few training examples.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-darkgrid&#39;</span><span class="p">)</span>

<span class="n">RANDOM_KEY</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">num_examples</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">row_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">num_examples</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">row_size</span><span class="p">,</span> <span class="n">row_size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">row_size</span><span class="p">,</span><span class="n">row_size</span><span class="p">))</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">RANDOM_KEY</span><span class="p">)</span>
<span class="n">idxs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="p">(</span><span class="n">num_examples</span><span class="p">,),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">raw_train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">exs</span> <span class="o">=</span> <span class="n">raw_train_images</span><span class="p">[</span><span class="n">idxs</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_examples</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">/</span><span class="mf">255.0</span>

<span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">ex</span> <span class="o">//</span> <span class="n">row_size</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">ex</span> <span class="o">%</span> <span class="n">row_size</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">exs</span><span class="p">[</span><span class="n">ex</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xmargin</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ymargin</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">13</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">row_size</span><span class="p">,</span> <span class="n">row_size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">row_size</span><span class="p">,</span><span class="n">row_size</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">RANDOM_KEY</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">13</span> <span class="n">idxs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="p">(</span><span class="n">num_examples</span><span class="p">,),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">raw_train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="n">exs</span> <span class="o">=</span> <span class="n">raw_train_images</span><span class="p">[</span><span class="n">idxs</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_examples</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">/</span><span class="mf">255.0</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>

<span class="ne">NameError</span>: name &#39;raw_train_images&#39; is not defined
</pre></div>
</div>
<img alt="_images/a18c9921a5eb511227e66433155cdd72433294ab5618ee2414a4fac12272d0a4.png" src="_images/a18c9921a5eb511227e66433155cdd72433294ab5618ee2414a4fac12272d0a4.png" />
</div>
</div>
<p>Let’s now preprocess the dataset and look at the input feature distrubition.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, let&#39;s plot the histogram for the unnormalized dataset</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">raw_train_images</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Unnormalized Input Feature Distribution&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature Values&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Counts&quot;</span><span class="p">)</span>

<span class="c1"># Let&#39;s normalize the  dataset</span>
<span class="n">train_mean</span> <span class="o">=</span> <span class="n">raw_train_images</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">train_std</span> <span class="o">=</span> <span class="n">raw_train_images</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">raw_train_images</span> <span class="o">-</span> <span class="n">train_mean</span><span class="p">)</span><span class="o">/</span><span class="n">train_std</span>
<span class="n">train_max</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">/</span><span class="n">train_max</span>

<span class="n">test_mean</span> <span class="o">=</span> <span class="n">raw_test_images</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">test_std</span> <span class="o">=</span> <span class="n">raw_test_images</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">raw_test_images</span> <span class="o">-</span> <span class="n">test_mean</span><span class="p">)</span><span class="o">/</span><span class="n">test_std</span>
<span class="n">test_max</span> <span class="o">=</span> <span class="n">test_images</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span><span class="o">/</span><span class="n">test_max</span>

<span class="c1"># Now let&#39;s plot the the histogram for the normalized dataset</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_images</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Normalized Input Feature Distribution&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature Values&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Counts&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># First, let&#39;s plot the histogram for the unnormalized dataset</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">raw_train_images</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Unnormalized Input Feature Distribution&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature Values&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;raw_train_images&#39; is not defined
</pre></div>
</div>
<img alt="_images/c029a7249cadaf1bc81b9350245cc1f132b422306993c9256dd38e4879125fad.png" src="_images/c029a7249cadaf1bc81b9350245cc1f132b422306993c9256dd38e4879125fad.png" />
</div>
</div>
</section>
</section>
<section id="let-s-define-the-models">
<h2>Let’s Define the Models<a class="headerlink" href="#let-s-define-the-models" title="Link to this heading">¶</a></h2>
<p>Our baseline model is a fully-connected network with ReLU activations. The output layer has 10 classes to generate logits for our class probabilities.
The candidate is just the baseline with an additional dropout layer after each ReLU activation (except the final layer).</p>
<p>The dropout layer is placed after the ReLU activation because we want to only deactivate those neurons that actively participate in classifying an example. Deactivating neurons with zero or negative values will not serve this purpose. In fact, it may lead to ‘dead neurons’ which never activate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">import</span> <span class="nn">operator</span>

<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>

<span class="kn">from</span> <span class="nn">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">flax.training</span> <span class="kn">import</span> <span class="n">train_state</span>
<span class="kn">import</span> <span class="nn">optax</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">RANDOM_KEY</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>             
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>             
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>              
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>              
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>              
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_4&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">outputs</span>

<span class="c1"># Initialize the candidate model</span>
<span class="n">baseline_model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">))</span>
<span class="n">baseline_variables</span> <span class="o">=</span> <span class="n">baseline_model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">baseline_params</span> <span class="o">=</span> <span class="n">baseline_variables</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>
<span class="n">baseline_batch_stats</span> <span class="o">=</span> <span class="n">baseline_variables</span><span class="p">[</span><span class="s1">&#39;batch_stats&#39;</span><span class="p">]</span>
<span class="n">dropout</span><span class="o">=</span><span class="mf">0.4</span>

<span class="k">class</span> <span class="nc">MLPDropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">rng</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>             
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dropout_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>             
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dropout_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>              
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dropout_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>              
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;BatchNorm_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Relu_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dropout_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>              
        <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;Dense_4&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">outputs</span>

<span class="c1"># Initialize the candidate model</span>
<span class="n">candidate_model</span> <span class="o">=</span> <span class="n">MLPDropout</span><span class="p">()</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">))</span>
<span class="n">candidate_variables</span> <span class="o">=</span> <span class="n">candidate_model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="n">candidate_params</span> <span class="o">=</span> <span class="n">candidate_variables</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>
<span class="n">candidate_batch_stats</span> <span class="o">=</span> <span class="n">candidate_variables</span><span class="p">[</span><span class="s1">&#39;batch_stats&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<section id="let-s-count-the-parameters-and-plot-the-initial-distributions-for-the-parameter-values">
<h3>Let’s count the parameters and plot the initial distributions for the parameter values.<a class="headerlink" href="#let-s-count-the-parameters-and-plot-the-initial-distributions-for-the-parameter-values" title="Link to this heading">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Count the # of model parameters</span>
<span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">num_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">num_params</span> <span class="o">+=</span> <span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">num_params</span>

<span class="n">num_baseline_params</span> <span class="o">=</span> <span class="n">count_params</span><span class="p">(</span><span class="n">baseline_params</span><span class="p">)</span>
<span class="n">num_candidate_params</span> <span class="o">=</span> <span class="n">count_params</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of baseline parameters: </span><span class="si">{</span><span class="n">num_baseline_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Baseline P/S ratio: </span><span class="si">{</span><span class="n">num_baseline_params</span><span class="o">/</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of candidate parameters: </span><span class="si">{</span><span class="n">num_candidate_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Candidate P/S ratio: </span><span class="si">{</span><span class="n">num_candidate_params</span><span class="o">/</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Lets plot the initial baseline model weight distributions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">baseline_dense0_w</span> <span class="o">=</span> <span class="n">baseline_params</span><span class="p">[</span><span class="s1">&#39;Dense_0&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">candidate_dense0_w</span> <span class="o">=</span> <span class="n">candidate_params</span><span class="p">[</span><span class="s1">&#39;Dense_0&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">baseline_dense1_w</span> <span class="o">=</span> <span class="n">baseline_params</span><span class="p">[</span><span class="s1">&#39;Dense_1&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">candidate_dense1_w</span> <span class="o">=</span> <span class="n">candidate_params</span><span class="p">[</span><span class="s1">&#39;Dense_1&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">baseline_dense2_w</span> <span class="o">=</span> <span class="n">baseline_params</span><span class="p">[</span><span class="s1">&#39;Dense_2&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">candidate_dense2_w</span> <span class="o">=</span> <span class="n">candidate_params</span><span class="p">[</span><span class="s1">&#39;Dense_2&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">baseline_dense3_w</span> <span class="o">=</span> <span class="n">baseline_params</span><span class="p">[</span><span class="s1">&#39;Dense_3&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">candidate_dense3_w</span> <span class="o">=</span> <span class="n">candidate_params</span><span class="p">[</span><span class="s1">&#39;Dense_3&#39;</span><span class="p">][</span><span class="s1">&#39;kernel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">baseline_dense0_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dense_0 Weights&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">baseline_dense1_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dense_1 Weights&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">baseline_dense2_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dense_2 Weights&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">baseline_dense3_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dense_3 Weights&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">candidate_dense0_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">candidate_dense1_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">candidate_dense2_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">candidate_dense3_w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>



<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of baseline parameters: 831210
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">14</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="n">num_candidate_params</span> <span class="o">=</span> <span class="n">count_params</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of baseline parameters: </span><span class="si">{</span><span class="n">num_baseline_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">14</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Baseline P/S ratio: </span><span class="si">{</span><span class="n">num_baseline_params</span><span class="o">/</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of candidate parameters: </span><span class="si">{</span><span class="n">num_candidate_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Candidate P/S ratio: </span><span class="si">{</span><span class="n">num_candidate_params</span><span class="o">/</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;train_images&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>As you can see, both models have the same number of parameters as well as initial distributions. Hence, we can safely assume that any performance improvement seen by the candidate model will be due to dropout.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">try</span><span class="p">:</span>
    <span class="nb">type</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">base_temp_dir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">gettempdir</span><span class="p">()</span>
    <span class="n">tmpdir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_temp_dir</span><span class="p">,</span> <span class="s2">&quot;model_outputs&quot;</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Save outputs to disk using pickle</span>
<span class="k">def</span> <span class="nf">save_outputs_to_disk</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">tmpdir</span><span class="o">=</span><span class="n">tmpdir</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">tmpdir</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">phase</span><span class="si">}</span><span class="s1">_outputs_epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">.npy&#39;</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>


<span class="c1"># Load outputs from disk using pickle</span>
<span class="k">def</span> <span class="nf">load_outputs_from_disk</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">tmpdir</span><span class="o">=</span><span class="n">tmpdir</span><span class="p">):</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">tmpdir</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">phase</span><span class="si">}</span><span class="s1">_outputs_epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s1">.npy&#39;</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No data found for file </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="nb">type</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">tmpdir</span> <span class="o">=</span> <span class="s2">&quot;/var/folders/x4/85_9sn3d1ng9q48ff6t3fw340000gn/T/model_outputs&quot;</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
<section id="let-s-train-the-baseline-model">
<h2>Let’s Train the Baseline Model<a class="headerlink" href="#let-s-train-the-baseline-model" title="Link to this heading">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">optax</span>
<span class="kn">from</span> <span class="nn">flax.training</span> <span class="kn">import</span> <span class="n">train_state</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">RANDOM_KEY</span><span class="p">)</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a train state</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
<span class="n">baseline_ts</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">baseline_model</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">baseline_params</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">apply_fn</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">params</span><span class="p">,</span> <span class="s1">&#39;batch_stats&#39;</span><span class="p">:</span> <span class="n">batch_stats</span><span class="p">}</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">updated_variables</span> <span class="o">=</span> <span class="n">apply_fn</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">mutable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;batch_stats&#39;</span><span class="p">])</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="n">one_hot_labels</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">one_hot_labels</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">updated_variables</span><span class="p">[</span><span class="s1">&#39;batch_stats&#39;</span><span class="p">]</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">updated_batch_stats</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">updated_batch_stats</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">eval_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="s1">&#39;batch_stats&#39;</span><span class="p">:</span> <span class="n">batch_stats</span><span class="p">}</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">outputs</span>


<span class="k">def</span> <span class="nf">train_and_evaluate</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">):</span>

    <span class="n">num_train</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">train_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
    <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">train_outputs</span><span class="p">)</span>
    <span class="n">train_outputs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">test_outputs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
    <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">test_outputs</span><span class="p">)</span>
    <span class="n">train_accuracy_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_accuracy</span><span class="p">]</span>
    <span class="n">test_accuracy_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_accuracy</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch 0, Train Accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">, Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">sub_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">permutation</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">sub_rng</span><span class="p">,</span> <span class="n">num_train</span><span class="p">)</span>
        <span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
        <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">grads</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>

        <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">train_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
        <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_outputs</span><span class="p">)</span>
        <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
        <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_outputs</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">, Train Accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">, Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_accuracy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="p">)</span>
        <span class="n">test_accuracy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span>

        <span class="n">train_outputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">test_outputs</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">train_accuracy_history</span><span class="p">,</span> <span class="n">test_accuracy_history</span>

<span class="n">baseline_train_accuracy_history</span><span class="p">,</span> <span class="n">baseline_test_accuracy_history</span> <span class="o">=</span> <span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">baseline_model</span><span class="p">,</span> <span class="n">baseline_ts</span><span class="p">,</span> <span class="n">baseline_batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">, Train Accuracy: </span><span class="si">{</span><span class="n">baseline_train_accuracy_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">, Test Accuracy: </span><span class="si">{</span><span class="n">baseline_test_accuracy_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Train Accuracy: 0.1044, Test Accuracy: 0.1060
Epoch 1, Train Accuracy: 0.4701, Test Accuracy: 0.4562
Epoch 51, Train Accuracy: 0.9221, Test Accuracy: 0.5429
Epoch 101, Train Accuracy: 0.9732, Test Accuracy: 0.5283
Epoch 151, Train Accuracy: 0.9874, Test Accuracy: 0.5322
Epoch 200, Train Accuracy: 0.9939, Test Accuracy: 0.5356
</pre></div>
</div>
</div>
</div>
</section>
<section id="next-let-s-train-the-candidate-model">
<h2>Next, Let’s Train the Candidate Model<a class="headerlink" href="#next-let-s-train-the-candidate-model" title="Link to this heading">¶</a></h2>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a train state</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
<span class="n">candidate_ts</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">candidate_model</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">candidate_params</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">apply_fn</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">params</span><span class="p">,</span> <span class="s1">&#39;batch_stats&#39;</span><span class="p">:</span> <span class="n">batch_stats</span><span class="p">}</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">updated_variables</span> <span class="o">=</span> <span class="n">apply_fn</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>  <span class="n">mutable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;batch_stats&#39;</span><span class="p">])</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="n">one_hot_labels</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">one_hot_labels</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">updated_variables</span><span class="p">[</span><span class="s1">&#39;batch_stats&#39;</span><span class="p">]</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">updated_batch_stats</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">updated_batch_stats</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">eval_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="s1">&#39;batch_stats&#39;</span><span class="p">:</span> <span class="n">batch_stats</span><span class="p">}</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">outputs</span>


<span class="k">def</span> <span class="nf">train_and_evaluate</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">):</span>

    <span class="n">num_train</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">train_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">train_outputs</span><span class="p">)</span>
    <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">test_outputs</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch 0, Train Accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">, Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">train_accuracy_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_accuracy</span><span class="p">]</span>
    <span class="n">test_accuracy_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_accuracy</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>

        <span class="n">rng</span><span class="p">,</span> <span class="n">sub_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">permutation</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">sub_rng</span><span class="p">,</span> <span class="n">num_train</span><span class="p">)</span>
        <span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
        <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">grads</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">,</span> <span class="n">sub_rng</span><span class="p">)</span>

        <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">train_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">sub_rng</span><span class="p">)</span>
        <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_outputs</span><span class="p">)</span>
        <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">eval_step</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">batch_stats</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">sub_rng</span><span class="p">)</span>
        <span class="n">save_outputs_to_disk</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_outputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">, Train Accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">, Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_accuracy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="p">)</span>
        <span class="n">test_accuracy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_accuracy_history</span><span class="p">,</span> <span class="n">test_accuracy_history</span>

<span class="n">candidate_train_accuracy_history</span><span class="p">,</span> <span class="n">candidate_test_accuracy_history</span> <span class="o">=</span> <span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">candidate_model</span><span class="p">,</span> <span class="n">candidate_ts</span><span class="p">,</span> <span class="n">candidate_batch_stats</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">, Train Accuracy: </span><span class="si">{</span><span class="n">candidate_train_accuracy_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">, Test Accuracy: </span><span class="si">{</span><span class="n">candidate_test_accuracy_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Train Accuracy: 0.1044, Test Accuracy: 0.1060
Epoch 1, Train Accuracy: 0.4273, Test Accuracy: 0.4257
Epoch 51, Train Accuracy: 0.6964, Test Accuracy: 0.5599
Epoch 101, Train Accuracy: 0.7741, Test Accuracy: 0.5612
Epoch 151, Train Accuracy: 0.8042, Test Accuracy: 0.5570
Epoch 200, Train Accuracy: 0.8414, Test Accuracy: 0.5571
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the accuracy metrics over time.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">baseline_train_accuracy_history</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">candidate_train_accuracy_history</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Train Accuracy&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">baseline_test_accuracy_history</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">candidate_test_accuracy_history</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Test Accuracy&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">9</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">baseline_train_accuracy_history</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">candidate_train_accuracy_history</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Train Accuracy&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;baseline_train_accuracy_history&#39; is not defined
</pre></div>
</div>
<img alt="_images/c5f9ba5a3e7bb7ae97bdd937bf5eee8903b80aced4f1a550e9e372f83bbcbb40.png" src="_images/c5f9ba5a3e7bb7ae97bdd937bf5eee8903b80aced4f1a550e9e372f83bbcbb40.png" />
</div>
</div>
<p>As you can see, the candidate model with the dropout layers performs better than the baseline. It also takes longer to converge than the baseline.</p>
</section>
<section id="so-why-does-dropout-help">
<h2>So Why Does Dropout Help?<a class="headerlink" href="#so-why-does-dropout-help" title="Link to this heading">¶</a></h2>
<section id="regularization-noise-addition">
<h3>Regularization / Noise Addition<a class="headerlink" href="#regularization-noise-addition" title="Link to this heading">¶</a></h3>
<p>Notice that the baseline model tends to overfit considerably more on the training data, reaching a training accuracy of 99%, but lagging behind the candidate in terms of test set performance.</p>
<p>This is because dropout can be seen as a way of adding noise, which prevents the model from overfitting to the specific training examples and forces it to learn more general patterns, thus improving test performance.</p>
<p>It can also be seen as a form of regularization, which means that the addition of noise makes the model more robust to small changes in the input distribution.</p>
</section>
<section id="sparse-activations">
<h3>Sparse Activations<a class="headerlink" href="#sparse-activations" title="Link to this heading">¶</a></h3>
<p>Let’s plot the number of activated neurons per training example over time.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_activation_stats</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    <span class="n">baseline_activation_counts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">baseline_dead_neuron_counts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dead_neuron_mask</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">baseline_outputs</span> <span class="o">=</span>  <span class="n">load_outputs_from_disk</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span><span class="p">)</span>
        <span class="n">baseline_datapoints</span> <span class="o">=</span> <span class="n">baseline_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1">#if dead_neuron_mask is None:</span>
        <span class="c1">#    dead_neuron_mask = (baseline_outputs &lt;= 0)</span>
        <span class="c1">#dead_neuron_mask = (baseline_outputs &lt;= 0) &amp; dead_neuron_mask</span>

        <span class="c1">#dead_neurons = baseline_outputs[dead_neuron_mask]</span>
        <span class="c1">#num_dead_neurons = dead_neurons.flatten().shape[0]</span>
        <span class="c1">#mean_dead_neurons = num_dead_neurons/baseline_datapoints</span>
        <span class="c1">#baseline_dead_neuron_counts.append(mean_dead_neurons)</span>

        <span class="n">active_baseline_outputs</span> <span class="o">=</span> <span class="n">baseline_outputs</span><span class="p">[</span><span class="n">baseline_outputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">bcount</span> <span class="o">=</span> <span class="n">active_baseline_outputs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">baseline_datapoints</span>
        <span class="n">baseline_activation_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bcount</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">baseline_activation_counts</span><span class="p">,</span> <span class="n">baseline_dead_neuron_counts</span>


<span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="mi">4</span><span class="p">])</span>

<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>

    <span class="n">baseline_test_activation_counts</span><span class="p">,</span> <span class="n">baseline_test_dead_neuron_counts</span> <span class="o">=</span> <span class="n">get_activation_stats</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Relu_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1">#baseline_train_activation_counts, baseline_train_dead_neuron_counts = get_activation_stats(&quot;Baseline&quot;, &quot;train&quot;, f&quot;Relu_{layer}&quot;)</span>
    <span class="n">candidate_test_activation_counts</span><span class="p">,</span> <span class="n">candidate_test_dead_neuron_counts</span> <span class="o">=</span> <span class="n">get_activation_stats</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dropout_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
    <span class="c1">#candidate_train_activation_counts, candidate_train_dead_neuron_counts = get_activation_stats(&quot;Candidate&quot;, &quot;train&quot;, f&quot;Dropout_{layer}&quot;) </span>

    <span class="c1">#row = 2*layer</span>
    <span class="c1">#ax[row,0].plot(baseline_train_dead_neuron_counts)</span>
    <span class="c1">#ax[row,0].plot(candidate_train_dead_neuron_counts)</span>
    <span class="c1">#ax[row,0].set_title(f&quot;Layer Dense_{layer} Dead Neuron Counts (Train)&quot;)</span>
    <span class="c1">#ax[row,0].set_xlabel(&quot;Epoch&quot;)</span>
    <span class="c1">#ax[row,0].set_ylabel(&quot;Dead Neurons on the Train Set&quot;)</span>
    <span class="c1">#ax[row,0].legend([&quot;Baseline&quot;, &quot;Candidate (Dropout)&quot;])</span>

    <span class="c1">#ax[row,1].plot(baseline_test_dead_neuron_counts)</span>
    <span class="c1">#ax[row,1].plot(candidate_test_dead_neuron_counts)</span>
    <span class="c1">#ax[row,1].set_title(f&quot;Layer Dense_{layer} Dead Neuron Counts (Test)&quot;)</span>
    <span class="c1">#ax[row,1].set_xlabel(&quot;Epoch&quot;)</span>
    <span class="c1">#ax[row,1].set_ylabel(&quot;Dead Neurons on the Test Set&quot;)</span>
    <span class="c1">#ax[row,1].legend([&quot;Baseline&quot;, &quot;Candidate (Dropout)&quot;])</span>

    <span class="c1">#row = 2*layer+1</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">layer</span>
    <span class="c1">#ax[row,0].plot(baseline_train_activation_counts)</span>
    <span class="c1">#ax[row,0].plot(candidate_train_activation_counts)</span>
    <span class="c1">#ax[row,0].set_title(f&quot;Layer Dense_{layer} Activation Counts (Train)&quot;)</span>
    <span class="c1">#ax[row,0].set_xlabel(&quot;Epoch&quot;)</span>
    <span class="c1">#ax[row,0].set_ylabel(&quot;Activations on the Train Set&quot;)</span>
    <span class="c1">#ax[row,0].legend([&quot;Baseline&quot;, &quot;Candidate (Dropout)&quot;])</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">baseline_test_activation_counts</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">candidate_test_activation_counts</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer Dense_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2"> Activation Counts (Test)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Activations on the Test Set&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;Candidate (Dropout)&quot;</span><span class="p">])</span>


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">line</span> <span class="mi">29</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span> <span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="mi">4</span><span class="p">])</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">29</span>     <span class="n">baseline_test_activation_counts</span><span class="p">,</span> <span class="n">baseline_test_dead_neuron_counts</span> <span class="o">=</span> <span class="n">get_activation_stats</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Relu_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">30</span>     <span class="c1">#baseline_train_activation_counts, baseline_train_dead_neuron_counts = get_activation_stats(&quot;Baseline&quot;, &quot;train&quot;, f&quot;Relu_{layer}&quot;)</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span>     <span class="n">candidate_test_activation_counts</span><span class="p">,</span> <span class="n">candidate_test_dead_neuron_counts</span> <span class="o">=</span> <span class="n">get_activation_stats</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dropout_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 

<span class="nn">Cell In[10], line 18,</span> in <span class="ni">get_activation_stats</span><span class="nt">(model, phase, layer)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">baseline_datapoints</span> <span class="o">=</span> <span class="n">baseline_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="c1">#if dead_neuron_mask is None:</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="c1">#    dead_neuron_mask = (baseline_outputs &lt;= 0)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="c1">#dead_neuron_mask = (baseline_outputs &lt;= 0) &amp; dead_neuron_mask</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="c1">#mean_dead_neurons = num_dead_neurons/baseline_datapoints</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="c1">#baseline_dead_neuron_counts.append(mean_dead_neurons)</span>
<span class="ne">---&gt; </span><span class="mi">18</span> <span class="n">active_baseline_outputs</span> <span class="o">=</span> <span class="n">baseline_outputs</span><span class="p">[</span><span class="n">baseline_outputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="n">bcount</span> <span class="o">=</span> <span class="n">active_baseline_outputs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">baseline_datapoints</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">baseline_activation_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bcount</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/array.py:366,</span> in <span class="ni">ArrayImpl.__getitem__</span><span class="nt">(self, idx)</span>
<span class="g g-Whitespace">    </span><span class="mi">361</span>       <span class="n">out</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="n">dims</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">363</span>     <span class="k">return</span> <span class="n">ArrayImpl</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">364</span>         <span class="n">out</span><span class="o">.</span><span class="n">aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="p">[</span><span class="n">out</span><span class="p">],</span> <span class="n">committed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">_skip_checks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">366</span> <span class="k">return</span> <span class="n">lax_numpy</span><span class="o">.</span><span class="n">_rewriting_take</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:10622,</span> in <span class="ni">_rewriting_take</span><span class="nt">(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)</span>
<span class="g g-Whitespace">  </span><span class="mi">10619</span>       <span class="k">return</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_index_in_dim</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">  </span><span class="mi">10621</span> <span class="n">treedef</span><span class="p">,</span> <span class="n">static_idx</span><span class="p">,</span> <span class="n">dynamic_idx</span> <span class="o">=</span> <span class="n">_split_index_for_jit</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="ne">&gt; </span><span class="mi">10622</span> <span class="k">return</span> <span class="n">_gather</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">treedef</span><span class="p">,</span> <span class="n">static_idx</span><span class="p">,</span> <span class="n">dynamic_idx</span><span class="p">,</span> <span class="n">indices_are_sorted</span><span class="p">,</span>
<span class="g g-Whitespace">  </span><span class="mi">10623</span>                <span class="n">unique_indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:10663,</span> in <span class="ni">_gather</span><span class="nt">(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)</span>
<span class="g g-Whitespace">  </span><span class="mi">10660</span>   <span class="n">y</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">rev</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">indexer</span><span class="o">.</span><span class="n">reversed_y_dims</span><span class="p">)</span>
<span class="g g-Whitespace">  </span><span class="mi">10662</span> <span class="c1"># This adds np.newaxis/None dimensions.</span>
<span class="ne">&gt; </span><span class="mi">10663</span> <span class="k">return</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">indexer</span><span class="o">.</span><span class="n">newaxis_dims</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:2196,</span> in <span class="ni">expand_dims</span><span class="nt">(a, axis)</span>
<span class="g g-Whitespace">   </span><span class="mi">2194</span> <span class="n">util</span><span class="o">.</span><span class="n">check_arraylike</span><span class="p">(</span><span class="s2">&quot;expand_dims&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2195</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">2196</span> <span class="k">return</span> <span class="n">lax</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:1689,</span> in <span class="ni">expand_dims</span><span class="nt">(array, dimensions)</span>
<span class="g g-Whitespace">   </span><span class="mi">1687</span>   <span class="n">result_shape</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1688</span> <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim_out</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dims_set</span><span class="p">]</span>
<span class="ne">-&gt; </span><span class="mi">1689</span> <span class="k">return</span> <span class="n">broadcast_in_dim</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">result_shape</span><span class="p">,</span> <span class="n">broadcast_dims</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:1086,</span> in <span class="ni">broadcast_in_dim</span><span class="nt">(operand, shape, broadcast_dimensions)</span>
<span class="g g-Whitespace">   </span><span class="mi">1084</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1085</span>   <span class="n">dyn_shape</span><span class="p">,</span> <span class="n">static_shape</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">shape</span>  <span class="c1"># type: ignore</span>
<span class="ne">-&gt; </span><span class="mi">1086</span> <span class="k">return</span> <span class="n">broadcast_in_dim_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1087</span>     <span class="n">operand</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_shape</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">static_shape</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1088</span>     <span class="n">broadcast_dimensions</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">broadcast_dimensions</span><span class="p">))</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/core.py:438,</span> in <span class="ni">Primitive.bind</span><span class="nt">(self, *args, **params)</span>
<span class="g g-Whitespace">    </span><span class="mi">435</span> <span class="k">def</span> <span class="nf">bind</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">436</span>   <span class="k">assert</span> <span class="p">(</span><span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_checks</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span>
<span class="g g-Whitespace">    </span><span class="mi">437</span>           <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tracer</span><span class="p">)</span> <span class="ow">or</span> <span class="n">valid_jaxtype</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)),</span> <span class="n">args</span>
<span class="ne">--&gt; </span><span class="mi">438</span>   <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind_with_trace</span><span class="p">(</span><span class="n">find_top_trace</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/core.py:442,</span> in <span class="ni">Primitive.bind_with_trace</span><span class="nt">(self, trace, args, params)</span>
<span class="g g-Whitespace">    </span><span class="mi">440</span> <span class="k">def</span> <span class="nf">bind_with_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trace</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span>   <span class="k">with</span> <span class="n">pop_level</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">level</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">442</span>     <span class="n">out</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">full_raise</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">443</span>   <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">full_lower</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_results</span> <span class="k">else</span> <span class="n">full_lower</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/core.py:948,</span> in <span class="ni">EvalTrace.process_primitive</span><span class="nt">(self, primitive, tracers, params)</span>
<span class="g g-Whitespace">    </span><span class="mi">946</span>   <span class="k">return</span> <span class="n">call_impl_with_key_reuse_checks</span><span class="p">(</span><span class="n">primitive</span><span class="p">,</span> <span class="n">primitive</span><span class="o">.</span><span class="n">impl</span><span class="p">,</span> <span class="o">*</span><span class="n">tracers</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">947</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">948</span>   <span class="k">return</span> <span class="n">primitive</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="o">*</span><span class="n">tracers</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/dispatch.py:90,</span> in <span class="ni">apply_primitive</span><span class="nt">(prim, *args, **params)</span>
<span class="g g-Whitespace">     </span><span class="mi">88</span> <span class="n">prev</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">jax_jit</span><span class="o">.</span><span class="n">swap_thread_local_state_disable_jit</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">89</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">90</span>   <span class="n">outs</span> <span class="o">=</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">91</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">92</span>   <span class="n">lib</span><span class="o">.</span><span class="n">jax_jit</span><span class="o">.</span><span class="n">swap_thread_local_state_disable_jit</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>

    <span class="p">[</span><span class="o">...</span> <span class="n">skipping</span> <span class="n">hidden</span> <span class="mi">1</span> <span class="n">frame</span><span class="p">]</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:356,</span> in <span class="ni">_cpp_pjit.&lt;locals&gt;.cache_miss</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">353</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">no_tracing</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">354</span>   <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;re-tracing function </span><span class="si">{</span><span class="n">jit_info</span><span class="o">.</span><span class="n">fun_sourceinfo</span><span class="si">}</span><span class="s2"> for &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">355</span>                      <span class="s2">&quot;`jit`, but &#39;no_tracing&#39; is set&quot;</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">356</span> <span class="n">outs</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">attrs_tracked</span> <span class="o">=</span> <span class="n">_python_pjit_helper</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">357</span>     <span class="n">fun</span><span class="p">,</span> <span class="n">jit_info</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">358</span> <span class="n">executable</span> <span class="o">=</span> <span class="n">_read_most_recent_pjit_call_executable</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">359</span> <span class="n">pgle_profiler</span> <span class="o">=</span> <span class="n">_read_pgle_profiler</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:189,</span> in <span class="ni">_python_pjit_helper</span><span class="nt">(fun, jit_info, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">186</span>   <span class="n">args_flat</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">init_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args_flat</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">188</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">189</span>   <span class="n">out_flat</span> <span class="o">=</span> <span class="n">pjit_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args_flat</span><span class="p">,</span> <span class="o">**</span><span class="n">p</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">190</span> <span class="k">except</span> <span class="n">pxla</span><span class="o">.</span><span class="n">DeviceAssignmentMismatchError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">191</span>   <span class="n">fails</span><span class="p">,</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">args</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/core.py:2781,</span> in <span class="ni">AxisPrimitive.bind</span><span class="nt">(self, *args, **params)</span>
<span class="g g-Whitespace">   </span><span class="mi">2777</span> <span class="n">axis_main</span> <span class="o">=</span> <span class="nb">max</span><span class="p">((</span><span class="n">axis_frame</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">main_trace</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">used_axis_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">)),</span>
<span class="g g-Whitespace">   </span><span class="mi">2778</span>                 <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;level&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">2779</span> <span class="n">top_trace</span> <span class="o">=</span> <span class="p">(</span><span class="n">top_trace</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">axis_main</span> <span class="ow">or</span> <span class="n">axis_main</span><span class="o">.</span><span class="n">level</span> <span class="o">&lt;</span> <span class="n">top_trace</span><span class="o">.</span><span class="n">level</span>
<span class="g g-Whitespace">   </span><span class="mi">2780</span>              <span class="k">else</span> <span class="n">axis_main</span><span class="o">.</span><span class="n">with_cur_sublevel</span><span class="p">())</span>
<span class="ne">-&gt; </span><span class="mi">2781</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind_with_trace</span><span class="p">(</span><span class="n">top_trace</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/core.py:442,</span> in <span class="ni">Primitive.bind_with_trace</span><span class="nt">(self, trace, args, params)</span>
<span class="g g-Whitespace">    </span><span class="mi">440</span> <span class="k">def</span> <span class="nf">bind_with_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trace</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span>   <span class="k">with</span> <span class="n">pop_level</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">level</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">442</span>     <span class="n">out</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">full_raise</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">443</span>   <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">full_lower</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_results</span> <span class="k">else</span> <span class="n">full_lower</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/core.py:948,</span> in <span class="ni">EvalTrace.process_primitive</span><span class="nt">(self, primitive, tracers, params)</span>
<span class="g g-Whitespace">    </span><span class="mi">946</span>   <span class="k">return</span> <span class="n">call_impl_with_key_reuse_checks</span><span class="p">(</span><span class="n">primitive</span><span class="p">,</span> <span class="n">primitive</span><span class="o">.</span><span class="n">impl</span><span class="p">,</span> <span class="o">*</span><span class="n">tracers</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">947</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">948</span>   <span class="k">return</span> <span class="n">primitive</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="o">*</span><span class="n">tracers</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1764,</span> in <span class="ni">_pjit_call_impl</span><span class="nt">(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)</span>
<span class="g g-Whitespace">   </span><span class="mi">1755</span> <span class="k">if</span> <span class="n">xla_extension_version</span> <span class="o">&gt;=</span> <span class="mi">286</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1756</span>   <span class="n">cache_key</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">JitGlobalCppCacheKeys</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1757</span>       <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donated_argnums</span><span class="p">,</span> <span class="n">donate_argnames</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1758</span>       <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1762</span>       <span class="n">out_layouts_treedef</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_layouts_leaves</span><span class="o">=</span><span class="n">out_layouts</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1763</span>       <span class="n">use_resource_env</span><span class="o">=</span><span class="n">resource_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1764</span>   <span class="k">return</span> <span class="n">xc</span><span class="o">.</span><span class="n">_xla</span><span class="o">.</span><span class="n">pjit</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1765</span>       <span class="n">name</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">call_impl_cache_miss</span><span class="p">,</span> <span class="p">[],</span> <span class="p">[],</span> <span class="n">cache_key</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1766</span>       <span class="n">tree_util</span><span class="o">.</span><span class="n">dispatch_registry</span><span class="p">,</span> <span class="n">pxla</span><span class="o">.</span><span class="n">cc_shard_arg</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1767</span>       <span class="n">_get_cpp_global_cache</span><span class="p">(</span><span class="n">cache_key</span><span class="o">.</span><span class="n">contains_explicit_attributes</span><span class="p">))(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1768</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1769</span>   <span class="n">has_explicit_sharding</span> <span class="o">=</span> <span class="n">_pjit_explicit_sharding_and_layout</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1770</span>       <span class="n">in_shardings</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">in_layouts</span><span class="p">,</span> <span class="n">out_layouts</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1739,</span> in <span class="ni">_pjit_call_impl.&lt;locals&gt;.call_impl_cache_miss</span><span class="nt">(*args_, **kwargs_)</span>
<span class="g g-Whitespace">   </span><span class="mi">1738</span> <span class="k">def</span> <span class="nf">call_impl_cache_miss</span><span class="p">(</span><span class="o">*</span><span class="n">args_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1739</span>   <span class="n">out_flat</span><span class="p">,</span> <span class="n">compiled</span> <span class="o">=</span> <span class="n">_pjit_call_impl_python</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1740</span>       <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">jaxpr</span><span class="o">=</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_shardings</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1741</span>       <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_shardings</span><span class="p">,</span> <span class="n">in_layouts</span><span class="o">=</span><span class="n">in_layouts</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1742</span>       <span class="n">out_layouts</span><span class="o">=</span><span class="n">out_layouts</span><span class="p">,</span> <span class="n">resource_env</span><span class="o">=</span><span class="n">resource_env</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1743</span>       <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">keep_unused</span><span class="o">=</span><span class="n">keep_unused</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1744</span>       <span class="n">inline</span><span class="o">=</span><span class="n">inline</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>   <span class="n">pgle_profiler</span> <span class="o">=</span> <span class="n">_read_pgle_profiler</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>   <span class="n">fastpath_data</span> <span class="o">=</span> <span class="n">_get_fastpath_data</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1747</span>       <span class="n">compiled</span><span class="p">,</span> <span class="n">tree_structure</span><span class="p">(</span><span class="n">out_flat</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">,</span> <span class="p">[],</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span>       <span class="n">jaxpr</span><span class="o">.</span><span class="n">consts</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">pgle_profiler</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1669,</span> in <span class="ni">_pjit_call_impl_python</span><span class="nt">(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)</span>
<span class="g g-Whitespace">   </span><span class="mi">1656</span>     <span class="n">compile_options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fdo_profile&#39;</span><span class="p">:</span> <span class="n">fdo_profile</span><span class="p">}</span>
<span class="g g-Whitespace">   </span><span class="mi">1658</span> <span class="c1"># TODO(patrios): Do not pass mutable profile session through cached lowering</span>
<span class="g g-Whitespace">   </span><span class="mi">1659</span> <span class="c1"># chain. Instead we need to move profilers dictionary to pxla module and use</span>
<span class="g g-Whitespace">   </span><span class="mi">1660</span> <span class="c1"># module as key. Right now we can&#39;t do that since there is no way to evict _pjit_lower_cached cache for in PGLE mode.</span>
<span class="g g-Whitespace">   </span><span class="mi">1661</span> <span class="n">compiled</span> <span class="o">=</span> <span class="n">_resolve_and_lower</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1662</span>     <span class="n">args</span><span class="p">,</span> <span class="n">jaxpr</span><span class="o">=</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_shardings</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1663</span>     <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_shardings</span><span class="p">,</span> <span class="n">in_layouts</span><span class="o">=</span><span class="n">in_layouts</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1664</span>     <span class="n">out_layouts</span><span class="o">=</span><span class="n">out_layouts</span><span class="p">,</span> <span class="n">resource_env</span><span class="o">=</span><span class="n">resource_env</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1665</span>     <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">keep_unused</span><span class="o">=</span><span class="n">keep_unused</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1666</span>     <span class="n">inline</span><span class="o">=</span><span class="n">inline</span><span class="p">,</span> <span class="n">lowering_platforms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1667</span>     <span class="n">lowering_parameters</span><span class="o">=</span><span class="n">mlir</span><span class="o">.</span><span class="n">LoweringParameters</span><span class="p">(),</span>
<span class="g g-Whitespace">   </span><span class="mi">1668</span>     <span class="n">pgle_profiler</span><span class="o">=</span><span class="n">pgle_profiler</span>
<span class="ne">-&gt; </span><span class="mi">1669</span> <span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">compile_options</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1671</span> <span class="n">_most_recent_pjit_call_executable</span><span class="o">.</span><span class="n">weak_key_dict</span><span class="p">[</span><span class="n">jaxpr</span><span class="p">]</span> <span class="o">=</span> <span class="n">compiled</span>
<span class="g g-Whitespace">   </span><span class="mi">1672</span> <span class="c1"># This check is expensive so only do it if enable_checks is on.</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2315,</span> in <span class="ni">MeshComputation.compile</span><span class="nt">(self, compiler_options)</span>
<span class="g g-Whitespace">   </span><span class="mi">2313</span> <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compiler_options</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MeshExecutable</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2314</span>   <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">compiler_options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">2315</span>     <span class="n">executable</span> <span class="o">=</span> <span class="n">UnloadedMeshExecutable</span><span class="o">.</span><span class="n">from_hlo</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2316</span>         <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2317</span>         <span class="n">compiler_options</span><span class="o">=</span><span class="n">compiler_options</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2318</span>     <span class="k">if</span> <span class="n">compiler_options</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2319</span>       <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="o">=</span> <span class="n">executable</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2829,</span> in <span class="ni">UnloadedMeshExecutable.from_hlo</span><span class="nt">(***failed resolving arguments***)</span>
<span class="g g-Whitespace">   </span><span class="mi">2826</span>       <span class="n">mesh</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">mesh</span>  <span class="c1"># type: ignore</span>
<span class="g g-Whitespace">   </span><span class="mi">2827</span>       <span class="k">break</span>
<span class="ne">-&gt; </span><span class="mi">2829</span> <span class="n">xla_executable</span> <span class="o">=</span> <span class="n">_cached_compilation</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2830</span>     <span class="n">hlo</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">spmd_lowering</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2831</span>     <span class="n">tuple_args</span><span class="p">,</span> <span class="n">auto_spmd_lowering</span><span class="p">,</span> <span class="n">allow_prop_to_inputs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2832</span>     <span class="n">allow_prop_to_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">host_callbacks</span><span class="p">),</span> <span class="n">backend</span><span class="p">,</span> <span class="n">da</span><span class="p">,</span> <span class="n">pmap_nreps</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2833</span>     <span class="n">compiler_options_keys</span><span class="p">,</span> <span class="n">compiler_options_values</span><span class="p">,</span> <span class="n">pgle_profiler</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2835</span> <span class="k">if</span> <span class="n">auto_spmd_lowering</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2836</span>   <span class="k">assert</span> <span class="n">mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2641,</span> in <span class="ni">_cached_compilation</span><span class="nt">(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)</span>
<span class="g g-Whitespace">   </span><span class="mi">2633</span> <span class="n">compile_options</span> <span class="o">=</span> <span class="n">create_compile_options</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2634</span>     <span class="n">computation</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">spmd_lowering</span><span class="p">,</span> <span class="n">tuple_args</span><span class="p">,</span> <span class="n">auto_spmd_lowering</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2635</span>     <span class="n">allow_prop_to_inputs</span><span class="p">,</span> <span class="n">allow_prop_to_outputs</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2636</span>     <span class="n">dev</span><span class="p">,</span> <span class="n">pmap_nreps</span><span class="p">,</span> <span class="n">compiler_options</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2638</span> <span class="k">with</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">log_elapsed_time</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2639</span>     <span class="s2">&quot;Finished XLA compilation of </span><span class="si">{fun_name}</span><span class="s2"> in </span><span class="si">{elapsed_time:.9f}</span><span class="s2"> sec&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2640</span>     <span class="n">fun_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">event</span><span class="o">=</span><span class="n">dispatch</span><span class="o">.</span><span class="n">BACKEND_COMPILE_EVENT</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">2641</span>   <span class="n">xla_executable</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">compile_or_get_cached</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2642</span>       <span class="n">backend</span><span class="p">,</span> <span class="n">computation</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">compile_options</span><span class="p">,</span> <span class="n">host_callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2643</span>       <span class="n">pgle_profiler</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2644</span> <span class="k">return</span> <span class="n">xla_executable</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/compiler.py:427,</span> in <span class="ni">compile_or_get_cached</span><span class="nt">(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)</span>
<span class="g g-Whitespace">    </span><span class="mi">425</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">426</span>   <span class="n">log_persistent_cache_miss</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">427</span>   <span class="k">return</span> <span class="n">_compile_and_write_cache</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">428</span>       <span class="n">backend</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">429</span>       <span class="n">computation</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">430</span>       <span class="n">compile_options</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">431</span>       <span class="n">host_callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">432</span>       <span class="n">module_name</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">433</span>       <span class="n">cache_key</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">434</span>   <span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/compiler.py:655,</span> in <span class="ni">_compile_and_write_cache</span><span class="nt">(backend, computation, compile_options, host_callbacks, module_name, cache_key)</span>
<span class="g g-Whitespace">    </span><span class="mi">646</span> <span class="k">def</span> <span class="nf">_compile_and_write_cache</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">647</span>     <span class="n">backend</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">Client</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">648</span>     <span class="n">computation</span><span class="p">:</span> <span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">652</span>     <span class="n">cache_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">653</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">xc</span><span class="o">.</span><span class="n">LoadedExecutable</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">654</span>   <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">monotonic</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">655</span>   <span class="n">executable</span> <span class="o">=</span> <span class="n">backend_compile</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">656</span>       <span class="n">backend</span><span class="p">,</span> <span class="n">computation</span><span class="p">,</span> <span class="n">compile_options</span><span class="p">,</span> <span class="n">host_callbacks</span>
<span class="g g-Whitespace">    </span><span class="mi">657</span>   <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">658</span>   <span class="n">compile_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">monotonic</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="g g-Whitespace">    </span><span class="mi">659</span>   <span class="n">_cache_write</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">660</span>       <span class="n">cache_key</span><span class="p">,</span> <span class="n">compile_time</span><span class="p">,</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">executable</span><span class="p">,</span> <span class="n">host_callbacks</span>
<span class="g g-Whitespace">    </span><span class="mi">661</span>   <span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:333,</span> in <span class="ni">annotate_function.&lt;locals&gt;.wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span> <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span> <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span>   <span class="k">with</span> <span class="n">TraceAnnotation</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">decorator_kwargs</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">333</span>     <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">334</span>   <span class="k">return</span> <span class="n">wrapper</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/compiler.py:267,</span> in <span class="ni">backend_compile</span><span class="nt">(backend, module, options, host_callbacks)</span>
<span class="g g-Whitespace">    </span><span class="mi">261</span>     <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">262</span>         <span class="n">built_c</span><span class="p">,</span> <span class="n">compile_options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span> <span class="n">host_callbacks</span><span class="o">=</span><span class="n">host_callbacks</span>
<span class="g g-Whitespace">    </span><span class="mi">263</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">264</span>   <span class="c1"># Some backends don&#39;t have `host_callbacks` option yet</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span>   <span class="c1"># TODO(sharadmv): remove this fallback when all backends allow `compile`</span>
<span class="g g-Whitespace">    </span><span class="mi">266</span>   <span class="c1"># to take in `host_callbacks`</span>
<span class="ne">--&gt; </span><span class="mi">267</span>   <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">built_c</span><span class="p">,</span> <span class="n">compile_options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">268</span> <span class="k">except</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaRuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">269</span>   <span class="k">for</span> <span class="n">error_handler</span> <span class="ow">in</span> <span class="n">_XLA_RUNTIME_ERROR_HANDLERS</span><span class="p">:</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
<img alt="_images/a2fd98adead867ee269b8ab097ca90f0ae0486b675be7227b3a9b703b91c48e2.png" src="_images/a2fd98adead867ee269b8ab097ca90f0ae0486b675be7227b3a9b703b91c48e2.png" />
</div>
</div>
<p>Notice that both the baseline and candidate models have similar numbers of activated neurons at the start of training. However, as training progresses, the candidate model tends to have fewer activations per training example. Dropout thus encourages sparse representations, which, as we saw in the results, also tend to be more robust to changes in the inputs (i.e. regularization).</p>
<p>In the baseline model, a neuron tends to co-adapt to other specific neurons, which leads to overfitting. By using dropout, a neuron cannot necessarily depend on other units and has to individually learn a more robust functiion.</p>
<p>Next, let’s look at the activation statistics.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>

    <span class="n">baseline_output_means</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">baseline_output_stds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">candidate_output_means</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">candidate_output_stds</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">baseline_o1</span> <span class="o">=</span> <span class="n">load_outputs_from_disk</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Relu_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">baseline_m1</span> <span class="o">=</span> <span class="n">baseline_o1</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">baseline_output_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">baseline_m1</span><span class="p">)</span>
        <span class="n">baseline_s1</span> <span class="o">=</span> <span class="n">baseline_o1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
        <span class="n">baseline_output_stds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">baseline_s1</span><span class="p">)</span>

        <span class="n">candidate_o1</span> <span class="o">=</span> <span class="n">load_outputs_from_disk</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dropout_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">candidate_m1</span> <span class="o">=</span> <span class="n">candidate_o1</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">candidate_output_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate_m1</span><span class="p">)</span>
        <span class="n">candidate_s1</span> <span class="o">=</span> <span class="n">candidate_o1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
        <span class="n">candidate_output_stds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate_s1</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer Dense_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2"> Outputs (Mean)&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Train Epoch&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">baseline_output_means</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">candidate_output_means</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer Dense_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2"> Outputs (Standard Deviation)&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Train Epoch&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">baseline_output_stds</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">candidate_output_stds</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;Candidate&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/62f25f288dc7f7085bb4912b45b9946b41893366a920d440ae1a4fef618c8a08.png" src="_images/62f25f288dc7f7085bb4912b45b9946b41893366a920d440ae1a4fef618c8a08.png" />
</div>
</div>
<p>Notice that both the mean and standard deviation of the baseline activations increase as training progressses. However, the candidate activation statistics tend to stabilize and even decrease.</p>
<p>Let’s confirm this by comparing activation histograms.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">baseline_all_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">candidate_all_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">load_outputs_from_disk</span><span class="p">(</span><span class="s2">&quot;Baseline&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Relu_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">baseline_all_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">load_outputs_from_disk</span><span class="p">(</span><span class="s2">&quot;Candidate&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Relu_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">candidate_all_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>


<span class="n">baseline_all_outputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">baseline_all_outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">candidate_all_outputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">candidate_all_outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">baseline_all_outputs</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.00</span><span class="p">,</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1e6</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Baseline Positive Activations (Test)&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">candidate_all_outputs</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.00</span><span class="p">,</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1e7</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Candidate Positive Activations (Test)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/dd1ed4a18e9b65dfd928bc4343cb1bc467af489e21c535d734b06b3de1bab4e2.png" src="_images/dd1ed4a18e9b65dfd928bc4343cb1bc467af489e21c535d734b06b3de1bab4e2.png" />
</div>
</div>
<p>It’s clear that the baseline activations are more spread out than the candidate ones, which are more sparse and smaller in magnitude. Thus, not only are fewer neurons activated per example, but the magnitudes of the activations themselves are diminished. This is due to the network having learned to avoid overly depending on specific neurons across training examples.</p>
</section>
<section id="ensembling">
<h3>Ensembling<a class="headerlink" href="#ensembling" title="Link to this heading">¶</a></h3>
<p>Dropout can thus be seen as a way of creating an ensemble of models with shared weights. Mathematically, a single neural network with dropout is an ensemble of <span class="math notranslate nohighlight">\(2^n\)</span> models, where <span class="math notranslate nohighlight">\(n\)</span> is the number of neurons. In the light of this, we can explain the fewer and sparser activations as smaller sub-networks specializing in recognizing specific types of examples.</p>
</section>
<section id="references">
<h3>References:<a class="headerlink" href="#references" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1).</p></li>
<li><p>Baldi, P., &amp; Sadowski, P. (2013). Understanding dropout. Advances in Neural Information Processing Systems, 26.</p></li>
<li><p>Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13).</p></li>
<li><p>Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10).</p></li>
</ol>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="intro.html">
              <img class="logo" src="_static/starburst-transparent.png" alt="Logo of Starburst Data Science Blog"/>
            </a></p>
<h1 class="logo"><a href="intro.html">Starburst Data Science Blog</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="09-jax-mnist-fc-batchnorm.html">How and Why Does Batch Normalization Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="nadaraya-watson-kernel-regression.html">Nadaraya-Watson Regression</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="intro.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024 Vikram Pawar novastar53.github.io.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/11-flax-cifar10-fc-dropout.ipynb"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>