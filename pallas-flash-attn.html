<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>How to Write a Flash Attention Kernel in Pallas &#8212; Vikram&#39;s Data Science Blog</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=3eba45b5" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css?v=8c9916cd" />
    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/custom.js?v=5af35df4"></script>
    <script src="_static/js/sidebar_template.js?v=e363622b"></script>
    <link rel="icon" href="_static/starburst (200 x 200 px).png"/>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How do Mixture of Experts Layers Work? Part 2" href="how-mixture-of-experts-works-part-2.html" />
    <link rel="prev" title="Machine Learning Blog" href="intro.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="how-to-write-a-flash-attention-kernel-in-pallas">
<h1>How to Write a Flash Attention Kernel in Pallas<a class="headerlink" href="#how-to-write-a-flash-attention-kernel-in-pallas" title="Link to this heading">¶</a></h1>
<p>In the previous posts in this series, we learnt how to write a <a class="reference external" href="https://blog.vikrampawar.com/pallas-matmul.html">matrix multiplication kernel</a> and a <a class="reference external" href="https://blog.vikrampawar.com/pallas-softmax.html">softmax kernel</a> using Pallas. Building on these, we will design a fused self-attention kernel. Self-attention is a major bottleneck in deep learning architectures. In a naive implementation, materializing the full <span class="math notranslate nohighlight">\(T \times T\)</span> attention matrix requires <span class="math notranslate nohighlight">\(O(T^2)\)</span> memory bandwidth. This creates a severe bottleneck on GPUs, as the time to transfer this data from high-bandwidth memory (HBM) dominates over the actual computation time.</p>
<section id="self-attention">
<h2>Self Attention<a class="headerlink" href="#self-attention" title="Link to this heading">¶</a></h2>
<p>Mathematically, the self-attention operation is <span class="math notranslate nohighlight">\(\text{softmax}(QK^T/\sqrt{d}) V\)</span>, where <span class="math notranslate nohighlight">\(Q\)</span> is a set of queries, <span class="math notranslate nohighlight">\(K\)</span> is a set of keys and <span class="math notranslate nohighlight">\(V\)</span> is a set of values.</p>
<p><span class="math notranslate nohighlight">\(Q\)</span> is usually a tensor of shape <span class="math notranslate nohighlight">\((B, H, T, D)\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is batch size, <span class="math notranslate nohighlight">\(H\)</span> is number of heads, <span class="math notranslate nohighlight">\(T\)</span> is sequence length, and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension (or head dimension). Each query vector represents a position in the sequence that attends to keys. Keys are used to compute attention scores with queries while values are the information retrieved based on attention weights.
Scaling by <span class="math notranslate nohighlight">\(1/\sqrt{d}\)</span> stabilizes the variance, ensuring the softmax behaves similarly across different embedding dimensions, improving optimization and generalization.</p>
<p>To understand self-attention in more detail, <a class="reference external" href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">here’s</a> an excellent blogpost by Sebastian Raschka.</p>
</section>
<section id="minimal-gpu-system-architecture">
<h2>Minimal GPU System Architecture<a class="headerlink" href="#minimal-gpu-system-architecture" title="Link to this heading">¶</a></h2>
<p>To test our implementation, we will use an NVIDIA RTX 4000 Ada Generation GPU. This is a fairly powerful (and cheap!) GPU with a modern architecture. Let’s build a simple model of its system architecture. This will help us understand compute and memory tradeoffs involved.</p>
<p><img alt="rtx-sys-diag" src="_images/rtx_4000_ada_system_diagram.png" /></p>
<section id="memory-hierarchy">
<h3>Memory Hierarchy<a class="headerlink" href="#memory-hierarchy" title="Link to this heading">¶</a></h3>
<p>SMEM/L1 cache is the fastest memory, orders of magnitude faster than HBM. Crucially, it’s software-managed—we explicitly control what gets loaded and stored via <code class="docutils literal notranslate"><span class="pre">plgpu.load</span></code> and <code class="docutils literal notranslate"><span class="pre">plgpu.store</span></code> in our Pallas kernels. Each SM has its own 128 KB (99 KB SMEM + L1 Cache), isolated from others. This is where flash attention keeps intermediate tiles, attention score tiles, and accumulators.</p>
<p>L2 cache (48 MB) sits in the middle. However, it’s hardware-managed. All SMs compete for this shared space, and hardware may evict data between operations. The kernel design assumes data lives in SMEM or HBM only.</p>
<p>HBM (20 GB) is the slowest memory at 360 GB/s, but it has the largest capacity. This stores all our tensors, <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> inputs, <span class="math notranslate nohighlight">\(O\)</span> outputs, and gradients <span class="math notranslate nohighlight">\(dQ, dK, dV\)</span>. Every read or write to HBM costs execution time, so minimizing HBM transfers is critical for performance. Flash attention reads <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> from HBM once and writes <span class="math notranslate nohighlight">\(O\)</span> to HBM once—everything else stays in SMEM.</p>
</section>
</section>
<section id="naive-self-attention">
<h2>Naive Self-Attention<a class="headerlink" href="#naive-self-attention" title="Link to this heading">¶</a></h2>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">naive_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bhqd,bhkd-&gt;bhqk&#39;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bhqk,bhkd-&gt;bhqd&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">o</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">64</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># Use bfloat16 for optimal performance</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">do</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="c1"># Forward check</span>
<span class="n">o_ref</span> <span class="o">=</span> <span class="n">naive_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reference output shape: </span><span class="si">{</span><span class="n">o_ref</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Backward check</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss_ref</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">naive_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="n">do</span><span class="p">)</span>

<span class="n">dq_ref</span><span class="p">,</span> <span class="n">dk_ref</span><span class="p">,</span> <span class="n">dv_ref</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_ref</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reference gradient shapes: dq=</span><span class="si">{</span><span class="n">dq_ref</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dk=</span><span class="si">{</span><span class="n">dk_ref</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dv=</span><span class="si">{</span><span class="n">dv_ref</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W0127 08:15:20.617041    2432 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn&#39;t support extracting fabric info or NVLink is not used by the device.
W0127 08:15:20.619851    2217 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn&#39;t support extracting fabric info or NVLink is not used by the device.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Reference output shape: (2, 4, 256, 64)
Reference gradient shapes: dq=(2, 4, 256, 64), dk=(2, 4, 256, 64), dv=(2, 4, 256, 64)
</pre></div>
</div>
</div>
</div>
<section id="why-it-is-slow">
<h3>Why it is Slow<a class="headerlink" href="#why-it-is-slow" title="Link to this heading">¶</a></h3>
<p>The naive implementation must handle a <span class="math notranslate nohighlight">\((T, T)\)</span> attention matrix that quickly exceeds GPU shared memory (SMEM) capacity.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>T</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\((T, T)\)</span> matrix (bf16)</p></th>
<th class="head"><p>Fits in SMEM?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>32KB</p></td>
<td><p>Yes (barely)</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>128KB</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
<p>For <span class="math notranslate nohighlight">\(T \geq 256\)</span>, the attention matrix must be materialized in HBM. Here’s the actual data flow:</p>
<ol class="arabic simple">
<li><p>Read <span class="math notranslate nohighlight">\(Q, K, V\)</span> from HBM</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(QK^T\)</span> in tiles (each tile fits in SMEM), write each tile to HBM as it completes</p></li>
<li><p>Read the <span class="math notranslate nohighlight">\((T, T)\)</span> matrix from HBM row-by-row for softmax: first pass computes row max, second pass computes <code class="docutils literal notranslate"><span class="pre">exp(x</span> <span class="pre">-</span> <span class="pre">max)</span></code>, sum, and normalizes</p></li>
<li><p>Write softmax output to HBM</p></li>
<li><p>Read softmax output from HBM for <span class="math notranslate nohighlight">\(PV\)</span> (tiled)</p></li>
<li><p>Write final output O to HBM</p></li>
</ol>
<p>The matmuls use tiling internally—small blocks are loaded into SMEM, computed, and written back—but the full <span class="math notranslate nohighlight">\((T, T)\)</span> result must still be materialized in HBM between operations.</p>
<p>Softmax processes one row at a time (<span class="math notranslate nohighlight">\(T\)</span> elements easily fit in SMEM), but requires two passes through each row: once to find the maximum value, and again to compute the normalized probabilities. This means the <span class="math notranslate nohighlight">\((T,T)\)</span> matrix is read twice during the softmax operation alone.</p>
</section>
</section>
<section id="flash-attention-algorithm-forward-pass">
<h2>Flash Attention Algorithm (Forward Pass)<a class="headerlink" href="#flash-attention-algorithm-forward-pass" title="Link to this heading">¶</a></h2>
<p>The key insight is that we can compute the attention output without ever materializing the full <span class="math notranslate nohighlight">\((T,T)\)</span> attention matrix in HBM. Instead, we process it in small tiles that fit in SMEM, discarding each tile after using it. Let <span class="math notranslate nohighlight">\(B_r\)</span> and <span class="math notranslate nohighlight">\(B_c\)</span> denote the tile sizes for rows (queries) and columns (keys) respectively.</p>
<ol class="arabic simple">
<li><p>Tile <span class="math notranslate nohighlight">\(Q\)</span> into <span class="math notranslate nohighlight">\(T/B_r\)</span> blocks and process them in parallel across the outer loop</p></li>
<li><p>For each query block, initialize running max <span class="math notranslate nohighlight">\(m\)</span>, sum <span class="math notranslate nohighlight">\(l\)</span>, and output accumulator <span class="math notranslate nohighlight">\(o\)</span> to zero</p></li>
<li><p>Tile <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> into <span class="math notranslate nohighlight">\(T/B_c\)</span> blocks and process them sequentially in the inner loop</p>
<ul class="simple">
<li><p>Load <span class="math notranslate nohighlight">\(Q\)</span> tile <span class="math notranslate nohighlight">\((B_r,D)\)</span>, <span class="math notranslate nohighlight">\(K\)</span> tile <span class="math notranslate nohighlight">\((B_c,D)\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> tile <span class="math notranslate nohighlight">\((B_c,D)\)</span> into SMEM</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(QK^T\)</span> to produce a <span class="math notranslate nohighlight">\((B_r,B_c)\)</span> attention tile in SMEM</p></li>
<li><p>Apply online softmax using running statistics:</p>
<ul>
<li><p>Find the maximum <span class="math notranslate nohighlight">\(m_{blk}\)</span> of the current tile</p></li>
<li><p>Update the running max: <span class="math notranslate nohighlight">\(m = \max(m, m_{blk})\)</span></p></li>
<li><p>Exponentiate the shifted scores: <span class="math notranslate nohighlight">\(\exp(S - m)\)</span></p></li>
<li><p>Update the running sum: <span class="math notranslate nohighlight">\(l = l \cdot \exp(m_{old} - m) + \sum \exp(S - m)\)</span></p></li>
</ul>
</li>
<li><p>Multiply the softmax result with <span class="math notranslate nohighlight">\(V\)</span> and accumulate into the output</p></li>
<li><p>Apply correction factor to both the running statistics and output accumulator when the maximum changes</p></li>
</ul>
</li>
<li><p>After processing all <span class="math notranslate nohighlight">\(K/V\)</span> tiles for a query block, perform final normalization</p></li>
<li><p>Write the final output <span class="math notranslate nohighlight">\((T,D)\)</span> to HBM once</p></li>
</ol>
<p>Check out the <a class="reference external" href="https://blog.vikrampawar.com/pallas-softmax.html">Pallas softmax kernel implementation</a> to understand the online softmax algorithm.</p>
<section id="the-logsumexp-trick">
<h3>The Logsumexp Trick<a class="headerlink" href="#the-logsumexp-trick" title="Link to this heading">¶</a></h3>
<p>The logsumexp trick is what enables the flash attention backward pass to avoid recomputing the softmax statistics. During the forward pass, we compute and store logsumexp values alongside the output.</p>
<p>Starting from the softmax definition:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
P_i = \frac{\exp(S_i - m)}{\sum_j \exp(S_j - m)} \\\end{split}\\\begin{split}\log P_i = \log \exp(S_i - m) - \log \left( \sum_j \exp(S_j - m) \right) \\\end{split}\\\begin{split}\log P_i = S_i - m - \log \left( \sum_j \exp(S_j - m) \right) \\
\end{split}\end{aligned}\end{align} \]</div>
<p>Let <span class="math notranslate nohighlight">\(l = \sum_j \exp(S_j - m)\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
\log P_i = S_i - m - \log l \\\end{split}\\\begin{split}\log P_i = S_i - (m + \log l) \\
\end{split}\end{aligned}\end{align} \]</div>
<p>Let <span class="math notranslate nohighlight">\(\text{logsumexp} = m + \log l\)</span></p>
<div class="math notranslate nohighlight">
\[
P_i = \exp(S_i - \text{logsumexp})
\]</div>
</section>
<section id="precision-optimization-from-float32-to-bfloat16">
<h3>Precision Optimization: From Float32 to Bfloat16<a class="headerlink" href="#precision-optimization-from-float32-to-bfloat16" title="Link to this heading">¶</a></h3>
<p>A key optimization in our implementation is the careful management of numerical precision. The naive approach of casting everything to float32 wastes memory bandwidth, while pure bfloat16 causes numerical instability. Our optimized approach uses mixed precision: bfloat16 for memory transfers and tensor core operations, float32 for sensitive intermediate computations.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Dtype</p></th>
<th class="head"><p>Reason</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Load Q, K, V, dO</p></td>
<td><p>bf16</p></td>
<td><p>Half the memory bandwidth</p></td>
</tr>
<tr class="row-odd"><td><p>Matmul inputs</p></td>
<td><p>bf16</p></td>
<td><p>Fast bf16 tensor cores</p></td>
</tr>
<tr class="row-even"><td><p>Matmul outputs</p></td>
<td><p>float32</p></td>
<td><p>Tensor cores accumulate in float32</p></td>
</tr>
<tr class="row-odd"><td><p>Softmax (exp, max, sum)</p></td>
<td><p>float32</p></td>
<td><p>Numerical stability</p></td>
</tr>
<tr class="row-even"><td><p>Running accumulators</p></td>
<td><p>float32</p></td>
<td><p>Avoid precision loss across blocks</p></td>
</tr>
<tr class="row-odd"><td><p>Store outputs</p></td>
<td><p>bf16</p></td>
<td><p>Match input dtype</p></td>
</tr>
</tbody>
</table>
<section id="why-certain-values-must-stay-float32">
<h4>Why Certain Values Must Stay Float32<a class="headerlink" href="#why-certain-values-must-stay-float32" title="Link to this heading">¶</a></h4>
<p><strong>Running max (<code class="docutils literal notranslate"><span class="pre">max_reg</span></code>)</strong>: Could technically be bf16 since it’s just tracking maximums, but keeping it float32 costs nothing (only BLOCK_R=64 elements) and avoids edge cases.</p>
<p><strong>Running sum (<code class="docutils literal notranslate"><span class="pre">l_reg</span></code>)</strong>: Must be float32. It accumulates across all K blocks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">l_reg</span> <span class="o">=</span> <span class="n">l_reg</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">max_reg</span> <span class="o">-</span> <span class="n">max_blk</span><span class="p">)</span> <span class="o">+</span> <span class="n">l_blk</span>
</pre></div>
</div>
<p>With T=1024 and BLOCK_C=64, that’s 64 iterations. Bf16 would lose small contributions when adding to large sums.</p>
<p><strong>Logsumexp</strong>: Used in backward pass as <code class="docutils literal notranslate"><span class="pre">exp(s_blk</span> <span class="pre">-</span> <span class="pre">logsumexp)</span></code>. Errors in the exponent get amplified exponentially.</p>
<p><strong>Output accumulator (<code class="docutils literal notranslate"><span class="pre">o_reg</span></code>)</strong>: Same accumulation issue as <code class="docutils literal notranslate"><span class="pre">l_reg</span></code> - must be float32 to avoid losing small corrections.</p>
</section>
</section>
<section id="warp-and-pipeline-stage-configuration">
<h3>Warp and Pipeline Stage Configuration<a class="headerlink" href="#warp-and-pipeline-stage-configuration" title="Link to this heading">¶</a></h3>
<p>The warp count and pipeline stage count are critical compiler parameters that control parallelism and memory hiding. A warp is 32 threads that execute instructions in lockstep on NVIDIA GPUs. Setting num_warps=4 means each block contains 4 warps (128 threads total).</p>
<p>The pipeline stage count controls software pipelining, an optimization that overlaps memory loads with computation. With num_stages=2, the compiler maintains two pipeline stages: while stage 1 executes computation on data already loaded into shared memory, stage 2 prefetches the next data block from high-bandwidth memory. This hiding of memory latency is crucial for compute-bound kernels like flash attention. Higher num_stages can further hide latency at the cost of additional shared memory consumption, which must stay within the 99KB limit.</p>
</section>
<section id="choosing-kernel-and-embedding-size">
<h3>Choosing Kernel and Embedding Size<a class="headerlink" href="#choosing-kernel-and-embedding-size" title="Link to this heading">¶</a></h3>
<p>The flash attention kernel in this implementation uses hardcoded block sizes <span class="math notranslate nohighlight">\((B_R, B_C)\)</span>.
Let’s calculate the shared memory usage for the forward pass kernel. The following tensors reside in shared memory:</p>
<p>For <span class="math notranslate nohighlight">\(C=64\)</span>:</p>
<ul class="simple">
<li><p>q_reg: <span class="math notranslate nohighlight">\(64 \times 64 \times 2\)</span> = 8KB</p></li>
<li><p>k_blk: <span class="math notranslate nohighlight">\(64 \times 64 \times 2\)</span> = 8KB</p></li>
<li><p>v_blk: <span class="math notranslate nohighlight">\(64 \times 64 \times 2\)</span> = 8KB</p></li>
<li><p>o_reg: <span class="math notranslate nohighlight">\(64 \times 64 \times 4\)</span> = 16KB</p></li>
<li><p>s_blk: <span class="math notranslate nohighlight">\(64 \times 64 \times 4\)</span> = 16KB</p></li>
<li><p>max_reg, l_reg, max_blk, l_blk, logsumexp_reg: <span class="math notranslate nohighlight">\(5 \times 64 \times 4\)</span> = 1.25KB</p></li>
</ul>
<p>Subtotal: ~57KB</p>
<p>With NUM_STAGES=2, software pipelining allocates additional copies of k_blk and v_blk to overlap memory loads with computation, adding ~16KB.</p>
<p>Total for C=64: ~73KB + compiler overhead</p>
<p>For <span class="math notranslate nohighlight">\(C=128\)</span>:</p>
<ul class="simple">
<li><p>q_reg, k_blk, v_blk: <span class="math notranslate nohighlight">\(3 \times 64 \times 128 \times 2\)</span> = 48KB</p></li>
<li><p>o_reg: <span class="math notranslate nohighlight">\(64 \times 128 \times 4\)</span> = 32KB</p></li>
<li><p>s_blk: <span class="math notranslate nohighlight">\(64 \times 64 \times 4\)</span> = 16KB</p></li>
<li><p>Scalar registers: ~1.25KB</p></li>
<li><p>Pipelining overhead: ~32KB</p></li>
</ul>
<p>Total for <span class="math notranslate nohighlight">\(C=128\)</span>: ~129KB — exceeds the 99KB limit</p>
<p>With the mixed-precision strategy and software pipelining overhead, C=64 fits comfortably while C=128 exceeds the limit.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BLOCK_R</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># Block size for rows (Q blocks)</span>
<span class="n">BLOCK_C</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># Block size for columns (KV blocks)</span>
<span class="n">NUM_WARPS</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Number of warps per block</span>
<span class="n">NUM_STAGES</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Number of pipeline stages</span>
<span class="n">INTERPRET_MODE</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Set True for CPU debugging, False for GPU execution</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">pallas</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental.pallas</span><span class="w"> </span><span class="kn">import</span> <span class="n">triton</span> <span class="k">as</span> <span class="n">plgpu</span>

<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_fwd_kernel</span><span class="p">(</span><span class="n">q_ref</span><span class="p">,</span> <span class="n">k_ref</span><span class="p">,</span> <span class="n">v_ref</span><span class="p">,</span> <span class="n">o_ref</span><span class="p">,</span> <span class="n">logsumexp_ref</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">num_k_blocks</span><span class="p">):</span>
    <span class="n">q_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">q_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
    <span class="n">o_reg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">q_reg</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># float32 accumulator</span>
    <span class="n">max_reg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">BLOCK_R</span><span class="p">,),</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">l_reg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BLOCK_R</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="n">max_reg</span><span class="p">,</span> <span class="n">l_reg</span><span class="p">,</span> <span class="n">o_reg</span> <span class="o">=</span> <span class="n">args</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dslice</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_C</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">)</span>
        <span class="n">k_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">v_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:])</span> 
        
        <span class="n">s_blk</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q_reg</span><span class="p">,</span> <span class="n">k_blk</span><span class="p">,</span> <span class="n">trans_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># float32 output</span>
        
        <span class="c1"># Softmax math in float32</span>
        <span class="n">max_blk</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_reg</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">s_blk</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">s_blk</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s_blk</span> <span class="o">-</span> <span class="n">max_blk</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="n">l_blk</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s_blk</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">o_blk</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s_blk</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">v_blk</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">v_blk</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">(</span><span class="n">max_blk</span><span class="p">,</span> 
                <span class="n">l_reg</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">max_reg</span> <span class="o">-</span> <span class="n">max_blk</span><span class="p">)</span> <span class="o">+</span> <span class="n">l_blk</span><span class="p">,</span> 
                <span class="n">o_reg</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">max_reg</span> <span class="o">-</span> <span class="n">max_blk</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">o_blk</span><span class="p">)</span>

    <span class="n">max_reg</span><span class="p">,</span> <span class="n">l_reg</span><span class="p">,</span> <span class="n">o_reg</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_k_blocks</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="p">(</span><span class="n">max_reg</span><span class="p">,</span> <span class="n">l_reg</span><span class="p">,</span> <span class="n">o_reg</span><span class="p">))</span>
    <span class="n">logsumexp_reg</span> <span class="o">=</span> <span class="n">max_reg</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">l_reg</span><span class="p">)</span>
    <span class="n">o_reg</span> <span class="o">=</span> <span class="n">o_reg</span> <span class="o">/</span> <span class="n">l_reg</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
    
    <span class="c1"># Store as bf16</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">o_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">o_reg</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">o_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">logsumexp_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">logsumexp_reg</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">logsumexp_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="forward-kernel-wrapper">
<h3>Forward Kernel Wrapper<a class="headerlink" href="#forward-kernel-wrapper" title="Link to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">flash_attention_fwd</span></code> function orchestrates the forward pass by launching the kernel across a 2D grid. The grid dimensions are <span class="math notranslate nohighlight">\((B\cdot H, T/B_R)\)</span>, where the first axis handles batch-head parallelism and the second axis handles sequence parallelism across query blocks. Each grid point processes one query tile from <span class="math notranslate nohighlight">\(Q\)</span> and produces the corresponding output tile.</p>
<p>Input block specifications use different tiling strategies for <span class="math notranslate nohighlight">\(Q\)</span> versus <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span>. The <span class="math notranslate nohighlight">\(Q\)</span> tensor is blocked into <span class="math notranslate nohighlight">\((1, B_R, C)\)</span> tiles. This blocking enables parallel processing across the sequence dimension. In contrast, <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are loaded in full <span class="math notranslate nohighlight">\((1, T, C)\)</span> blocks per batch-head. This design allows each query block to attend to all keys and values.</p>
<p>Compiler parameters configure the GPU execution strategy. The <code class="docutils literal notranslate"><span class="pre">num_warps=4</span></code> setting divides each thread block into 4 warps of 32 threads each, matching the <span class="math notranslate nohighlight">\(64 \times 64\)</span> tile size for efficient memory access patterns. The <code class="docutils literal notranslate"><span class="pre">num_stages=2</span></code> parameter enables software pipelining, which overlaps memory loads with computation by prefetching the next <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> blocks while computing on the current block. This pipelining reduces memory latency impact and improves throughput.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_fwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Flash attention forward pass.&quot;&quot;&quot;</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">B_flat</span> <span class="o">=</span> <span class="n">B</span><span class="o">*</span><span class="n">H</span>
    <span class="n">q_flat</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">k_flat</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">v_flat</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">num_k_blocks</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">B_flat</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">))</span>

    <span class="n">out_flat</span><span class="p">,</span> <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">flash_attention_fwd_kernel</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">num_k_blocks</span><span class="o">=</span><span class="n">num_k_blocks</span><span class="p">),</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="p">[</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">q_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">q_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">B</span><span class="o">*</span><span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">q_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="p">],</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
        <span class="p">],</span>
        <span class="n">interpret</span><span class="o">=</span><span class="n">INTERPRET_MODE</span><span class="p">,</span>
        <span class="n">compiler_params</span><span class="o">=</span><span class="n">plgpu</span><span class="o">.</span><span class="n">CompilerParams</span><span class="p">(</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="n">NUM_WARPS</span><span class="p">,</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="n">NUM_STAGES</span>
        <span class="p">)</span>
    <span class="p">)(</span><span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">logsumexp</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="backward-pass">
<h2>Backward Pass<a class="headerlink" href="#backward-pass" title="Link to this heading">¶</a></h2>
<p>The backward pass computes gradients <span class="math notranslate nohighlight">\(dQ\)</span>, <span class="math notranslate nohighlight">\(dK\)</span>, <span class="math notranslate nohighlight">\(dV\)</span> given the upstream gradient <span class="math notranslate nohighlight">\(dO\)</span>. Just as the forward pass avoids materializing the full <span class="math notranslate nohighlight">\((T, T)\)</span> attention matrix, the backward pass avoids materializing the full Jacobians. Instead, each gradient is computed using blocked matrix multiplication, processing small tiles that fit in SMEM.</p>
<p>The key insight is that we can recompute the attention weights <span class="math notranslate nohighlight">\(P\)</span> from the stored logsumexp values rather than storing them:</p>
<div class="math notranslate nohighlight">
\[P = \exp(QK^T / \sqrt{d} - \text{logsumexp})\]</div>
<p>The gradient formulas derived from the chain rule are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D = \text{rowsum}(O \odot dO)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dP = dO \cdot V^T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dS = P \odot (dP - D) / \sqrt{d}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dQ = dS \cdot K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dK = dS^T \cdot Q\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dV = P^T \cdot dO\)</span></p></li>
</ul>
<section id="three-separate-kernels">
<h3>Three Separate Kernels<a class="headerlink" href="#three-separate-kernels" title="Link to this heading">¶</a></h3>
<p>We use three separate kernels because <span class="math notranslate nohighlight">\(dQ\)</span> has a different parallelism structure than <span class="math notranslate nohighlight">\(dK\)</span> and <span class="math notranslate nohighlight">\(dV\)</span>. Pallas does not support atomic operations, so we cannot have multiple thread blocks accumulating into the same output location. This forces us to choose the outer loop dimension carefully for each gradient.</p>
<p>Consider computing <span class="math notranslate nohighlight">\(dK\)</span>. Each row of <span class="math notranslate nohighlight">\(dK\)</span> depends on all rows of <span class="math notranslate nohighlight">\(Q\)</span> through the formula <span class="math notranslate nohighlight">\(dK = dS^T Q\)</span>. If we parallelize over KV blocks in the outer loop, each thread block owns a distinct tile of <span class="math notranslate nohighlight">\(K\)</span> and can independently accumulate contributions from all <span class="math notranslate nohighlight">\(Q\)</span> blocks in the inner loop. The rows of <span class="math notranslate nohighlight">\(Q\)</span> are embarrassingly parallel with respect to their contributions to a single <span class="math notranslate nohighlight">\(K\)</span> tile. No atomics are needed because each output tile is written by exactly one thread block.</p>
<p>The same logic applies to <span class="math notranslate nohighlight">\(dV\)</span>. Each row of <span class="math notranslate nohighlight">\(dV\)</span> depends on all rows of <span class="math notranslate nohighlight">\(dO\)</span> through <span class="math notranslate nohighlight">\(dV = P^T dO\)</span>. Parallelizing over KV blocks in the outer loop lets each thread block accumulate into its own <span class="math notranslate nohighlight">\(dV\)</span> tile.</p>
<p>Computing <span class="math notranslate nohighlight">\(dQ\)</span> requires the opposite structure. Each row of <span class="math notranslate nohighlight">\(dQ\)</span> depends on all rows of <span class="math notranslate nohighlight">\(K\)</span> through <span class="math notranslate nohighlight">\(dQ = dS \cdot K\)</span>. Here we must parallelize over Q blocks in the outer loop, iterating over KV blocks in the inner loop. The rows of <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are embarrassingly parallel with respect to their contributions to a single <span class="math notranslate nohighlight">\(Q\)</span> tile.</p>
<p>This asymmetry is why we cannot fuse <span class="math notranslate nohighlight">\(dQ\)</span>, <span class="math notranslate nohighlight">\(dK\)</span>, and <span class="math notranslate nohighlight">\(dV\)</span> into a single kernel without atomics. The <span class="math notranslate nohighlight">\(dK/dV\)</span> kernel has its outer loop over KV blocks with Q blocks in the inner loop. The <span class="math notranslate nohighlight">\(dQ\)</span> kernel has its outer loop over Q blocks with KV blocks in the inner loop. Attempting to compute all three in one kernel would require atomic additions, which Pallas does not support.</p>
<p>The preprocess kernel computes <span class="math notranslate nohighlight">\(D = \text{rowsum}(O \odot dO)\)</span> as a separate pass. This value is used by both backward kernels and is trivially parallel across sequence positions.</p>
</section>
<section id="preprocess-kernel">
<h3>Preprocess Kernel<a class="headerlink" href="#preprocess-kernel" title="Link to this heading">¶</a></h3>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_preprocess_kernel</span><span class="p">(</span><span class="n">o_ref</span><span class="p">,</span> <span class="n">do_ref</span><span class="p">,</span> <span class="n">d_ref</span><span class="p">):</span>
    <span class="n">o_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">o_ref</span><span class="p">)</span>  
    <span class="n">do_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">do_ref</span><span class="p">)</span> 
    <span class="n">d_reg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">o_reg</span> <span class="o">*</span> <span class="n">do_reg</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">d_ref</span><span class="p">,</span> <span class="n">d_reg</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">d_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_preprocess</span><span class="p">(</span><span class="n">o_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">):</span>
    <span class="n">B_flat</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">o_flat</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">B_flat</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">))</span>

    <span class="n">d_flat</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
        <span class="n">flash_attention_bwd_preprocess_kernel</span><span class="p">,</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">B_flat</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">o_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> 
        <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
        <span class="p">],</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">)),</span>
        <span class="n">interpret</span><span class="o">=</span><span class="n">INTERPRET_MODE</span><span class="p">,</span>
        <span class="n">compiler_params</span><span class="o">=</span><span class="n">plgpu</span><span class="o">.</span><span class="n">CompilerParams</span><span class="p">(</span><span class="n">num_warps</span><span class="o">=</span><span class="n">NUM_WARPS</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="n">NUM_STAGES</span><span class="p">)</span>
    <span class="p">)(</span><span class="n">o_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d_flat</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dk-dv-kernel">
<h3><span class="math notranslate nohighlight">\(dK, dV\)</span> Kernel<a class="headerlink" href="#dk-dv-kernel" title="Link to this heading">¶</a></h3>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_dkv_kernel</span><span class="p">(</span>
    <span class="n">q_ref</span><span class="p">,</span> <span class="n">k_ref</span><span class="p">,</span> <span class="n">v_ref</span><span class="p">,</span> <span class="n">do_ref</span><span class="p">,</span> <span class="n">logsumexp_ref</span><span class="p">,</span> <span class="n">d_ref</span><span class="p">,</span>
    <span class="n">dk_ref</span><span class="p">,</span> <span class="n">dv_ref</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">num_q_blocks</span>
<span class="p">):</span>
    <span class="n">k_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> 
    <span class="n">v_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> 

    <span class="n">dk_acc</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dk_ref</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dv_acc</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dv_ref</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">carry</span><span class="p">):</span>
        <span class="n">dk_acc</span><span class="p">,</span> <span class="n">dv_acc</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dslice</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">)</span>
        <span class="n">q_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">q_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">do_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">do_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">logsumexp_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">logsumexp_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">])</span>
        <span class="n">d_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">d_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">])</span>          
        
        <span class="n">s_blk</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q_blk</span><span class="p">,</span> <span class="n">k_reg</span><span class="p">,</span> <span class="n">trans_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span> 
        <span class="n">p_blk</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s_blk</span> <span class="o">-</span> <span class="n">logsumexp_blk</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>  
        
        <span class="n">dp_blk</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">do_blk</span><span class="p">,</span> <span class="n">v_reg</span><span class="p">,</span> <span class="n">trans_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># float32</span>
        <span class="n">ds_blk</span> <span class="o">=</span> <span class="n">p_blk</span> <span class="o">*</span> <span class="p">(</span><span class="n">dp_blk</span> <span class="o">-</span> <span class="n">d_blk</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># float32</span>
        
        <span class="n">dv_acc</span> <span class="o">+=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p_blk</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">do_blk</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">do_blk</span><span class="p">,</span> <span class="n">trans_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">dk_acc</span> <span class="o">+=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ds_blk</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">q_blk</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">q_blk</span><span class="p">,</span> <span class="n">trans_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dk_acc</span><span class="p">,</span> <span class="n">dv_acc</span>
        
    <span class="n">dk_acc</span><span class="p">,</span> <span class="n">dv_acc</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_q_blocks</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="p">(</span><span class="n">dk_acc</span><span class="p">,</span> <span class="n">dv_acc</span><span class="p">))</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">dk_ref</span><span class="p">,</span> <span class="n">dk_acc</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dk_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">dv_ref</span><span class="p">,</span> <span class="n">dv_acc</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dv_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_dkv</span><span class="p">(</span><span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">,</span> <span class="n">logsumexp_flat</span><span class="p">,</span> <span class="n">d_flat</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">B_flat</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">q_flat</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_q_blocks</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">B_flat</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">))</span>

    <span class="n">dk_flat</span><span class="p">,</span> <span class="n">dv_flat</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">flash_attention_bwd_dkv_kernel</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">num_q_blocks</span><span class="o">=</span><span class="n">num_q_blocks</span><span class="p">),</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="p">[</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">k_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">v_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>       <span class="c1"># q (full)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># k (blocked)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># v (blocked)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>       <span class="c1"># do (full)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>             <span class="c1"># logsumexp (full)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>             <span class="c1"># d (full)</span>
        <span class="p">],</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
        <span class="p">],</span>
        <span class="n">interpret</span><span class="o">=</span><span class="n">INTERPRET_MODE</span><span class="p">,</span>
        <span class="n">compiler_params</span><span class="o">=</span><span class="n">plgpu</span><span class="o">.</span><span class="n">CompilerParams</span><span class="p">(</span><span class="n">num_warps</span><span class="o">=</span><span class="n">NUM_WARPS</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="n">NUM_STAGES</span><span class="p">)</span>
    <span class="p">)(</span><span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">,</span> <span class="n">logsumexp_flat</span><span class="p">,</span> <span class="n">d_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dk_flat</span><span class="p">,</span> <span class="n">dv_flat</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dq-kernel">
<h3><span class="math notranslate nohighlight">\(dQ\)</span> Kernel<a class="headerlink" href="#dq-kernel" title="Link to this heading">¶</a></h3>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_dq_kernel</span><span class="p">(</span>
    <span class="n">q_ref</span><span class="p">,</span> <span class="n">k_ref</span><span class="p">,</span> <span class="n">v_ref</span><span class="p">,</span> <span class="n">do_ref</span><span class="p">,</span> <span class="n">logsumexp_ref</span><span class="p">,</span> <span class="n">d_ref</span><span class="p">,</span>
    <span class="n">dq_ref</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">num_kv_blocks</span>
<span class="p">):</span>
    <span class="n">q_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">q_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>              <span class="c1"># bf16</span>
    <span class="n">do_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">do_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>            <span class="c1"># bf16</span>
    <span class="n">logsumexp_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">logsumexp_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span> <span class="c1"># bf16</span>
    <span class="n">d_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">d_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>                 <span class="c1"># bf16</span>
    <span class="n">dq_acc</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dq_ref</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># float32 accumulator</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">carry</span><span class="p">):</span>
        <span class="n">dq_acc</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dslice</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_C</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">)</span>
        <span class="n">k_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>  <span class="c1"># bf16</span>
        <span class="n">v_blk</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>  <span class="c1"># bf16</span>
        
        <span class="n">s_blk</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q_reg</span><span class="p">,</span> <span class="n">k_blk</span><span class="p">,</span> <span class="n">trans_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># float32</span>
        <span class="n">p_blk</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s_blk</span> <span class="o">-</span> <span class="n">logsumexp_reg</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>   <span class="c1"># float32</span>
        
        <span class="n">dp_blk</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">do_reg</span><span class="p">,</span> <span class="n">v_blk</span><span class="p">,</span> <span class="n">trans_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># float32</span>
        <span class="n">ds_blk</span> <span class="o">=</span> <span class="n">p_blk</span> <span class="o">*</span> <span class="p">(</span><span class="n">dp_blk</span> <span class="o">-</span> <span class="n">d_reg</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># float32</span>
        
        <span class="n">dq_acc</span> <span class="o">+=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ds_blk</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">k_blk</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">k_blk</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dq_acc</span>

    <span class="n">dq_acc</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_kv_blocks</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">dq_acc</span><span class="p">)</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">dq_ref</span><span class="p">,</span> <span class="n">dq_acc</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dq_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_dq</span><span class="p">(</span><span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">,</span> <span class="n">logsumexp_flat</span><span class="p">,</span> <span class="n">d_flat</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">B_flat</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">q_flat</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_kv_blocks</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_C</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">B_flat</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">))</span>

    <span class="n">dq_flat</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">flash_attention_bwd_dq_kernel</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">num_kv_blocks</span><span class="o">=</span><span class="n">num_kv_blocks</span><span class="p">),</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">q_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">q_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># q (blocked)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>       <span class="c1"># k (full)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>       <span class="c1"># v (full)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># do (blocked)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">)),</span>       <span class="c1"># logsumexp (blocked)</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">)),</span>       <span class="c1"># d (blocked)</span>
        <span class="p">],</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">BLOCK_R</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
        <span class="n">interpret</span><span class="o">=</span><span class="n">INTERPRET_MODE</span><span class="p">,</span>
        <span class="n">compiler_params</span><span class="o">=</span><span class="n">plgpu</span><span class="o">.</span><span class="n">CompilerParams</span><span class="p">(</span><span class="n">num_warps</span><span class="o">=</span><span class="n">NUM_WARPS</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="n">NUM_STAGES</span><span class="p">)</span>
    <span class="p">)(</span><span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">,</span> <span class="n">logsumexp_flat</span><span class="p">,</span> <span class="n">d_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dq_flat</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">flash_attention_bwd</span></code> function will call each backward-pass kernel sequentially.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">logsumexp</span><span class="p">,</span> <span class="n">do</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

    <span class="n">q_flat</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">k_flat</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">v_flat</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">o_flat</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">do_flat</span> <span class="o">=</span> <span class="n">do</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">logsumexp_flat</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

    <span class="c1"># Kernel 1: Preprocess - compute D = rowsum(O * dO)</span>
    <span class="n">d_flat</span> <span class="o">=</span> <span class="n">flash_attention_bwd_preprocess</span><span class="p">(</span><span class="n">o_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">)</span>

    <span class="c1"># Kernel 2: Compute dK, dV</span>
    <span class="n">dk_flat</span><span class="p">,</span> <span class="n">dv_flat</span> <span class="o">=</span> <span class="n">flash_attention_bwd_dkv</span><span class="p">(</span>
        <span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">,</span> <span class="n">logsumexp_flat</span><span class="p">,</span> <span class="n">d_flat</span><span class="p">,</span> <span class="n">scale</span>
    <span class="p">)</span>

    <span class="c1"># Kernel 3: Compute dQ</span>
    <span class="n">dq_flat</span> <span class="o">=</span> <span class="n">flash_attention_bwd_dq</span><span class="p">(</span>
        <span class="n">q_flat</span><span class="p">,</span> <span class="n">k_flat</span><span class="p">,</span> <span class="n">v_flat</span><span class="p">,</span> <span class="n">do_flat</span><span class="p">,</span> <span class="n">logsumexp_flat</span><span class="p">,</span> <span class="n">d_flat</span><span class="p">,</span> <span class="n">scale</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">dq_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
        <span class="n">dk_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
        <span class="n">dv_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="register-the-forward-and-backward-pass-with-jax">
<h3>Register the forward and backward pass with Jax<a class="headerlink" href="#register-the-forward-and-backward-pass-with-jax" title="Link to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">&#64;jax.custom_vjp</span></code> decorator creates a custom automatic differentiation rule. The <code class="docutils literal notranslate"><span class="pre">flash_attention</span></code> function is the user-facing API that executes during the forward pass. The <code class="docutils literal notranslate"><span class="pre">flash_attention_fwd_rule</span></code> function is only used internally by JAX’s autograd system. When JAX needs to compute gradients, it calls <code class="docutils literal notranslate"><span class="pre">flash_attention_fwd_rule</span></code> to get both the output and the saved tensors needed for backpropagation, then calls <code class="docutils literal notranslate"><span class="pre">flash_attention_bwd_rule</span></code> with those saved tensors and the upstream gradient. The decorator tells JAX that <code class="docutils literal notranslate"><span class="pre">flash_attention</span></code> has a custom gradient rule, preventing it from attempting to automatically differentiate through the forward rule itself. The <code class="docutils literal notranslate"><span class="pre">flash_attention.defvjp</span></code> method registers these forward and backward rules with JAX’s autograd system.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">custom_vjp</span>
<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">flash_attention_fwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">o</span>

<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_fwd_rule</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">flash_attention_fwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">o</span><span class="p">,</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">logsumexp</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">flash_attention_bwd_rule</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">do</span><span class="p">):</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">dq</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span> <span class="o">=</span> <span class="n">flash_attention_bwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">logsumexp</span><span class="p">,</span> <span class="n">do</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dq</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span>

<span class="n">flash_attention</span><span class="o">.</span><span class="n">defvjp</span><span class="p">(</span><span class="n">flash_attention_fwd_rule</span><span class="p">,</span> <span class="n">flash_attention_bwd_rule</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="correctness-check">
<h2>Correctness Check<a class="headerlink" href="#correctness-check" title="Link to this heading">¶</a></h2>
<p>We verify correctness by comparing our flash attention implementation against the reference (materialized) attention for both forward and backward passes.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o_flash</span> <span class="o">=</span> <span class="n">flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flash attention output shape: </span><span class="si">{</span><span class="n">o_flash</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Forward pass matches: </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">o_flash</span><span class="p">,</span><span class="w"> </span><span class="n">o_ref</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">loss_flash</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="n">do</span><span class="p">)</span>

<span class="n">dq_flash</span><span class="p">,</span> <span class="n">dk_flash</span><span class="p">,</span> <span class="n">dv_flash</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_flash</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flash attention gradient shapes: dq=</span><span class="si">{</span><span class="n">dq_flash</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dk=</span><span class="si">{</span><span class="n">dk_flash</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dv=</span><span class="si">{</span><span class="n">dv_flash</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dQ matches: </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dq_flash</span><span class="p">,</span><span class="w"> </span><span class="n">dq_ref</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dK matches: </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dk_flash</span><span class="p">,</span><span class="w"> </span><span class="n">dk_ref</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dV matches: </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dv_flash</span><span class="p">,</span><span class="w"> </span><span class="n">dv_ref</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Flash attention output shape: (2, 4, 256, 64)
Forward pass matches: True
Flash attention gradient shapes: dq=(2, 4, 256, 64), dk=(2, 4, 256, 64), dv=(2, 4, 256, 64)
dQ matches: True
dK matches: True
dV matches: True
</pre></div>
</div>
</div>
</div>
</section>
<section id="performance-comparison">
<h2>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">¶</a></h2>
<p>We compare our Pallas flash attention implementation against both <code class="docutils literal notranslate"><span class="pre">jax.nn.dot_product_attention(implementation='cudnn')</span></code> - NVIDIA’s highly optimized implementation as well as our naive implementation.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cudnn_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">q_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">k_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">v_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">impl</span> <span class="o">=</span> <span class="s2">&quot;xla&quot;</span> <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">default_backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="k">else</span> <span class="s2">&quot;cudnn&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dot_product_attention</span><span class="p">(</span><span class="n">q_t</span><span class="p">,</span> <span class="n">k_t</span><span class="p">,</span> <span class="n">v_t</span><span class="p">,</span> <span class="n">implementation</span><span class="o">=</span><span class="n">impl</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="flops-calculation-for-attention">
<h3>FLOPS Calculation for Attention<a class="headerlink" href="#flops-calculation-for-attention" title="Link to this heading">¶</a></h3>
<p>Understanding FLOP counts is essential for interpreting benchmark results correctly. Different attention implementations perform different amounts of arithmetic work, particularly in the backward pass where flash attention trades extra computation for reduced memory traffic. The FLOP counts also feed into the roofline analysis later, where we use arithmetic intensity (FLOPs per byte) to diagnose whether kernels are compute-bound or memory-bound.</p>
<p>For attention with shape <span class="math notranslate nohighlight">\((B, H, T, D)\)</span> where <span class="math notranslate nohighlight">\(B\)</span> is batch size, <span class="math notranslate nohighlight">\(H\)</span> is number of heads, <span class="math notranslate nohighlight">\(T\)</span> is sequence length, and <span class="math notranslate nohighlight">\(D\)</span> is head dimension:</p>
<section id="forward-pass-same-for-all-implementations">
<h4>Forward Pass (same for all implementations)<a class="headerlink" href="#forward-pass-same-for-all-implementations" title="Link to this heading">¶</a></h4>
<p>The forward pass computes <span class="math notranslate nohighlight">\(\text{softmax}(QK^T / \sqrt{d})V\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(QK^T\)</span>: <span class="math notranslate nohighlight">\((T, D) \times (D, T) \rightarrow (T, T)\)</span>. Total: <span class="math notranslate nohighlight">\(2 \cdot B \cdot H \cdot T^2 \cdot D\)</span> FLOPs.</p></li>
<li><p>Softmax: For each row of the <span class="math notranslate nohighlight">\(T \times T\)</span> attention matrix we subtract the max (<span class="math notranslate nohighlight">\(T\)</span> ops), compute exp (<span class="math notranslate nohighlight">\(T\)</span> ops), sum (<span class="math notranslate nohighlight">\(T\)</span> ops), and divide (<span class="math notranslate nohighlight">\(T\)</span> ops), giving approximately <span class="math notranslate nohighlight">\(5T\)</span> ops per row. Total: <span class="math notranslate nohighlight">\(5 \cdot B \cdot H \cdot T^2\)</span> FLOPs.</p></li>
<li><p><span class="math notranslate nohighlight">\(PV\)</span>: <span class="math notranslate nohighlight">\((T, T) \times (T, D) \rightarrow (T, D)\)</span>. Total: <span class="math notranslate nohighlight">\(2 \cdot B \cdot H \cdot T^2 \cdot D\)</span> FLOPs.</p></li>
</ol>
<p>Total Forward FLOPs: <span class="math notranslate nohighlight">\(4 \cdot B \cdot H \cdot T^2 \cdot D + 5 \cdot B \cdot H \cdot T^2\)</span></p>
<p>For large <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(D\)</span>, the <span class="math notranslate nohighlight">\(4 \cdot B \cdot H \cdot T^2 \cdot D\)</span> term dominates.</p>
</section>
<section id="backward-pass-varies-by-implementation">
<h4>Backward Pass (varies by implementation)<a class="headerlink" href="#backward-pass-varies-by-implementation" title="Link to this heading">¶</a></h4>
<p>The backward pass is where naive and flash attention differ significantly.</p>
<p><strong>Naive Attention Backward</strong> stores the full attention matrix and computes four matmuls: <span class="math notranslate nohighlight">\(dV = P^T dO\)</span>, <span class="math notranslate nohighlight">\(dP = dO V^T\)</span>, <span class="math notranslate nohighlight">\(dQ = dS K\)</span>, and <span class="math notranslate nohighlight">\(dK = dS^T Q\)</span>. Each costs <span class="math notranslate nohighlight">\(2 \cdot T^2 \cdot D\)</span> FLOPs. Total: <span class="math notranslate nohighlight">\(8 \cdot B \cdot H \cdot T^2 \cdot D\)</span>.</p>
<p><strong>Pallas Flash Attention Backward</strong> recomputes the attention matrix twice. The <span class="math notranslate nohighlight">\(dK, dV\)</span> kernel recomputes <span class="math notranslate nohighlight">\(S = QK^T\)</span>, then computes <span class="math notranslate nohighlight">\(dP\)</span>, <span class="math notranslate nohighlight">\(dV\)</span>, <span class="math notranslate nohighlight">\(dK\)</span> (4 matmuls). The <span class="math notranslate nohighlight">\(dQ\)</span> kernel recomputes <span class="math notranslate nohighlight">\(S = QK^T\)</span>, then computes <span class="math notranslate nohighlight">\(dP\)</span>, <span class="math notranslate nohighlight">\(dQ\)</span> (3 matmuls). Total: <span class="math notranslate nohighlight">\(14 \cdot B \cdot H \cdot T^2 \cdot D\)</span>.</p>
<p><strong>cuDNN Flash Attention Backward</strong> uses an optimized fused kernel that recomputes <span class="math notranslate nohighlight">\(S\)</span> once and computes <span class="math notranslate nohighlight">\(dQ\)</span>, <span class="math notranslate nohighlight">\(dK\)</span>, <span class="math notranslate nohighlight">\(dV\)</span> together. Total: approximately <span class="math notranslate nohighlight">\(10 \cdot B \cdot H \cdot T^2 \cdot D\)</span>.</p>
</section>
</section>
<section id="memory-transfer-bytes-calculation">
<h3>Memory Transfer (Bytes) Calculation<a class="headerlink" href="#memory-transfer-bytes-calculation" title="Link to this heading">¶</a></h3>
<p>The key insight of flash attention is reducing memory traffic, not FLOPs. This is where the implementations differ most dramatically.</p>
<section id="forward-pass-memory-traffic">
<h4>Forward Pass Memory Traffic<a class="headerlink" href="#forward-pass-memory-traffic" title="Link to this heading">¶</a></h4>
<p><strong>Naive MHA Forward</strong> materializes the full attention matrix. It reads <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> for <span class="math notranslate nohighlight">\(3 \cdot B \cdot H \cdot T \cdot D \cdot b\)</span> bytes, writes the attention matrix <span class="math notranslate nohighlight">\(P\)</span> for <span class="math notranslate nohighlight">\(B \cdot H \cdot T^2 \cdot b\)</span> bytes, and writes output <span class="math notranslate nohighlight">\(O\)</span> for <span class="math notranslate nohighlight">\(B \cdot H \cdot T \cdot D \cdot b\)</span> bytes, where <span class="math notranslate nohighlight">\(b\)</span> is the bytes per element. The <span class="math notranslate nohighlight">\(T^2\)</span> term in the attention matrix dominates.</p>
<p><strong>Flash Attention Forward</strong> avoids materializing the attention matrix. It reads <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span> for <span class="math notranslate nohighlight">\(3 \cdot B \cdot H \cdot T \cdot D \cdot b\)</span> bytes, writes only the logsumexp values for <span class="math notranslate nohighlight">\(B \cdot H \cdot T \cdot b\)</span> bytes, and writes output <span class="math notranslate nohighlight">\(O\)</span> for <span class="math notranslate nohighlight">\(B \cdot H \cdot T \cdot D \cdot b\)</span> bytes.</p>
<p>The difference is <span class="math notranslate nohighlight">\(O(T^2)\)</span> versus <span class="math notranslate nohighlight">\(O(T)\)</span>. For sequence length <span class="math notranslate nohighlight">\(T = 1024\)</span>, the attention matrix requires <span class="math notranslate nohighlight">\(T^2 = 1M\)</span> elements per head, while logsumexp requires only <span class="math notranslate nohighlight">\(T = 1K\)</span> elements.</p>
</section>
<section id="backward-pass-memory-traffic">
<h4>Backward Pass Memory Traffic<a class="headerlink" href="#backward-pass-memory-traffic" title="Link to this heading">¶</a></h4>
<p><strong>Naive MHA Backward</strong> reads <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(O\)</span>, <span class="math notranslate nohighlight">\(dO\)</span> for <span class="math notranslate nohighlight">\(5 \cdot B \cdot H \cdot T \cdot D \cdot b\)</span> bytes, reads the stored attention matrix for <span class="math notranslate nohighlight">\(B \cdot H \cdot T^2 \cdot b\)</span> bytes, and writes <span class="math notranslate nohighlight">\(dQ\)</span>, <span class="math notranslate nohighlight">\(dK\)</span>, <span class="math notranslate nohighlight">\(dV\)</span> for <span class="math notranslate nohighlight">\(3 \cdot B \cdot H \cdot T \cdot D \cdot b\)</span> bytes.</p>
<p><strong>Flash Attention Backward</strong> reads <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(O\)</span>, <span class="math notranslate nohighlight">\(dO\)</span> for <span class="math notranslate nohighlight">\(5 \cdot B \cdot H \cdot T \cdot D \cdot b\)</span> bytes, reads the logsumexp values for <span class="math notranslate nohighlight">\(B \cdot H \cdot T \cdot b\)</span> bytes, and writes <span class="math notranslate nohighlight">\(dQ\)</span>, <span class="math notranslate nohighlight">\(dK\)</span>, <span class="math notranslate nohighlight">\(dV\)</span> for <span class="math notranslate nohighlight">\(3 \cdot B \cdot H \cdot T \cdot D \cdot b\)</span> bytes.</p>
<div class="cell tag_skip-execution tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-darkgrid&#39;</span><span class="p">)</span>

<span class="c1"># GPU specifications for RTX 4000 Ada</span>
<span class="n">GPU_SPECS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;NVIDIA RTX 4000 Ada Generation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;peak_compute_tflops&quot;</span><span class="p">:</span> <span class="mf">26.7</span><span class="p">,</span>       <span class="c1"># FP32 CUDA cores</span>
    <span class="s2">&quot;peak_compute_tflops_tc&quot;</span><span class="p">:</span> <span class="mf">106.91</span><span class="p">,</span>  <span class="c1"># BF16 Tensor cores</span>
    <span class="s2">&quot;peak_bandwidth_gb_s&quot;</span><span class="p">:</span> <span class="mf">360.0</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># FLOP calculations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">calculate_flops_fwd</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward: Q@K^T + softmax + P@V&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_flops_bwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Naive backward: 4 matmuls without recomputation&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_flops_bwd_pallas</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pallas backward: recomputes attention twice (dKV + dQ kernels)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">14</span> <span class="o">*</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_flops_bwd_cudnn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;cuDNN backward: optimized single recompute&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span>

<span class="c1"># Byte transfer calculations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">calculate_bytes_fwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Naive forward: materializes T×T attention matrix&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span> <span class="o">*</span> <span class="n">bytes_per_elem</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_bytes_fwd_flash</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Flash forward: only stores logsumexp (T elements, not T×T)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span> <span class="o">*</span> <span class="n">bytes_per_elem</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_bytes_bwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Naive backward: reads attention matrix&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">bytes_per_elem</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_bytes_bwd_flash</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Flash backward: reads logsumexp instead of attention matrix&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">+</span> <span class="n">B</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">bytes_per_elem</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_config</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark all implementations for a single configuration.&quot;&quot;&quot;</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">do</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    
    <span class="n">bytes_per_elem</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">]</span> <span class="k">else</span> <span class="mi">4</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_bench</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">warmup</span><span class="p">):</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">(</span><span class="n">fn</span><span class="p">())</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
            <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">(</span><span class="n">fn</span><span class="p">())</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
    
    <span class="c1"># Forward benchmarks</span>
    <span class="n">naive_fwd</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">naive_attention</span><span class="p">)</span>
    <span class="n">flash_fwd</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">flash_attention</span><span class="p">)</span>
    <span class="n">cudnn_fwd</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">cudnn_attention</span><span class="p">)</span>
    
    <span class="n">naive_fwd_time</span> <span class="o">=</span> <span class="n">_bench</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">naive_fwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="n">flash_fwd_time</span> <span class="o">=</span> <span class="n">_bench</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">flash_fwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="n">cudnn_fwd_time</span> <span class="o">=</span> <span class="n">_bench</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">cudnn_fwd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    
    <span class="c1"># Backward benchmarks</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">naive_vjp</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">naive_attention</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">flash_vjp</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">flash_attention</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">cudnn_vjp</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">cudnn_attention</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    
    <span class="n">naive_bwd_time</span> <span class="o">=</span> <span class="n">_bench</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">naive_vjp</span><span class="p">(</span><span class="n">do</span><span class="p">))</span>
    <span class="n">flash_bwd_time</span> <span class="o">=</span> <span class="n">_bench</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">flash_vjp</span><span class="p">(</span><span class="n">do</span><span class="p">))</span>
    <span class="n">cudnn_bwd_time</span> <span class="o">=</span> <span class="n">_bench</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">cudnn_vjp</span><span class="p">(</span><span class="n">do</span><span class="p">))</span>
    
    <span class="c1"># Calculate metrics (used for roofline analysis later)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">calc_metrics</span><span class="p">(</span><span class="n">time_s</span><span class="p">,</span> <span class="n">flops</span><span class="p">,</span> <span class="n">bytes_transferred</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;time_ms&quot;</span><span class="p">:</span> <span class="n">time_s</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span>
            <span class="s2">&quot;gflops_s&quot;</span><span class="p">:</span> <span class="n">flops</span> <span class="o">/</span> <span class="p">(</span><span class="n">time_s</span> <span class="o">*</span> <span class="mf">1e9</span><span class="p">),</span>
            <span class="s2">&quot;ai&quot;</span><span class="p">:</span> <span class="n">flops</span> <span class="o">/</span> <span class="n">bytes_transferred</span><span class="p">,</span>
        <span class="p">}</span>
    
    <span class="n">flops_fwd</span> <span class="o">=</span> <span class="n">calculate_flops_fwd</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;naive&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;fwd&quot;</span><span class="p">:</span> <span class="n">calc_metrics</span><span class="p">(</span><span class="n">naive_fwd_time</span><span class="p">,</span> <span class="n">flops_fwd</span><span class="p">,</span> <span class="n">calculate_bytes_fwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="p">)),</span>
            <span class="s2">&quot;bwd&quot;</span><span class="p">:</span> <span class="n">calc_metrics</span><span class="p">(</span><span class="n">naive_bwd_time</span><span class="p">,</span> <span class="n">calculate_flops_bwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">calculate_bytes_bwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="p">)),</span>
        <span class="p">},</span>
        <span class="s2">&quot;flash&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;fwd&quot;</span><span class="p">:</span> <span class="n">calc_metrics</span><span class="p">(</span><span class="n">flash_fwd_time</span><span class="p">,</span> <span class="n">flops_fwd</span><span class="p">,</span> <span class="n">calculate_bytes_fwd_flash</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="p">)),</span>
            <span class="s2">&quot;bwd&quot;</span><span class="p">:</span> <span class="n">calc_metrics</span><span class="p">(</span><span class="n">flash_bwd_time</span><span class="p">,</span> <span class="n">calculate_flops_bwd_pallas</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">calculate_bytes_bwd_flash</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="p">)),</span>
        <span class="p">},</span>
        <span class="s2">&quot;cudnn&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;fwd&quot;</span><span class="p">:</span> <span class="n">calc_metrics</span><span class="p">(</span><span class="n">cudnn_fwd_time</span><span class="p">,</span> <span class="n">flops_fwd</span><span class="p">,</span> <span class="n">calculate_bytes_fwd_flash</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="p">)),</span>
            <span class="s2">&quot;bwd&quot;</span><span class="p">:</span> <span class="n">calc_metrics</span><span class="p">(</span><span class="n">cudnn_bwd_time</span><span class="p">,</span> <span class="n">calculate_flops_bwd_cudnn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">calculate_bytes_bwd_flash</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bytes_per_elem</span><span class="p">)),</span>
        <span class="p">},</span>
    <span class="p">}</span>

<span class="c1"># Run benchmarks</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Backend: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">default_backend</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">default_backend</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;gpu&quot;</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: Running on CPU. Set INTERPRET_MODE=True or use GPU for accurate benchmarks.&quot;</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">64</span>
<span class="n">seq_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sequence_lengths&quot;</span><span class="p">:</span> <span class="n">seq_lengths</span><span class="p">,</span> <span class="s2">&quot;naive&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;fwd&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;bwd&quot;</span><span class="p">:</span> <span class="p">[]},</span> <span class="s2">&quot;flash&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;fwd&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;bwd&quot;</span><span class="p">:</span> <span class="p">[]},</span> <span class="s2">&quot;cudnn&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;fwd&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;bwd&quot;</span><span class="p">:</span> <span class="p">[]}}</span>

<span class="k">for</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">benchmark_config</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">])</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">])</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">])</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">])</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">])</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">])</span>

<span class="c1"># Print FLOP comparison table (algorithms do different amounts of work!)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FLOP COUNT BY ALGORITHM (GFLOP)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Forward pass: All algorithms perform the same FLOPs&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Backward pass: Flash attention recomputes attention instead of storing it&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;T&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Fwd (all)&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Bwd Naive&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Bwd Pallas&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Bwd cuDNN&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="k">for</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">:</span>
    <span class="n">fwd</span> <span class="o">=</span> <span class="n">calculate_flops_fwd</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="n">bwd_naive</span> <span class="o">=</span> <span class="n">calculate_flops_bwd_naive</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="n">bwd_pallas</span> <span class="o">=</span> <span class="n">calculate_flops_bwd_pallas</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="n">bwd_cudnn</span> <span class="o">=</span> <span class="n">calculate_flops_bwd_cudnn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">T</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">fwd</span><span class="si">:</span><span class="s2">&lt;12.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">bwd_naive</span><span class="si">:</span><span class="s2">&lt;12.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">bwd_pallas</span><span class="si">:</span><span class="s2">&lt;12.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">bwd_cudnn</span><span class="si">:</span><span class="s2">&lt;12.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Print timing tables</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FORWARD PASS TIMING&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;T&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Naive (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Flash (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;cuDNN (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;14</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">T</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;naive&#39;</span><span class="p">][</span><span class="s1">&#39;fwd&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;14.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">][</span><span class="s1">&#39;fwd&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;14.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;cudnn&#39;</span><span class="p">][</span><span class="s1">&#39;fwd&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;14.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BACKWARD PASS TIMING&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;T&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Naive (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Flash (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;cuDNN (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;14</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">T</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;naive&#39;</span><span class="p">][</span><span class="s1">&#39;bwd&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;14.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">][</span><span class="s1">&#39;bwd&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;14.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;cudnn&#39;</span><span class="p">][</span><span class="s1">&#39;bwd&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;14.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Backend: gpu

======================================================================
FLOP COUNT BY ALGORITHM (GFLOP)
======================================================================
Forward pass: All algorithms perform the same FLOPs
Backward pass: Flash attention recomputes attention instead of storing it
----------------------------------------------------------------------
T        Fwd (all)    Bwd Naive    Bwd Pallas   Bwd cuDNN   
----------------------------------------------------------------------
128      0.1          0.3          0.5          0.3         
256      0.5          1.1          1.9          1.3         
512      2.2          4.3          7.5          5.4         
1024     8.8          17.2         30.1         21.5        
2048     35.0         68.7         120.3        85.9        
4096     140.1        274.9        481.0        343.6       

============================================================
FORWARD PASS TIMING
============================================================
T        Naive (ms)     Flash (ms)     cuDNN (ms)    
------------------------------------------------------------
128      0.164          0.151          0.197         
256      0.142          0.097          0.104         
512      0.212          0.153          0.168         
1024     1.048          0.264          0.268         
2048     4.539          0.673          0.833         
4096     17.421         2.376          2.427         

============================================================
BACKWARD PASS TIMING
============================================================
T        Naive (ms)     Flash (ms)     cuDNN (ms)    
------------------------------------------------------------
128      0.457          0.704          0.419         
256      0.433          0.342          0.407         
512      0.588          0.389          0.498         
1024     2.356          0.757          0.847         
2048     8.504          2.389          2.422         
4096     31.159         7.839          6.979         
</pre></div>
</div>
</div>
</div>
<div class="cell tag_skip-execution tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Timing comparison plots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>


<span class="n">seq_lengths</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;sequence_lengths&quot;</span><span class="p">]</span>
<span class="n">naive_fwd_times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;time_ms&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">]]</span>
<span class="n">flash_fwd_times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;time_ms&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">]]</span>
<span class="n">cudnn_fwd_times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;time_ms&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="s2">&quot;fwd&quot;</span><span class="p">]]</span>

<span class="n">naive_bwd_times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;time_ms&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">]]</span>
<span class="n">flash_bwd_times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;time_ms&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">]]</span>
<span class="n">cudnn_bwd_times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;time_ms&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="s2">&quot;bwd&quot;</span><span class="p">]]</span>

<span class="c1"># Forward pass plot</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">naive_fwd_times</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Naive MHA&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">flash_fwd_times</span><span class="p">,</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Flash (Pallas)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">cudnn_fwd_times</span><span class="p">,</span> <span class="s1">&#39;^-&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cuDNN Flash&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length (T)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Forward Pass Timing&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#axes[0].set_yscale(&#39;log&#39;)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">])</span>
<span class="c1">#axes[0].grid(True, alpha=0.3)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

<span class="c1"># Backward pass plot</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">naive_bwd_times</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Naive MHA&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">flash_bwd_times</span><span class="p">,</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Flash (Pallas)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">cudnn_bwd_times</span><span class="p">,</span> <span class="s1">&#39;^-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cuDNN Flash&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length (T)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Backward Pass Timing&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#axes[1].set_yscale(&#39;log&#39;)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">])</span>
<span class="c1">#axes[1].grid(True, alpha=0.3)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/1af08f69856ea4f16ca8c5117284779f7926f20e3031ead8d4aef50d60b31921.png" src="_images/1af08f69856ea4f16ca8c5117284779f7926f20e3031ead8d4aef50d60b31921.png" />
</div>
</div>
<p><strong>Key observations:</strong></p>
<ul class="simple">
<li><p><strong>Forward pass</strong>: Our Pallas implementation matches cuDNN in wall-clock time at large sequence lengths <span class="math notranslate nohighlight">\((T \geq 1024)\)</span>, with both completing in ~2.4ms at <span class="math notranslate nohighlight">\(T=4096\)</span>. Our implementation is marginally faster (2.38ms vs 2.43ms).</p></li>
<li><p><strong>Backward pass</strong>: cuDNN is faster in wall-clock time (7.0ms vs 7.8ms at T=4096), despite our implementation showing higher GFLOP/s. This is because our backward pass does more total FLOPs due to recomputing attention twice.</p></li>
<li><p><strong>Massive speedup over naive</strong>: Both flash implementations are 4-7x faster than naive attention at long sequences, which is the key benefit.</p></li>
</ul>
<p>Wall-clock time is the true measure of performance. GFLOP/s measures hardware utilization, not algorithm efficiency—an algorithm doing more unnecessary work can show higher GFLOP/s while being slower overall.</p>
</section>
</section>
</section>
<section id="roofline-analysis-understanding-performance-bottlenecks">
<h2>Roofline Analysis: Understanding Performance Bottlenecks<a class="headerlink" href="#roofline-analysis-understanding-performance-bottlenecks" title="Link to this heading">¶</a></h2>
<p>The roofline model is a visual framework for understanding whether a kernel is compute-bound or memory-bound. It helps explain why flash attention significantly outperforms naive attention despite doing the same mathematical computation.</p>
<section id="the-roofline-model">
<h3>The Roofline Model<a class="headerlink" href="#the-roofline-model" title="Link to this heading">¶</a></h3>
<p>The roofline model plots Arithmetic Intensity (AI)  against Performance (GFLOP/s):</p>
<ul class="simple">
<li><p>Arithmetic Intensity (AI)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{FLOPs / Bytes transferred}\)</span></p></li>
<li><p>Measures how much computation you do per byte of data moved</p></li>
<li><p>Higher AI means the kernel reuses data more efficiently</p></li>
</ul>
</li>
<li><p>Performance</p>
<ul>
<li><p>Achieved <span class="math notranslate nohighlight">\(\text{FLOP/s}\)</span></p></li>
<li><p>How fast the kernel actually runs</p></li>
</ul>
</li>
</ul>
<p>The “roofline” consists of two lines:</p>
<ul class="simple">
<li><p>Memory Roof (diagonal)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{Performance} = \text{Bandwidth} × \text{AI}\)</span></p></li>
<li><p>When AI is low, performance is limited by how fast you can move data</p></li>
</ul>
</li>
<li><p>Compute Roof (horizontal)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{Performance} = \text{Peak FLOP/s}\)</span></p></li>
<li><p>When AI is high, performance is limited by how fast you can compute</p></li>
</ul>
</li>
</ul>
<p>The intersection is called the ridge point:</p>
<div class="math notranslate nohighlight">
\[
\text{Ridge AI} = \frac{\text{Peak Compute (FLOP/s)}}{\text{Peak Bandwidth (Bytes/s)}}
\]</div>
<p>Kernels with AI left of the ridge are memory-bound; right of the ridge are compute-bound.</p>
<div class="cell tag_skip-execution tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_roofline_plot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">pass_type</span><span class="o">=</span><span class="s2">&quot;fwd&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate roofline plot for forward or backward pass.&quot;&quot;&quot;</span>
    <span class="n">gpu</span> <span class="o">=</span> <span class="n">GPU_SPECS</span>
    <span class="n">pass_name</span> <span class="o">=</span> <span class="s2">&quot;Forward&quot;</span> <span class="k">if</span> <span class="n">pass_type</span> <span class="o">==</span> <span class="s2">&quot;fwd&quot;</span> <span class="k">else</span> <span class="s2">&quot;Backward&quot;</span>
    <span class="n">ridge_ai</span> <span class="o">=</span> <span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;peak_compute_tflops_tc&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;peak_bandwidth_gb_s&quot;</span><span class="p">]</span>
    
    <span class="n">seq_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;sequence_lengths&quot;</span><span class="p">])</span>
    <span class="n">naive_ai</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;ai&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="n">pass_type</span><span class="p">]])</span>
    <span class="n">flash_ai</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;ai&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="n">pass_type</span><span class="p">]])</span>
    <span class="n">cudnn_ai</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;ai&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="n">pass_type</span><span class="p">]])</span>
    <span class="n">naive_perf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;gflops_s&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;naive&quot;</span><span class="p">][</span><span class="n">pass_type</span><span class="p">]])</span>
    <span class="n">flash_perf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;gflops_s&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;flash&quot;</span><span class="p">][</span><span class="n">pass_type</span><span class="p">]])</span>
    <span class="n">cudnn_perf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;gflops_s&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;cudnn&quot;</span><span class="p">][</span><span class="n">pass_type</span><span class="p">]])</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    
    <span class="n">all_ai</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">naive_ai</span><span class="p">,</span> <span class="n">flash_ai</span><span class="p">,</span> <span class="n">cudnn_ai</span><span class="p">])</span>
    <span class="n">ai_min</span><span class="p">,</span> <span class="n">ai_max</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">all_ai</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">ridge_ai</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">all_ai</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">ridge_ai</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">ai_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ai_min</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ai_max</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
    
    <span class="n">memory_roof</span> <span class="o">=</span> <span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;peak_bandwidth_gb_s&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">ai_range</span>
    <span class="n">compute_roof</span> <span class="o">=</span> <span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;peak_compute_tflops_tc&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ai_range</span><span class="p">)</span>
    <span class="n">memory_roof</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">memory_roof</span><span class="p">,</span> <span class="n">compute_roof</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ai_range</span><span class="p">,</span> <span class="n">memory_roof</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Memory roof&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ai_range</span><span class="p">,</span> <span class="n">compute_roof</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;TC roof (</span><span class="si">{</span><span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;peak_compute_tflops_tc&quot;</span><span class="p">]</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1"> TFLOP/s)&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">naive_ai</span><span class="p">,</span> <span class="n">naive_perf</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Naive MHA&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">flash_ai</span><span class="p">,</span> <span class="n">flash_perf</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Flash (Pallas)&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">cudnn_ai</span><span class="p">,</span> <span class="n">cudnn_perf</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cuDNN Flash&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;T=</span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">naive_ai</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">naive_perf</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;T=</span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">flash_ai</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">flash_perf</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">15</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">ridge_ai</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">ridge_ai</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;peak_compute_tflops&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Ridge</span><span class="se">\n</span><span class="s1">AI=</span><span class="si">{</span><span class="n">ridge_ai</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Arithmetic Intensity (FLOPs/byte)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Performance (GFLOP/s)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Roofline Analysis (</span><span class="si">{</span><span class="n">pass_name</span><span class="si">}</span><span class="s1"> Pass, BF16)</span><span class="se">\n</span><span class="si">{</span><span class="n">gpu</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Generate roofline plots</span>
<span class="n">fig_fwd</span> <span class="o">=</span> <span class="n">generate_roofline_plot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="s2">&quot;fwd&quot;</span><span class="p">)</span>
<span class="n">fig_bwd</span> <span class="o">=</span> <span class="n">generate_roofline_plot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="s2">&quot;bwd&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/a093eb382d6edab3cdc2ecb459535d7384b0cfaf8c457fcee0e39bc2c0dde094.png" src="_images/a093eb382d6edab3cdc2ecb459535d7384b0cfaf8c457fcee0e39bc2c0dde094.png" />
<img alt="_images/d9ccf0948add86e0b550f56c76061c21485d6d8472380b2c38715263e5162340.png" src="_images/d9ccf0948add86e0b550f56c76061c21485d6d8472380b2c38715263e5162340.png" />
</div>
</div>
<p>The key insight from the roofline analysis is that flash attention moves both implementations from the memory-bound regime (low AI) to the compute-bound regime (high AI). This explains the dramatic speedup over naive attention. Note that our Pallas implementation shows higher GFLOP/s than cuDNN in the backward pass, but this reflects our higher FLOP count (recomputing attention twice) rather than better performance.</p>
</section>
</section>
<section id="limitations-and-future-work">
<h2>Limitations and Future Work<a class="headerlink" href="#limitations-and-future-work" title="Link to this heading">¶</a></h2>
<section id="performance-achievement">
<h3>Performance Achievement<a class="headerlink" href="#performance-achievement" title="Link to this heading">¶</a></h3>
<p>Our Pallas implementation achieves performance competitive with NVIDIA’s cuDNN flash attention. In the forward pass, our kernel completes in 2.38ms compared to cuDNN’s 2.43ms at <span class="math notranslate nohighlight">\(T = 4096\)</span>, making our implementation marginally faster. In the backward pass, our implementation takes 7.84ms versus cuDNN’s 6.98ms, approximately 12% slower. This gap is due to our three-kernel design requiring two attention recomputations, while cuDNN uses a single fused kernel. Both flash implementations deliver a 7x speedup over naive attention in the forward pass and 4x in the backward pass at long sequence lengths.</p>
</section>
<section id="remaining-gaps">
<h3>Remaining Gaps<a class="headerlink" href="#remaining-gaps" title="Link to this heading">¶</a></h3>
<p>Our backward pass recomputes the attention matrix twice (once for <span class="math notranslate nohighlight">\(dK\)</span>/<span class="math notranslate nohighlight">\(dV\)</span>, once for <span class="math notranslate nohighlight">\(dQ\)</span>), adding approximately 40% more FLOPs compared to cuDNN’s fused approach. This is a fundamental limitation of our three-kernel design.</p>
</section>
<section id="pallas-limitations">
<h3>Pallas Limitations<a class="headerlink" href="#pallas-limitations" title="Link to this heading">¶</a></h3>
<p>Pallas provides a high-level abstraction for writing GPU kernels, but it does not expose certain low-level primitives. There is no warp-level programming available. You can configure <code class="docutils literal notranslate"><span class="pre">num_warps</span></code> but cannot coordinate work between warps within a block. Shared memory control is limited. Pallas manages shared memory implicitly through BlockSpec. You cannot explicitly allocate shared memory or control synchronization barriers. Atomic operations are not available, requiring separate kernels for reductions like our three-kernel backward pass.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Dao, T., Fu, D., Ermon, S., Rudra, A., &amp; Re, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. <em>NeurIPS 2022</em>. <a class="reference external" href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></p></li>
<li><p>Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. <em>arXiv preprint arXiv:2307.08691</em>. <a class="reference external" href="https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a></p></li>
<li><p>JAX Official Flash Attention (TPU): <a class="reference external" href="https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py">https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py</a></p></li>
<li><p>JAX Official Fused Attention (GPU): <a class="reference external" href="https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/gpu/attention.py">https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/gpu/attention.py</a></p></li>
<li><p>Umar Jamil’s Triton Flash Attention: <a class="reference external" href="https://github.com/hkproj/triton-flash-attention">https://github.com/hkproj/triton-flash-attention</a></p></li>
<li><p>Sebastian Raschka - Understanding and Coding Self-Attention from Scratch: <a class="reference external" href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a></p></li>
<li><p>NVIDIA Ada GPU Architecture Tuning Guide: <a class="reference external" href="https://docs.nvidia.com/cuda/ada-tuning-guide/index.html">https://docs.nvidia.com/cuda/ada-tuning-guide/index.html</a></p></li>
<li><p>TechPowerUp RTX 4000 Ada Generation GPU Specs: <a class="reference external" href="https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171">https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171</a></p></li>
</ol>
<script src="https://utteranc.es/client.js"
        repo="novastar53/novastar53.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="intro.html">
              <img class="logo" src="_static/starburst_narrow.png" alt="Logo of Starburst"/>
            </a></p>
<h1 class="logo"><a href="intro.html">Starburst</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to Write a Flash Attention Kernel in Pallas</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-mixture-of-experts-works-part-2.html">How do Mixture of Experts Layers Work? Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="pallas-matmul.html">How to Write a Matrix Multiplication Kernel using Pallas</a></li>
<li class="toctree-l1"><a class="reference internal" href="pallas-softmax.html">How to Write a Softmax Kernel in Pallas</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-mixture-of-experts-works-part-1.html">How do Mixture of Experts Layers Work? Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-batchnorm-works-part-2.html">How Does Batch Normalization Work? Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-positional-embeddings-work.html">How do Positional Embeddings Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-residual-connections-work.html">How Do Residual Connections Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-dropout-works.html">How and Why Does Dropout Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-batchnorm-works.html">How Does Batch Normalization Work? Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="nadaraya-watson-kernel-regression.html">Nadaraya-Watson Regression</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="intro.html">Documentation overview</a><ul>
      <li>Previous: <a href="intro.html" title="previous chapter">Machine Learning Blog</a></li>
      <li>Next: <a href="how-mixture-of-experts-works-part-2.html" title="next chapter">How do Mixture of Experts Layers Work? Part 2</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;Vikram Pawar [novastar53.github.io].
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/pallas-flash-attn.ipynb"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>