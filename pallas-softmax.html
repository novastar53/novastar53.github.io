<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>How to Write a Softmax Kernel in Pallas &#8212; Vikram&#39;s Data Science Blog</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=3eba45b5" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="icon" href="_static/starburst (200 x 200 px).png"/>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How to Write a Matrix Multiplication Kernel using Pallas" href="pallas-matmul.html" />
    <link rel="prev" title="Explorations in Data Science" href="intro.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="how-to-write-a-softmax-kernel-in-pallas">
<h1>How to Write a Softmax Kernel in Pallas<a class="headerlink" href="#how-to-write-a-softmax-kernel-in-pallas" title="Link to this heading">¶</a></h1>
<p>The softmax function is fundamental to neural network training because it converts raw model outputs (logits) into valid probability distributions. This is essential for classification tasks where we need to interpret network predictions as probabilities over discrete classes. Additionally, softmax is differentiable, allowing gradients to flow effectively through the network during training, which is why it’s the standard choice for the output layer in multi-class classification models.</p>
<p>In the <a class="reference external" href="https://vikrampawar.com/pallas-matmul.html">previous post</a>, we wrote a GPU kernel in Pallas for performing efficient matrix multiplication. In this post, we’ll build on this by writing a GPU kernel for the softmax function. We will also write the backward pass and test it with a neural network training run.</p>
<section id="softmax-operation">
<h2>Softmax Operation<a class="headerlink" href="#softmax-operation" title="Link to this heading">¶</a></h2>
<p>Given an input vector <span class="math notranslate nohighlight">\(z = (z_1, ..., z_n) \in R^n\)</span>, the softmax function <span class="math notranslate nohighlight">\(σ : R^n → (0,1)^n\)</span> produces a probability distribution over the n entries:</p>
<div class="math notranslate nohighlight">
\[
\sigma(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^n \exp(z_j)} \quad\text{for } i=1,\dots,n.
\]</div>
<p>Softmax is invariant to shifts: <span class="math notranslate nohighlight">\(σ(z) = σ(z + c)\)</span> for any scalar c.
For numerical stability one commonly uses</p>
<div class="math notranslate nohighlight">
\[
\sigma(z)_i = \frac{\exp(z_i - \max_j z_j)}{\sum_{k=1}^n \exp(z_k - \max_j z_j)}.
\]</div>
<p>let’s start with a naive implementation below:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">manual_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>         <span class="c1"># 1</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>   <span class="c1"># 2</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>              <span class="c1"># 3</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">/</span> <span class="n">l</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>              <span class="c1"># 4</span>
</pre></div>
</div>
</div>
</div>
<p>Now imagine trying to implement this on a GPU. For computing line #1, the entire logits tensor will have to be loaded into the GPU cache. This will be extremely slow for large matrices.</p>
</section>
<section id="online-softmax">
<h2>Online Softmax<a class="headerlink" href="#online-softmax" title="Link to this heading">¶</a></h2>
<p>Since the max and sum operations are across columns, it’s easy to tile across the rows. However, the column axis might still cause a bottleneck.
Since we cannot parallelize the row operations, we can calculate the max (m) and normalizing factor (l) across each block of columns in a loop and use a trick to correct both as we process each block.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">new_m</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">old_m</span><span class="p">,</span> <span class="n">block_max</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="n">exp</span><span class="p">(</span><span class="n">old_m</span> <span class="o">-</span> <span class="n">new_m</span><span class="p">)</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">x_block</span> <span class="o">-</span> <span class="n">new_m</span><span class="p">))</span>
</pre></div>
</div>
<p>Let’s update our implementation to reflect our new algorithm:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BLOCK_M</span> <span class="o">=</span> <span class="mi">64</span> <span class="c1"># These can be tuned for your GPU</span>
<span class="n">BLOCK_N</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Online softmax</span>
<span class="k">def</span><span class="w"> </span><span class="nf">online_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">BLOCK_M</span><span class="p">):</span>  <span class="c1"># This axis can be tiled in parallel blocks.</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">BLOCK_N</span><span class="p">):</span>  <span class="c1"># This axis cannot be tiled in parallel, so it is tiled sequentially</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">BLOCK_N</span><span class="p">]</span> <span class="c1"># Load a block</span>
            <span class="n">block_max</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Get the max across the block</span>
            <span class="n">curr_max</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">]</span> <span class="c1"># Retrieve the previous computed max for the rows</span>
            <span class="n">new_max</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">curr_max</span><span class="p">,</span> <span class="n">block_max</span><span class="p">)</span> <span class="c1"># Update the max for all the rows</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">new_max</span><span class="p">)</span>  
            <span class="n">l_block</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">]</span> <span class="c1"># Get the denominator for the rows in the block</span>
            <span class="n">l_block</span> <span class="o">=</span> <span class="n">l_block</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">curr_max</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="c1"># Correct and update the denominator based on the current block</span>
                <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">block</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">l_block</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">BLOCK_N</span><span class="p">):</span>  <span class="c1"># Loop over the column blocks and generate the output values </span>
            <span class="n">out_block</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">BLOCK_N</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">/</span> <span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">BLOCK_N</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">out_block</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<p>The next step is to convert this into an efficient GPU kernel.</p>
</section>
<section id="let-s-implement-the-forward-pass-kernel">
<h2>Let’s Implement the Forward Pass Kernel<a class="headerlink" href="#let-s-implement-the-forward-pass-kernel" title="Link to this heading">¶</a></h2>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.experimental.pallas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.experimental.pallas</span><span class="w"> </span><span class="kn">import</span> <span class="n">triton</span> <span class="k">as</span> <span class="n">plgpu</span>

<span class="n">INTERPRET_MODE</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># Set to False on GPU</span>
<span class="n">NUM_WARPS</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">NUM_STAGES</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">def</span><span class="w"> </span><span class="nf">softmax_kernel</span><span class="p">(</span><span class="n">x_ref</span><span class="p">,</span> <span class="n">out_ref</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">n_col_blocks</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">):</span>
    <span class="n">max_reg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,),</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> 
    <span class="n">l_reg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> 
    <span class="n">row_ids</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)</span>
    <span class="n">row_mask</span> <span class="o">=</span> <span class="n">row_ids</span> <span class="o">&lt;</span> <span class="n">n_rows</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stats_body</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="n">max_reg</span><span class="p">,</span> <span class="n">l_reg</span> <span class="o">=</span> <span class="n">args</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dslice</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_N</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>
        <span class="n">col_ids</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_N</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">BLOCK_N</span><span class="p">)</span>
        <span class="n">cols_mask</span> <span class="o">=</span> <span class="n">col_ids</span> <span class="o">&lt;</span> <span class="n">n_cols</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">row_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">cols_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x_tile</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
            <span class="n">x_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span>
            <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
            <span class="n">other</span><span class="o">=-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">max_tile</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x_tile</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">max_new</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_reg</span><span class="p">,</span> <span class="n">max_tile</span><span class="p">)</span>
        <span class="n">l_update</span> <span class="o">=</span> <span class="n">l_reg</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">max_reg</span> <span class="o">-</span> <span class="n">max_new</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_tile</span> <span class="o">-</span> <span class="n">max_new</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">max_new</span><span class="p">,</span> <span class="n">l_update</span>
        
    <span class="n">max_reg</span><span class="p">,</span> <span class="n">l_reg</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_col_blocks</span><span class="p">,</span> <span class="n">stats_body</span><span class="p">,</span> <span class="p">(</span><span class="n">max_reg</span><span class="p">,</span> <span class="n">l_reg</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">out_body</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">dslice</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_N</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>
        <span class="n">col_ids</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">BLOCK_N</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">BLOCK_N</span><span class="p">)</span>
        <span class="n">cols_mask</span> <span class="o">=</span> <span class="n">col_ids</span> <span class="o">&lt;</span> <span class="n">n_cols</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">row_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">cols_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x_tile</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
            <span class="n">x_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span>
            <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
            <span class="n">other</span><span class="o">=-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">out_tile</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_tile</span> <span class="o">-</span> <span class="n">max_reg</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">/</span> <span class="n">l_reg</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">out_ref</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">out_tile</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

    <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_col_blocks</span><span class="p">,</span> <span class="n">out_body</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">n_row_blocks</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">BLOCK_M</span><span class="p">)</span>
    <span class="n">n_col_blocks</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">BLOCK_N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">softmax_kernel</span><span class="p">,</span> <span class="n">n_col_blocks</span><span class="o">=</span><span class="n">n_col_blocks</span><span class="p">,</span> <span class="n">n_rows</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_cols</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">n_row_blocks</span><span class="p">,),</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">))],</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
        <span class="n">interpret</span><span class="o">=</span><span class="n">INTERPRET_MODE</span><span class="p">,</span>
        <span class="n">compiler_params</span><span class="o">=</span><span class="n">plgpu</span><span class="o">.</span><span class="n">CompilerParams</span><span class="p">(</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="n">NUM_WARPS</span><span class="p">,</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="n">NUM_STAGES</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="performance">
<h2>Performance<a class="headerlink" href="#performance" title="Link to this heading">¶</a></h2>
<p>Let’s compare our performance with the out-of-the-box softmax implementation provided by Jax.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bench</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="n">out</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
    <span class="n">times</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">times</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">times</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>


<span class="n">d</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>

<span class="n">out_jax</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">out_online</span> <span class="o">=</span> <span class="n">online_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">out_pallas</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">out_jax</span><span class="p">),</span> <span class="n">out_pallas</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">out_jax</span><span class="p">),</span> <span class="n">out_online</span><span class="p">)</span>

<span class="n">softmax_jit</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">softmax_jit</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>

<span class="n">t_jax</span> <span class="o">=</span> <span class="n">bench</span><span class="p">(</span><span class="n">softmax_jit</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
<span class="n">t_pallas</span> <span class="o">=</span> <span class="n">bench</span><span class="p">(</span><span class="n">softmax</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jax Softmax: </span><span class="si">{</span><span class="n">t_jax</span><span class="o">*</span><span class="mf">1e3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pallas Softmax: </span><span class="si">{</span><span class="n">t_pallas</span><span class="o">*</span><span class="mf">1e3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup (jax / pallas): </span><span class="si">{</span><span class="n">t_jax</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">t_pallas</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W0107 10:44:50.336112    3714 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn&#39;t support extracting fabric info or NVLink is not used by the device.
W0107 10:44:50.340067    3436 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn&#39;t support extracting fabric info or NVLink is not used by the device.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jax Softmax: 0.30 ms
Pallas Softmax: 0.33 ms
Speedup (jax / pallas): 0.93x
</pre></div>
</div>
</div>
</div>
<p>Our softmax kernel is around 7% slower than the Jax implementation. This is not surprising considering that Jax uses Nvidia’s CUDA kernels under the hood, which are highly tuned. Now, let’s implement the backward pass.</p>
</section>
<section id="let-s-implement-the-backward-pass-kernel">
<h2>Let’s Implement the Backward Pass Kernel<a class="headerlink" href="#let-s-implement-the-backward-pass-kernel" title="Link to this heading">¶</a></h2>
<p>Let’s first derive the expression for the backward pass.</p>
<p>Let <span class="math notranslate nohighlight">\(x\in\mathbb{R}^n, y=\mathrm{softmax}(x)\)</span> with
<span class="math notranslate nohighlight">\(y_i=\frac{e^{x_i}}{\sum_{k=1}^n e^{x_k}}.\)</span>
Assume an upstream gradient <span class="math notranslate nohighlight">\(g=\frac{\partial L}{\partial y}\)</span> is given, and we want <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial x}\)</span>.</p>
<p>First compute the Jacobian of softmax. Let <span class="math notranslate nohighlight">\(S=\sum_k e^{x_k}\)</span>, so <span class="math notranslate nohighlight">\(y_i=e^{x_i}/S\)</span>.</p>
<p>Differentiate <span class="math notranslate nohighlight">\(y_i\)</span> w.r.t. <span class="math notranslate nohighlight">\(x_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y_i}{\partial x_j}
=\frac{\partial}{\partial x_j}\left(\frac{e^{x_i}}{S}\right)
=\frac{\delta_{ij}e^{x_i}\,S - e^{x_i}\,\frac{\partial S}{\partial x_j}}{S^2}\]</div>
<p>But <span class="math notranslate nohighlight">\(\frac{\partial S}{\partial x_j}=e^{x_j}\)</span>
Substitute:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y_i}{\partial x_j}
=\frac{\delta_{ij}e^{x_i}S - e^{x_i}e^{x_j}}{S^2}
=\delta_{ij}\frac{e^{x_i}}{S} - \frac{e^{x_i}}{S}\frac{e^{x_j}}{S}
=\delta_{ij}y_i - y_i y_j
\]</div>
<p>Now apply the chain rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial x_j}
=\sum_{i=1}^n \frac{\partial L}{\partial y_i}\frac{\partial y_i}{\partial x_j}
=\sum_i g_i\left(\delta_{ij}y_i - y_i y_j\right)
\]</div>
<div class="math notranslate nohighlight">
\[
= g_j y_j - y_j\sum_i g_i y_i
= y_j\left(g_j - \sum_i g_i y_i\right)
\]</div>
<p>Finally, in vector form:</p>
<div class="math notranslate nohighlight">
\[
\;\frac{\partial L}{\partial x} = y \odot (g - \langle g, y\rangle)\;
\]</div>
<p>The kernel for the backward pass can be implemented in two steps. First, we can compute the inner product <span class="math notranslate nohighlight">\( \langle g, y\rangle \)</span>, then an elementwise operation to compute the final expression. Since this is a binary classifier, both the upstream gradient <span class="math notranslate nohighlight">\(g\)</span> and the output <span class="math notranslate nohighlight">\(y\)</span> will be of shape (B, C) where B is the batch size and C is the number of classes. Since C = 2, we only need to tile our kernel along the B axis, simplifying our implementation greatly.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax_backward_kernel</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">dy_ref</span><span class="p">,</span> <span class="n">dx_ref</span><span class="p">):</span>
    <span class="c1"># compute the inner product &lt;g_ref, y_ref&gt;</span>
    <span class="n">dy_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dy_ref</span><span class="p">)</span>
    <span class="n">y_reg</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_ref</span><span class="p">)</span>
    <span class="n">g_dot_y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dy_reg</span> <span class="o">*</span> <span class="n">y_reg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute the output block</span>
    <span class="n">output_reg</span> <span class="o">=</span> <span class="n">y_reg</span> <span class="o">*</span> <span class="p">(</span> <span class="n">dy_reg</span> <span class="o">-</span> <span class="n">g_dot_y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="p">)</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">dx_ref</span><span class="p">,</span> <span class="n">output_reg</span><span class="p">)</span>


<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">softmax_backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">),)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
        <span class="n">softmax_backward_kernel</span><span class="p">,</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="n">out_shape</span><span class="p">,</span>
        <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>  <span class="c1"># y</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>  <span class="c1"># dy </span>
        <span class="p">],</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>  <span class="c1"># dx</span>
        <span class="n">interpret</span><span class="o">=</span><span class="n">INTERPRET_MODE</span><span class="p">,</span>
        <span class="n">compiler_params</span><span class="o">=</span><span class="n">plgpu</span><span class="o">.</span><span class="n">CompilerParams</span><span class="p">(</span>
            <span class="n">num_warps</span><span class="o">=</span><span class="n">NUM_WARPS</span><span class="p">,</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="n">NUM_STAGES</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>


<span class="nd">@jax</span><span class="o">.</span><span class="n">custom_vjp</span>
<span class="k">def</span><span class="w"> </span><span class="nf">softmax_pallas</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">softmax_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span>


<span class="k">def</span><span class="w"> </span><span class="nf">softmax_bwd</span><span class="p">(</span><span class="n">saved_y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
    <span class="p">(</span><span class="n">y</span><span class="p">,)</span> <span class="o">=</span> <span class="p">(</span><span class="n">saved_y</span><span class="p">,)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">softmax_backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>


<span class="n">softmax_pallas</span><span class="o">.</span><span class="n">defvjp</span><span class="p">(</span><span class="n">softmax_fwd</span><span class="p">,</span> <span class="n">softmax_bwd</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="let-s-evaluate-our-kernel">
<h2>Let’s Evaluate our Kernel<a class="headerlink" href="#let-s-evaluate-our-kernel" title="Link to this heading">¶</a></h2>
<p>We will attempt to train a binary classifier model on some synthetic data. Let’s start by generating a toy dataset.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>

<span class="n">B</span><span class="p">,</span> <span class="n">E</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">24</span> <span class="c1"># (batch size, number of features)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">E</span><span class="p">))</span>
<span class="n">class_ids</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">class_ids</span>
</pre></div>
</div>
</div>
</div>
<p>Next, let’s define our binary classifier and loss function.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">flax.nnx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nnx</span>

<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ModelConfig</span><span class="p">:</span>
    <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ModelConfig</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span><span class="w"> </span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax_pallas</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">probs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">one_hot</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>Before we train the model, let’s first test if our backward pass kernel is correct.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">loss_from_logits_pallas</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax_pallas</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">probs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">one_hot</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span><span class="w"> </span><span class="nf">loss_from_logits_gt</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">probs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">one_hot</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="nd">@nnx</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">verify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">d_logits_pallas</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_from_logits_pallas</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">d_logits_gt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_from_logits_gt</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">d_logits_pallas</span><span class="p">,</span> <span class="n">d_logits_gt</span><span class="p">)</span>


<span class="n">default</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">69</span><span class="p">)</span>
<span class="n">rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">default</span><span class="p">)</span> 

<span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">E</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">E</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">rngs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">verify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Excellent! Looks like our backward pass kernel works correctly. Finally, let’s overfit the model on our toy dataset using our softmax kernel.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">optax</span> 

<span class="nd">@nnx</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tx</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">)</span>

<span class="n">iters</span> <span class="o">=</span> <span class="mi">15</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iter 0: loss=0.7163699865341187
iter 1: loss=0.578166127204895
iter 2: loss=1.0504496097564697
iter 3: loss=0.13622123003005981
iter 4: loss=0.3550783097743988
iter 5: loss=0.1289454996585846
iter 6: loss=0.06486515700817108
iter 7: loss=0.056444257497787476
iter 8: loss=0.05389563366770744
iter 9: loss=0.03236997872591019
iter 10: loss=0.016692688688635826
iter 11: loss=0.0100052859634161
iter 12: loss=0.003909544087946415
iter 13: loss=0.0019604917615652084
iter 14: loss=0.0014198491116985679
</pre></div>
</div>
</div>
</div>
<p>We were able to successfully overfit our toy dataset using our softmax implementation. In the next one, we’ll build on this and implement a custom Pallas kernel for computing self-attention.</p>
<script src="https://utteranc.es/client.js"
        repo="novastar53/novastar53.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="intro.html">
              <img class="logo" src="_static/starburst_narrow.png" alt="Logo of Starburst Data Science Blog"/>
            </a></p>
<h1 class="logo"><a href="intro.html">Starburst Data Science Blog</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to Write a Softmax Kernel in Pallas</a></li>
<li class="toctree-l1"><a class="reference internal" href="pallas-matmul.html">How to Write a Matrix Multiplication Kernel using Pallas</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-mixture-of-experts-works-part-1.html">How do Mixture of Experts Layers Work? Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-batchnorm-works-part-2.html">How Does Batch Normalization Work? Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-residual-connections-work.html">How Do Residual Connections Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-positional-embeddings-work.html">How do Positional Embeddings Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-dropout-works.html">How and Why Does Dropout Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-batchnorm-works.html">How Does Batch Normalization Work? Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="nadaraya-watson-kernel-regression.html">Nadaraya-Watson Regression</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="intro.html">Documentation overview</a><ul>
      <li>Previous: <a href="intro.html" title="previous chapter">Explorations in Data Science</a></li>
      <li>Next: <a href="pallas-matmul.html" title="next chapter">How to Write a Matrix Multiplication Kernel using Pallas</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;Vikram Pawar [novastar53.github.io].
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/pallas-softmax.ipynb"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>