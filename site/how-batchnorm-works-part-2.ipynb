{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0QZXjN-b9Tc"
      },
      "source": [
        "# How and Why BatchNorm Works - Part 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxG2sVcsb9Td"
      },
      "source": [
        "In Part 1, we showed that batch normalization (BN) can help train neural networks by reducing internal covariate shift (ICS) and smoothing gradient updates. ICS was defined in terms of activation distributions by Ioffe and Szegedy (2015). For our fully-connected network, we indeed observed that the activation distributions for the network with BN were comparatively stable compared to those for the network without BN.\n",
        "We also touched upon the idea that BN has a smoothing effect on the loss function making it more convex (residual connections have a similar effect). \n",
        "\n",
        "In their 2018 paper, Santukar et al. used a new definition of ICS and argued that BN has no substantial effect on it. In fact, BN improves training performance even when random noise is added to the activations to force covariate shift.  They argue that the major reason for improved performance loss surface smoothing.\n",
        "\n",
        "Santurkar et. al define internal covariate shift (ICS) of activation i at time t to be the difference $||Gt,i − G′ t,i||^2$, where \n",
        "$$\n",
        "G_{t,i} = ∇W^{(t)}_i L(W (t) 1 , . . . , W (t) k ; x(t), y(t)) G′ t,i = ∇W (t) i L(W (t+1) 1 , . . . , W (t+1) i−1 , W (t) i , W (t) i+1, . . . , W (t) k ; x(t), y(t)).\n",
        "$$\n",
        "\n",
        "This is more of an *operational* definition that measures the impact on the gradient of a particular layer due covariate shift. In simple terms, if updating the previous layers changes the activations for the current layer significantly, it should impact the gradient of the current layer significantly as well.\n",
        "\n",
        "\n",
        "In this experiment, we will start by reproducing the results from Santukar et al. Then we will try and understand why their results and our previous experiment differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EpsseJYsc0Y1",
        "tags": [
          "hide-input",
          "hide-output"
        ]
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/deepkit\n",
        "    !cd deepkit && git pull && uv build . --wheel && pip uninstall deepkit -y && pip install ./dist/* --quiet\n",
        "else:\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "    package_path = Path(\"../../deepkit/src\").resolve()\n",
        "    if str(package_path) not in sys.path:\n",
        "        sys.path.append(str(package_path))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7bhT4GGb9Td"
      },
      "source": [
        "\n",
        "## Let's set up the model\n",
        "We'll be using the standard VGG-Net architecture as per the paper.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GPSb_PkVb9Td"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "\n",
        "kernel_init = nnx.initializers.glorot_normal()\n",
        "\n",
        "\n",
        "class VGGBlock(nnx.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
        "        self.conv = nnx.Conv(in_features=in_features,\n",
        "                             out_features=out_features,\n",
        "                             kernel_size=(3, 3),\n",
        "                             kernel_init=kernel_init,\n",
        "                             padding='SAME',\n",
        "                             rngs=rngs)\n",
        "        self.bn = nnx.BatchNorm(num_features=out_features, momentum=0.90, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.conv(x)\n",
        "        conv_activation = x\n",
        "        x = self.bn(x)\n",
        "        x = nnx.relu(x)\n",
        "        return x, conv_activation\n",
        "\n",
        "class VGGNet(nnx.Module):\n",
        "    def __init__(self, rngs: nnx.Rngs):\n",
        "        self.convs = [\n",
        "            VGGBlock(in_features=3, out_features=64,  rngs=rngs),\n",
        "            VGGBlock(in_features=64, out_features=64,  rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=64, out_features=128, rngs=rngs),\n",
        "            VGGBlock(in_features=128, out_features=128, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=128, out_features=256, rngs=rngs),\n",
        "            VGGBlock(in_features=256, out_features=256, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=256, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "        ]\n",
        "\n",
        "        self.fc1 = nnx.Linear(in_features=512, out_features=96, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.fc2 = nnx.Linear(in_features=96, out_features=96, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.out = nnx.Linear(in_features=96, out_features=10,  kernel_init=kernel_init, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        activations = {}\n",
        "        max_pool_after = [1, 3, 5, 7, 9]\n",
        "        for conv_idx in range(len(self.convs)):\n",
        "            layer = self.convs[conv_idx]\n",
        "            x, act = layer(x)\n",
        "            activations[f\"conv_{conv_idx}\"] = act\n",
        "            if conv_idx in max_pool_after:\n",
        "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        x = x.squeeze()\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        activations[\"fc1\"] = x\n",
        "        x = nnx.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        activations[\"fc2\"] = x\n",
        "        x = nnx.relu(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        activations[\"out\"] = x\n",
        "        return x, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWUeygKCb9Te"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "candidate = VGGNet(rngs=rngs)\n",
        "graphdef, state = nnx.split(candidate)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLfMaFY4i0jj"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "baseline = VGGNet(rngs=rngs)\n",
        "\n",
        "class Dummy(nnx.Module):\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "# Remove the batchnorm layers\n",
        "for vgg in baseline.convs:\n",
        "  vgg.bn = lambda x: x\n",
        "graphdef, state = nnx.split(baseline)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZWAvrpdSb9Te"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "baseline_tx = optax.chain(#optax.add_decayed_weights(5e-4),\n",
        "                           #optax.clip_by_global_norm(1.0),\n",
        "                           optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False)\n",
        "                           )\n",
        "\n",
        "optimizer_tx = optax.chain(#optax.add_decayed_weights(5e-4),\n",
        "                            #optax.clip_by_global_norm(1.0),\n",
        "                            optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False)\n",
        "                            )\n",
        "\n",
        "baseline_optimizer = nnx.Optimizer(baseline, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
        "candidate_optimizer = nnx.Optimizer(candidate, optimizer_tx)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VkkhUv_6b9Te"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "def loss_fn(model, batch, targets):\n",
        "    logits, activations = model(batch)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "    return loss, activations\n",
        "\n",
        "@nnx.jit\n",
        "def step_fn(model: nnx.Module, optimizer: nnx.Optimizer, batch: jax.Array, labels: jax.Array):\n",
        "    (loss, activations), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, batch, labels)\n",
        "    optimizer.update(grads)\n",
        "    return loss, activations, grads\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def accuracy(model: nnx.Module, batch: jax.Array, labels: jax.Array):\n",
        "    logits, _ = model(batch)\n",
        "    #probs = nnx.softmax(logits, axis=-1)\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    sum = jnp.sum(preds == labels)\n",
        "    acc = sum/logits.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def test_accuracy(model: nnx.Module, testloader):\n",
        "    acc, n = 0, 0\n",
        "    for batch, labels in testloader:\n",
        "        batch = jnp.array(batch)\n",
        "        labels = jnp.array(labels)\n",
        "        acc += accuracy(model, batch, labels)\n",
        "        n += 1\n",
        "    return acc/n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DCGFto6Kb9Te"
      },
      "outputs": [],
      "source": [
        "from deepkit.datasets import load_CIFAR10\n",
        "from deepkit.loggers import DiskLogger\n",
        "from deepkit.internal_covariate_shift import partial_step_fn\n",
        "\n",
        "\n",
        "num_epochs = 39\n",
        "\n",
        "train_loader, test_loader = load_CIFAR10(augment=False)\n",
        "num_steps = num_epochs*len(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nwV6fYtb9Te"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "baseline.train()\n",
        "candidate.train()\n",
        "baseline_train_accs, candidate_train_accs = [], []\n",
        "baseline_test_accs, candidate_test_accs = [], []\n",
        "baseline_train_losses, candidate_train_losses = [], []\n",
        "#baseline_grad_norms, candidate_grad_norms = [], []\n",
        "#baseline_activations_log, candidate_activations_log = [], []\n",
        "\n",
        "baseline_activations_logger = DiskLogger(\"baseline_activations\")\n",
        "candidate_activations_logger = DiskLogger(\"candidate_activations\")\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch, labels in train_loader:\n",
        "          batch = jnp.array(batch)\n",
        "          labels = jnp.array(labels)\n",
        "          baseline.train()\n",
        "          candidate.train()\n",
        "          baseline_loss, baseline_activations, baseline_grads = step_fn(baseline, baseline_optimizer, batch, labels)\n",
        "          candidate_loss, candidate_activations, candidate_grads = step_fn(candidate, candidate_optimizer, batch, labels)\n",
        "          baseline_train_losses.append(baseline_loss)\n",
        "          candidate_train_losses.append(candidate_loss)\n",
        "          baseline.eval()\n",
        "          candidate.eval()\n",
        "          baseline_acc = accuracy(baseline, batch, labels)\n",
        "          candidate_acc = accuracy(candidate, batch, labels)\n",
        "          baseline_train_accs.append(baseline_acc)\n",
        "          candidate_train_accs.append(candidate_acc)\n",
        "          if i % 200 == 0:\n",
        "            baseline_test_acc = test_accuracy(baseline, test_loader)\n",
        "            candidate_test_acc = test_accuracy(candidate, test_loader)\n",
        "            baseline_test_accs.append(baseline_test_acc)\n",
        "            candidate_test_accs.append(candidate_test_acc)\n",
        "            baseline_activations_logger.log(i, baseline_activations)\n",
        "            candidate_activations_logger.log(i, candidate_activations)\n",
        "          if i % 20 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"iter: {i} | baseline loss: {baseline_loss:0.4f} | candidate loss: {candidate_loss:0.4f} | baseline train acc: {baseline_acc:0.2f} | candidate train acc: {candidate_acc:0.2f} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "            axes[0].plot(baseline_train_losses, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[0].plot(candidate_train_losses, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[0].set_title(\"Loss\")\n",
        "            axes[1].plot(baseline_train_accs, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[1].plot(candidate_train_accs, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[1].set_title(\"Train Accuracy\")\n",
        "            axes[2].plot(baseline_test_accs, label=\"Without Batchnorm\")\n",
        "            axes[2].plot(candidate_test_accs, label=\"With Batchnorm\")\n",
        "            axes[2].set_title(\"Test Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "          #print(f\"iter: {i} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "          i += 1\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unaL0Z6SeHw4"
      },
      "outputs": [],
      "source": [
        "from matplotlib import colormaps as cm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "layers = [f\"conv_{i}\" for i in range(10)] + [\"fc1\", \"fc2\", \"out\"]\n",
        "\n",
        "fig, axs = plt.subplots(13, 1, figsize=(5,24), constrained_layout=True)\n",
        "\n",
        "\n",
        "def update(frame):\n",
        "    baseline_activations = baseline_activations_logger[frame]\n",
        "    candidate_activations = candidate_activations_logger[frame]\n",
        "    for layer_idx, layer in enumerate(layers):\n",
        "        axs[layer_idx].cla()\n",
        "        layer_baseline_activations = baseline_activations[layer].flatten()\n",
        "        layer_baseline_mean = layer_baseline_activations.mean()\n",
        "        layer_baseline_std = layer_baseline_activations.std()\n",
        "        axs[layer_idx].hist(layer_baseline_activations, color=cm[\"Blues\"](50), bins=60, alpha=1.0)\n",
        "\n",
        "        layer_candidate_activations = candidate_activations[layer].flatten()\n",
        "        layer_candidate_mean = layer_candidate_activations.mean()\n",
        "        layer_candidate_std = layer_candidate_activations.std()\n",
        "        axs[layer_idx].hist(layer_candidate_activations, color=cm[\"Reds\"](90),  bins=60, alpha=0.5)\n",
        "        axs[layer_idx].set_title(f\"{layer} Outputs - Iteration:{frame}\")\n",
        "\n",
        "        axs[layer_idx].margins(x=0, y=0)\n",
        "        axs[layer_idx].set_xlim(-8, 8)\n",
        "        axs[layer_idx].legend([f\"Baseline: {layer_baseline_mean:0.2f} ± {layer_baseline_std:0.2f}\",\n",
        "                              f\"Candidate:{layer_candidate_mean:0.2f} ± {layer_candidate_std:0.2f}\"])\n",
        "\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(baseline_activations_logger), interval=300, repeat=True)\n",
        "plt.close(fig)\n",
        "video_html = ani.to_html5_video().replace('<video', '<video muted')\n",
        "HTML(video_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Santurkar et. al Implementation of Internal Covariate Shift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "baseline.train()\n",
        "candidate.train()\n",
        "baseline_train_accs, candidate_train_accs = [], []\n",
        "baseline_test_accs, candidate_test_accs = [], []\n",
        "baseline_train_losses, candidate_train_losses = [], []\n",
        "#baseline_grad_norms, candidate_grad_norms = [], []\n",
        "#baseline_activations_log, candidate_activations_log = [], []\n",
        "\n",
        "baseline_activations_logger = DiskLogger(\"baseline_activations\")\n",
        "candidate_activations_logger = DiskLogger(\"candidate_activations\")\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch, labels in train_loader:\n",
        "          batch = jnp.array(batch)\n",
        "          labels = jnp.array(labels)\n",
        "          baseline.train()\n",
        "          candidate.train()\n",
        "          baseline_loss, baseline_activations, baseline_grads = partial_step_fn(baseline, baseline_optimizer, batch, labels, wrt_layer=\"convs.3\")\n",
        "          time.sleep(1)\n",
        "          raise(Exception(\"Done.\"))\n",
        "          candidate_loss, candidate_activations, candidate_grads = step_fn(candidate, candidate_optimizer, batch, labels)\n",
        "          baseline_train_losses.append(baseline_loss)\n",
        "          candidate_train_losses.append(candidate_loss)\n",
        "          baseline.eval()\n",
        "          candidate.eval()\n",
        "          baseline_acc = accuracy(baseline, batch, labels)\n",
        "          candidate_acc = accuracy(candidate, batch, labels)\n",
        "          baseline_train_accs.append(baseline_acc)\n",
        "          candidate_train_accs.append(candidate_acc)\n",
        "          if i % 200 == 0:\n",
        "            baseline_test_acc = test_accuracy(baseline, test_loader)\n",
        "            candidate_test_acc = test_accuracy(candidate, test_loader)\n",
        "            baseline_test_accs.append(baseline_test_acc)\n",
        "            candidate_test_accs.append(candidate_test_acc)\n",
        "            baseline_activations_logger.log(i, baseline_activations)\n",
        "            candidate_activations_logger.log(i, candidate_activations)\n",
        "          if i % 20 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"iter: {i} | baseline loss: {baseline_loss:0.4f} | candidate loss: {candidate_loss:0.4f} | baseline train acc: {baseline_acc:0.2f} | candidate train acc: {candidate_acc:0.2f} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "            axes[0].plot(baseline_train_losses, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[0].plot(candidate_train_losses, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[0].set_title(\"Loss\")\n",
        "            axes[1].plot(baseline_train_accs, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[1].plot(candidate_train_accs, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[1].set_title(\"Train Accuracy\")\n",
        "            axes[2].plot(baseline_test_accs, label=\"Without Batchnorm\")\n",
        "            axes[2].plot(candidate_test_accs, label=\"With Batchnorm\")\n",
        "            axes[2].set_title(\"Test Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "          #print(f\"iter: {i} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "          i += 1\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
