{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0QZXjN-b9Tc"
      },
      "source": [
        "# How and Why BatchNorm Works - Part 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxG2sVcsb9Td"
      },
      "source": [
        "In [Part 1](https://blog.vikrampawar.com/how-batchnorm-works.html), we saw that batch normalization (BN) can help train neural networks by reducing internal covariate shift (ICS) and smoothing gradient updates. We measured ICS as distributional shift in layer activations, in line with Ioffe and Szegedy (2015). For a fully-connected network with 4 layers, we observed that the activation distributions with BN were comparatively stable compared to those for the network without BN.  We also touched upon the idea that BN has a smoothing effect on the loss surface, making it more convex.\n",
        "\n",
        "In their 2018 paper, Santurkar et al., using a new definition of ICS, argued that BN has no substantial effect on ICS, and might even increase it. They further demonstrated that ICS has no substantial effect on the performance of BN - BN improves training performance even when random noise is added to the activations, forcing ICS. Finally, they showed that the desirable effects of ICS like avoiding vanishing/exploding gradients as well as robustness to initialization and hyperparameter choices are merely downstream effects of an underlying set of causes:\n",
        "\n",
        "1. Lower Lipschitzness of the (i.e. smoother) loss surface.\n",
        "2. Lower Lipschitzness of the gradients i.e. better gradient predictiveness and consequently, higher \"effective\" $\\beta$-smoothness of the gradients.\n",
        "3. More favourable initialization.\n",
        "\n",
        "In this experiment, I will attempt to reproduce and verify the key findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpsseJYsc0Y1",
        "outputId": "86f0393a-32fd-44be-c68c-0253e8be6ce4",
        "tags": [
          "hide-input",
          "hide-output",
          "skip-execution",
          "remove_cell"
        ]
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !mkdir /backend-container\n",
        "    !mkdir /backend-container/containers\n",
        "    !touch /backend-container/containers/build.constraints\n",
        "    !touch /backend-container/containers/requirements.constraints\n",
        "    !git clone https://github.com/novastar53/deepkit\n",
        "    #!git checkout 0957acdf4a007350a5f393f6777765b637ec8739\n",
        "    !cd deepkit && git checkout santurkar-ics\n",
        "    !cd deepkit && uv build . --wheel --prerelease allow && pip uninstall deepkit -y && pip install ./dist/* --quiet\n",
        "else:\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "    package_path = Path(\"../../deepkit/src\").resolve()\n",
        "    if str(package_path) not in sys.path:\n",
        "        sys.path.append(str(package_path))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's start by downloading and preprocessing the dataset. \n",
        "CIFAR-10 is a popular benchmark dataset for evaluating machine learning models in computer vision. It includes 60,000 color images of size 32×32 pixels, divided into 50,000 training images and 10,000 testing images. The dataset is organized into ten distinct classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. \n",
        "\n",
        "I have written a custom CIFAR10 loader which downloads and performs standard preprocessing (normalization and augmentation). For this experiment, I will not be augmenting the dataset. \n",
        "You can check out the details of the dataloader [here](https://github.com/novastar53/deepkit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "from deepkit.datasets import load_CIFAR10\n",
        "\n",
        "train_loader, test_loader = load_CIFAR10(augment=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7bhT4GGb9Td"
      },
      "source": [
        "\n",
        "## Let's set up the model\n",
        "I'll be using a VGG-Net style network with 10 3x3 convolutional and 3 FC layers including the final classifier. The activations are ReLU. The weights for both networks are initialized using the Glorot/Xavier method, which is standard for convolutional networks. To simplify our analysis, we will not use any regularization like dropout or weight decay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPSb_PkVb9Td",
        "outputId": "d1942099-f7dd-4a22-b192-6faece841be2",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "\n",
        "kernel_init = nnx.initializers.glorot_normal()\n",
        "\n",
        "\n",
        "class VGGBlock(nnx.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
        "        self.conv = nnx.Conv(in_features=in_features,\n",
        "                             out_features=out_features,\n",
        "                             kernel_size=(3, 3),\n",
        "                             kernel_init=kernel_init,\n",
        "                             padding='SAME',\n",
        "                             rngs=rngs)\n",
        "        self.bn = nnx.BatchNorm(num_features=out_features, momentum=0.90, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        conv_activation = x\n",
        "        x = nnx.relu(x)\n",
        "        return x, conv_activation\n",
        "\n",
        "class VGGNet(nnx.Module):\n",
        "    def __init__(self, rngs: nnx.Rngs):\n",
        "        self.convs = [\n",
        "            VGGBlock(in_features=3, out_features=64,  rngs=rngs),\n",
        "            VGGBlock(in_features=64, out_features=64,  rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=64, out_features=128, rngs=rngs),\n",
        "            VGGBlock(in_features=128, out_features=128, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=128, out_features=256, rngs=rngs),\n",
        "            VGGBlock(in_features=256, out_features=256, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=256, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "        ]\n",
        "\n",
        "        self.fc1 = nnx.Linear(in_features=512, out_features=96, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.fc2 = nnx.Linear(in_features=96, out_features=96, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.out = nnx.Linear(in_features=96, out_features=10,  kernel_init=kernel_init, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        activations = {}\n",
        "        max_pool_after = [1, 3, 5, 7, 9]\n",
        "        for conv_idx in range(len(self.convs)):\n",
        "            layer = self.convs[conv_idx]\n",
        "            x, act = layer(x)\n",
        "            activations[f\"conv_{conv_idx}\"] = act\n",
        "            if conv_idx in max_pool_after:\n",
        "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        x = x.squeeze()\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        activations[\"fc1\"] = x\n",
        "        x = nnx.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        activations[\"fc2\"] = x\n",
        "        x = nnx.relu(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        activations[\"out\"] = x\n",
        "        return x, activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's initialize the baseline (non-BN) and candidate (BN) models. Both networks are initialized with the same random key for consistency and reproducibility. Both networks have roughly 9.5 million paramters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "AWUeygKCb9Te",
        "outputId": "0066e76d-99a0-47e1-c6b9-c3e08c18902c",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "candidate = VGGNet(rngs=rngs)\n",
        "candidate_graphdef, candidate_state = nnx.split(candidate)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, candidate_state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(candidate_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Flax-NNX library makes it quite easy to modify networks. To remove the BN layers, I simply set them to the identity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "kLfMaFY4i0jj",
        "outputId": "c86b95e2-0c43-4b81-bd08-89fe243bd02b",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "baseline = VGGNet(rngs=rngs)\n",
        "\n",
        "# Remove the batchnorm layers\n",
        "for vgg in baseline.convs:\n",
        "  vgg.bn = lambda x: x\n",
        "\n",
        "baseline_graphdef, baseline_state = nnx.split(baseline)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, baseline_state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(baseline_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'll be using standard stochastic gradient descent with momentum to simplify the analysis and isolate the effects of batch normalization. The learning rate is tuned to be as high as can be sustained by the non-BN network without diverging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZWAvrpdSb9Te",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "lr = 0.035\n",
        "momentum = 0.9\n",
        "\n",
        "baseline_optimizer = nnx.Optimizer(baseline, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
        "candidate_optimizer = nnx.Optimizer(candidate, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss is measured using cross-entropy, which is standard for multi-label classification. Accuracy is measured on both the train and test datasets and tracked through the training run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VkkhUv_6b9Te",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "def loss_fn(model, batch, targets):\n",
        "    logits, activations = model(batch)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "    return loss, activations\n",
        "\n",
        "@nnx.jit\n",
        "def step_fn(model: nnx.Module, optimizer: nnx.Optimizer, batch: jax.Array, labels: jax.Array):\n",
        "    (loss, activations), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, batch, labels)\n",
        "    optimizer.update(grads)\n",
        "    return loss, activations, grads\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def accuracy(model: nnx.Module, batch: jax.Array, labels: jax.Array):\n",
        "    logits, _ = model(batch)\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    sum = jnp.sum(preds == labels)\n",
        "    acc = sum/logits.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def test_accuracy(model: nnx.Module, testloader):\n",
        "    acc, n = 0, 0\n",
        "    for batch, labels in testloader:\n",
        "        batch = jnp.array(batch)\n",
        "        labels = jnp.array(labels)\n",
        "        acc += accuracy(model, batch, labels)\n",
        "        n += 1\n",
        "    return acc/n\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of epochs is set to 39, which is roughly 15000 training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCGFto6Kb9Te",
        "outputId": "3158f923-c25e-4148-f4b7-9708443dfbb6",
        "tags": [
          "skip-execution",
          "hide-output"
        ]
      },
      "outputs": [],
      "source": [
        "from deepkit.loggers import DiskLogger\n",
        "\n",
        "\n",
        "num_epochs = 39\n",
        "num_steps = num_epochs*len(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IQnfnp8D0FBW",
        "tags": [
          "hide-input",
          "hide-output",
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "baseline_train_accs, candidate_train_accs = [], []\n",
        "baseline_test_accs, candidate_test_accs = [], []\n",
        "baseline_train_losses, candidate_train_losses = [], []\n",
        "baseline_activations_logger = DiskLogger(\"baseline_activations\")\n",
        "candidate_activations_logger = DiskLogger(\"candidate_activations\")\n",
        "baseline_ics_results, candidate_ics_results = [], []\n",
        "baseline_loss_landscape_ranges, candidate_loss_landscape_ranges = [], []\n",
        "baseline_grad_landscape_ranges, candidate_grad_landscape_ranges = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's Set up the Training Loop\n",
        "\n",
        "The train loop is quite involved because we need to calculate and track all the metrics. Both the baseline and candidate models are trained in the same loop. At each step, the internal covariate shift, loss landscape smoothness and gradient predictiveness calculations are performed for both models. Both the gradient and loss landscape measurement steps are capped at roughly 1.6 times the gradient.\n",
        "\n",
        "I have moved the measurements to a separate library [Deepkit](https://github.com/novatar53/deepkit) to save some space here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "2nwV6fYtb9Te",
        "outputId": "fe108dcf-2b1a-4fba-bfd0-84bd1f7a41f2",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "from deepkit.internal_covariate_shift import (\n",
        "    santurkar_ics_step,\n",
        "    loss_landscape_step,\n",
        "    grad_landscape_step\n",
        ")\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch, labels in train_loader:\n",
        "          batch = jnp.array(batch)\n",
        "          labels = jnp.array(labels)\n",
        "\n",
        "          baseline_optimizer_copy = baseline_optimizer.__deepcopy__()\n",
        "          baseline.train()\n",
        "          baseline_loss, baseline_activations, baseline_grads = (\n",
        "              step_fn(baseline, baseline_optimizer, batch, labels)\n",
        "          )\n",
        "          baseline_train_losses.append(baseline_loss)\n",
        "\n",
        "          # Calculate ICS\n",
        "          baseline_optimizer_copy.model.eval()\n",
        "          baseline_ics_measures = santurkar_ics_step(baseline_optimizer_copy,\n",
        "                                           baseline_grads,\n",
        "                                           batch,\n",
        "                                           labels)\n",
        "          baseline_ics_results.append(baseline_ics_measures)\n",
        "\n",
        "          # Calculate loss landscape\n",
        "          baseline_copy = baseline.__deepcopy__()\n",
        "          baseline_copy.eval()\n",
        "          baseline_loss_range = (\n",
        "              loss_landscape_step(baseline_copy,\n",
        "                                  batch,\n",
        "                                  labels,\n",
        "                                  baseline_grads,\n",
        "                                  max_step=10,\n",
        "                                  step_size=1,\n",
        "                                  lr=lr\n",
        "             )\n",
        "          )\n",
        "          baseline_loss_landscape_ranges.append(baseline_loss_range)\n",
        "\n",
        "          # Calculate gradient predictiveness\n",
        "          baseline_grad_range = (\n",
        "              grad_landscape_step(baseline_copy,\n",
        "                                  batch,\n",
        "                                  labels,\n",
        "                                  baseline_grads,\n",
        "                                  max_step=10,\n",
        "                                  step_size=2,\n",
        "                                  lr=lr\n",
        "             )\n",
        "          )\n",
        "          baseline_grad_landscape_ranges.append(baseline_grad_range)\n",
        "\n",
        "\n",
        "          candidate_optimizer_copy = candidate_optimizer.__deepcopy__()\n",
        "          candidate.train()\n",
        "          candidate_loss, candidate_activations, candidate_grads = (\n",
        "              step_fn(candidate, candidate_optimizer, batch, labels)\n",
        "          )\n",
        "          candidate_train_losses.append(candidate_loss)\n",
        "\n",
        "          # Calculate ICS\n",
        "          candidate_optimizer_copy.model.eval()\n",
        "          candidate_ics_measures = santurkar_ics_step(candidate_optimizer_copy,\n",
        "                                           candidate_grads,\n",
        "                                           batch,\n",
        "                                           labels)\n",
        "          candidate_ics_results.append(candidate_ics_measures)\n",
        "          \n",
        "          # Calculate loss landscape\n",
        "          candidate_copy = candidate.__deepcopy__()\n",
        "          candidate_copy.eval()\n",
        "          candidate_loss_range = (\n",
        "            loss_landscape_step(candidate_copy,\n",
        "                                batch,\n",
        "                                labels,\n",
        "                                candidate_grads,\n",
        "                                max_step=10,\n",
        "                                step_size=1,\n",
        "                                lr=lr\n",
        "             )\n",
        "          )\n",
        "          candidate_loss_landscape_ranges.append(candidate_loss_range)\n",
        "\n",
        "          # Calculate gradient predictiveness\n",
        "          candidate_grad_range = (\n",
        "            grad_landscape_step(candidate_copy,\n",
        "                                batch,\n",
        "                                labels,\n",
        "                                candidate_grads,\n",
        "                                max_step=10,\n",
        "                                step_size=2,\n",
        "                                lr=lr\n",
        "             )\n",
        "          )\n",
        "          candidate_grad_landscape_ranges.append(candidate_grad_range)\n",
        "\n",
        "\n",
        "          baseline.eval()\n",
        "          baseline_acc = accuracy(baseline, batch, labels)\n",
        "          baseline_train_accs.append(baseline_acc)\n",
        "\n",
        "          candidate.eval()\n",
        "          candidate_acc = accuracy(candidate, batch, labels)\n",
        "          candidate_train_accs.append(candidate_acc)\n",
        "\n",
        "          if i % 200 == 0:\n",
        "            baseline_test_acc = test_accuracy(baseline, test_loader)\n",
        "            candidate_test_acc = test_accuracy(candidate, test_loader)\n",
        "            baseline_test_accs.append(baseline_test_acc)\n",
        "            candidate_test_accs.append(candidate_test_acc)\n",
        "            baseline_activations_logger.log(i, baseline_activations)\n",
        "            candidate_activations_logger.log(i, candidate_activations)\n",
        "          if i % 20 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"iter: {i} | baseline loss: {baseline_loss:0.4f} | \"\n",
        "                  f\"candidate loss: {candidate_loss:0.4f} | \"\n",
        "                  f\"baseline train acc: {baseline_acc:0.2f} | \"\n",
        "                  f\"candidate train acc: {candidate_acc:0.2f} | \"\n",
        "                  f\"baseline test acc: {baseline_test_acc: 0.2f} | \"\n",
        "                  f\"candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "            axes[0].plot(baseline_train_losses, alpha=0.9,\n",
        "                         label=\"Without BatchNorm\")\n",
        "            axes[0].plot(candidate_train_losses, alpha=0.5,\n",
        "                         label=\"With BatchNorm\")\n",
        "            axes[0].set_title(\"Loss\")\n",
        "            axes[1].plot(baseline_train_accs, alpha=0.9,\n",
        "                         label=\"Without BatchNorm\")\n",
        "            axes[1].plot(candidate_train_accs, alpha=0.5,\n",
        "                         label=\"With BatchNorm\")\n",
        "            axes[1].set_title(\"Train Accuracy\")\n",
        "            axes[2].plot(baseline_test_accs, label=\"Without Batchnorm\")\n",
        "            axes[2].plot(candidate_test_accs, label=\"With Batchnorm\")\n",
        "            axes[2].set_title(\"Test Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "          i += 1\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkhCEDVoJ8rb"
      },
      "source": [
        "## Ioffe et. al ICS Measurement i.e. Distributional Shift\n",
        "\n",
        "First, let's plot the activation distributions in the same manner as Part 1. Initially, the BN activations are much more widely distributed due to scaling. However, this changes quickly as training progresses.\n",
        "\n",
        "For some layers, the non-BN activations appear to shift quite significantly, while for others, they remain more stable than the BN ones. The FC network in Part 1 had just 4 layers compared to the 13 layers here, suggesting that lower distributional shift due to BN is more significant in shallow networks than deeper ones. \n",
        "\n",
        "This also indicates that more stable activation distributions might be a secondary effect of BN rather than the cause of its effectiveness. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "unaL0Z6SeHw4",
        "outputId": "327d222a-1e04-434e-fe5b-7e22e94e86ee",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "from matplotlib import colormaps as cm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "layers = [f\"conv_{i}\" for i in range(10)]\n",
        "\n",
        "fig, axs = plt.subplots(10, 3, figsize=(15,8), constrained_layout=True)\n",
        "axs = axs.flatten()\n",
        "\n",
        "def update(frame):\n",
        "    baseline_activations = baseline_activations_logger[frame]\n",
        "    candidate_activations = candidate_activations_logger[frame]\n",
        "    for layer_idx, layer in enumerate(layers):\n",
        "        axs[layer_idx].cla()\n",
        "        layer_baseline_activations = baseline_activations[layer].flatten()\n",
        "        layer_baseline_mean = layer_baseline_activations.mean()\n",
        "        layer_baseline_std = layer_baseline_activations.std()\n",
        "        axs[layer_idx].hist(layer_baseline_activations, color=cm[\"Blues\"](50), bins=60, alpha=1.0)\n",
        "\n",
        "        layer_candidate_activations = candidate_activations[layer].flatten()\n",
        "        layer_candidate_mean = layer_candidate_activations.mean()\n",
        "        layer_candidate_std = layer_candidate_activations.std()\n",
        "        axs[layer_idx].hist(layer_candidate_activations, color=cm[\"Reds\"](90),  bins=60, alpha=0.5)\n",
        "        axs[layer_idx].set_title(f\"{layer} Outputs - Iteration:{frame}\")\n",
        "\n",
        "        axs[layer_idx].margins(x=0, y=0)\n",
        "        axs[layer_idx].set_xlim(-8, 8)\n",
        "        axs[layer_idx].legend([f\"Baseline: {layer_baseline_mean:0.2f} ± {layer_baseline_std:0.2f}\",\n",
        "                              f\"Candidate:{layer_candidate_mean:0.2f} ± {layer_candidate_std:0.2f}\"])\n",
        "\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(baseline_activations_logger), interval=300, repeat=True)\n",
        "plt.close(fig)\n",
        "video_html = ani.to_html5_video().replace('<video', '<video muted')\n",
        "HTML(video_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnie4bOgIe6k"
      },
      "source": [
        "## Santurkar et. al Internal Covariate Shift\n",
        "\n",
        "As we observed, there is no significant correlation between distributional shift in the activations and the performance of BN.\n",
        "Rather than the activations, this definition measures the change in gradient norms for each layer before and after the previous layers are updated: \n",
        "\n",
        "Mathematically, internal covariate shift (ICS) of activation $i$ at time $t$ is the difference $||G_{t,i} − G^′_{t,i}||^2$, where\n",
        "$$\\begin{align}\n",
        "G_{t,i} &= ∇_{W^{(t)}_i} \\mathcal{L}(W_{1}^{(t)},..., W^{(t)}_k ; x(t), y(t)) \\\\\n",
        "G^′_{t,i} &= ∇_{W^{(t)}_i} \\mathcal{L}(W^{(t+1)}_1 ,..., W^{(t+1)}_{i−1}, W^{(t)}_i, W^{(t)}_{i+1},..., W^{(t)}_k; x(t), y(t)).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is more of an *operational* definition that measures the impact on the gradient of a particular layer due covariate shift. This is useful because the gradient is ultimately what impacts learning. A sensitive loss landscape may cause a significant change in the gradient of a layer even without significant changes in its inputs. Conversely, a less sensitive loss landscape may cause little change in the gradient inspite of significant changes to the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip-execution",
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "conv_layer_ids = [0,3,5,7,9]\n",
        "\n",
        "fig, axes = plt.subplots(5, 2, figsize=(10,10), constrained_layout=True)\n",
        "\n",
        "for row_id, conv_layer_id in enumerate(conv_layer_ids):\n",
        "\n",
        "  baseline_kernel_ics_l2_norms = [ n[conv_layer_id][0].conv.kernel.value \n",
        "                                  for n in baseline_ics_results ]\n",
        "  candidate_kernel_ics_l2_norms = [ n[conv_layer_id][0].conv.kernel.value \n",
        "                                   for n in candidate_ics_results ]\n",
        "\n",
        "  axes[row_id, 0].plot(baseline_kernel_ics_l2_norms, label=\"Without BatchNorm\", alpha=0.9)\n",
        "  axes[row_id, 0].plot(candidate_kernel_ics_l2_norms, label=\"With BatchNorm\", alpha=0.5)\n",
        "  axes[row_id, 0].set_yscale('log')\n",
        "  title = \"\"\n",
        "  if row_id == 0:\n",
        "    title += f\"ICS L2 Differences:\\n\"\n",
        "  title += f\"Conv-{conv_layer_id}\"\n",
        "  axes[row_id, 0].set_title(title)\n",
        "  \n",
        "\n",
        "  baseline_kernel_ics_cosines =  [ n[conv_layer_id][1].conv.kernel.value \n",
        "                                  for n in baseline_ics_results ]\n",
        "  candidate_kernel_ics_cosines = [ n[conv_layer_id][1].conv.kernel.value \n",
        "                                  for n in candidate_ics_results ]\n",
        "\n",
        "  axes[row_id, 1].plot(baseline_kernel_ics_cosines, label=\"Without BatchNorm\", alpha=0.9)\n",
        "  axes[row_id, 1].plot(candidate_kernel_ics_cosines, label=\"With BatchNorm\", alpha=0.5)\n",
        "  title = \"\"\n",
        "  if row_id == 0:\n",
        "    title += f\"ICS Cosines:\\n\"\n",
        "  title += f\" Conv-{conv_layer_id}\"\n",
        "  axes[row_id, 1].set_title(title)\n",
        "\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuBR7anozIVp"
      },
      "source": [
        "## Loss Landscape Smoothness i.e. Lipschitzness\n",
        "\n",
        "Lipschnitzness is a way to upper-bound the steepness of a function. It is defined as: \n",
        "\n",
        "$|| f(x) - f(y) || \\leq L|| x - y ||$ for all $x,y \\in \\mathbb{R}^n$\n",
        "\n",
        "where $L$ is the Lipschitz constant. \n",
        "\n",
        "If $f(x)$ is the loss function, smaller $L$ indicates a 'flatter' loss surface. This is hard to measure because it involves an intractable computation over all pairs $(x, y)$. However, we can approximate it by taking a few steps in the gradient direction at every point in the optimization trajectory and measuring $|| f(x^`) - f(x) ||$ at each step.\n",
        "\n",
        "In simple terms, a bumpy loss surface should show larger variation in the loss values along the optimization trajectory, while a smoother one should show smaller variation. \n",
        "\n",
        "In the plot below, the non-BN model shows significantly larger variation in loss values at each step, especially earlier in the training run. Remember, the y-axis is log-scale, so the spikes of the baseline model plot are orders of magnitude higher than the candidate ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "1Mknyo9N1hzg",
        "outputId": "4e63c6db-31e6-4710-bd6f-e4a1d6be1d73",
        "tags": [
          "skip-execution",
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "baseline_min_vals = [ s[0] for s in baseline_loss_landscape_ranges ]\n",
        "baseline_max_vals = [ s[1] for s in baseline_loss_landscape_ranges ]\n",
        "baseline_x_vals = list(range(len(baseline_min_vals)))\n",
        "\n",
        "plt.fill_between(baseline_x_vals, baseline_min_vals, baseline_max_vals, \n",
        "                 alpha=1.0, label=\"Without BatchNorm\")\n",
        "\n",
        "candidate_min_vals = [ s[0] for s in candidate_loss_landscape_ranges ]\n",
        "candidate_max_vals = [ s[1] for s in candidate_loss_landscape_ranges ]\n",
        "candidate_x_vals = list(range(len(candidate_min_vals)))\n",
        "\n",
        "plt.fill_between(candidate_x_vals, candidate_min_vals, candidate_max_vals, \n",
        "                 alpha=0.9, label=\"With BatchNorm\")\n",
        "\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.title(\"Lipschtzness of the Loss\")\n",
        "plt.ylabel(\"Loss Range\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqy200uCdEYF"
      },
      "source": [
        "## More Predictive Gradients\n",
        "\n",
        "Lipschitzness can be defined for the gradient as well and can be approximated by how much the L2 norm of the gradient changes in the gradient direction at a point $x$. This is also called gradient predictiveness. This comes from the idea that a more predictive gradient should remain stable and not change significantly in the gradient direction. \n",
        "\n",
        "## \"Effective\" $\\beta$-Smoothness \n",
        "\n",
        "This follows the notion of gradient predictiveness above. Similar to Lipschitzness, $\\beta$-smoothness provides an upper bound on the rate of change of the gradient. A smaller rate of change indicates a smoother loss surface. Formally,\n",
        "\n",
        "$\\| \\nabla f(x) - \\nabla f(y) \\| \\leq \\beta \\| x - y \\| \\quad \\text{for all } x, y \\in \\mathbb{R}^n $\n",
        "\n",
        "Once again this is intractable to calculate, however we can approximate it at each point in the optimization trajectory by taking a few steps in the gradient direction and recording the maximum change in the gradient norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "9hz3rtwadEYF",
        "outputId": "555591f0-f185-4d68-b00e-d51303c8e4b3",
        "tags": [
          "skip-execution",
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "baseline_min_vals = [ s[0] for s in baseline_grad_landscape_ranges ]\n",
        "baseline_max_vals = [ s[1] for s in baseline_grad_landscape_ranges ]\n",
        "baseline_x_vals = list(range(len(baseline_min_vals)))\n",
        "\n",
        "plt.fill_between(baseline_x_vals, baseline_min_vals, baseline_max_vals, \n",
        "                 alpha=1.0, label=\"Without BatchNorm\")\n",
        "\n",
        "candidate_min_vals = [ s[0] for s in candidate_grad_landscape_ranges ]\n",
        "candidate_max_vals = [ s[1] for s in candidate_grad_landscape_ranges ]\n",
        "candidate_x_vals = list(range(len(candidate_min_vals)))\n",
        "\n",
        "plt.fill_between(candidate_x_vals, candidate_min_vals, candidate_max_vals, \n",
        "                 alpha=0.9, label=\"With BatchNorm\")\n",
        "\n",
        "#plt.ylim(0, 250)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"L2 Differences\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.title(\"Gradient Predictiveness\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sefHUt_ElOMT"
      },
      "source": [
        "## Favorable Initialization\n",
        "\n",
        "BN leads to more favourable initialization for the weights. Mathematically, \n",
        "\n",
        "$$\n",
        "|| W_0 - \\hat{W}^∗ ||^2 ≤ ||W_0 − W^∗||^2 − \\frac{1}{||W^∗||^2} ( ||W^∗||^2 −〈W^∗, W_0〉)^2 ,\n",
        "$$\n",
        " \n",
        "where $W_0$ is the initial weights and $\\hat{W}^*$ and $W^*$ are the nearest local minima for the BN and non-BN networks respectively.\n",
        "\n",
        "Empirically, we can test this by measuring the L2 differences between the initial weights and the optima for both networks. As seen in the plot below, the value is consistently smaller for the BN network for all the convolutional layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9jq0y0Yloent",
        "outputId": "2647a178-cc3d-4f9b-d378-bc7733b90d29",
        "tags": [
          "skip-execution",
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "from deepkit import utils\n",
        "\n",
        "\n",
        "init_norms = []\n",
        "\n",
        "for conv_id in range(len(baseline_state.convs)):\n",
        "  w0 = baseline_state.convs[conv_id].conv.kernel.value\n",
        "  baseline_w = baseline.convs[conv_id].conv.kernel.value\n",
        "  candidate_w = candidate.convs[conv_id].conv.kernel.value\n",
        "  ##norm = layer_l2_diff(state.fc1, candidate.fc1)\n",
        "  #print(norm)\n",
        "  delta1 = jax.tree_util.tree_map(\n",
        "    lambda x, y: jnp.sqrt(jnp.sum(jnp.square(x.flatten() - y.flatten()))), w0, baseline_w)\n",
        "  delta2 = jax.tree_util.tree_map(\n",
        "    lambda x, y: jnp.sqrt(jnp.sum(jnp.square(x.flatten() - y.flatten()))), w0, candidate_w)\n",
        "  init_norms.append((conv_id, delta1, delta2))\n",
        "  #print(f\"Conv-{conv_id}: Baseline ||w_0 - w|| {delta1:0.2f}, Candidate ||w_0, w|| {delta2:0.2f})\")\n",
        "\n",
        "\n",
        "x = [ t[0] for t in init_norms ]\n",
        "baseline_norms = [ t[1] for t in init_norms ]\n",
        "candidate_norms = [ t[2] for t in init_norms ]\n",
        "plt.plot(x, baseline_norms, label=\"Baseline ||W_0 - W||\")\n",
        "plt.plot(x, candidate_norms, label=\"Candidate ||W_0 - W||\")\n",
        "plt.xticks(x)\n",
        "plt.xlabel(\"Conv Layer\")\n",
        "plt.ylabel(\"L2 Differences\")\n",
        "plt.legend()\n",
        "plt.title(\"L2 Differences b/w Init and Learned Params\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
