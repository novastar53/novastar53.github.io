{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0QZXjN-b9Tc"
      },
      "source": [
        "# How and Why BatchNorm Works - Part 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxG2sVcsb9Td"
      },
      "source": [
        "In Part 1, we showed how using batch normalization can help train neural networks by reducing internal covariate shift and smoothing gradient updates. We also touch upon the idea that it makes the loss surface smoother and more convex (residual connections have a similar effect).\n",
        "\n",
        "In their 2018 paper, Santukar et al. argue that internal covariate shift isn't substantially reduced at all. In fact, when additional noise is added to the activations to force covariate shift, batch normalization continues to improve training performance. This is despite no significant reduction in covariate shift. They argue that the major reason for improved performance is the smoother loss surface.\n",
        "\n",
        "However, we saw in our own experiments using a shallow neural network that the reduction in covariate shift is substantial. Let's get to the bottom of this.\n",
        "\n",
        "In this experiment, we will start by reproducing the results from Santukar et al. Then we will try and understand why their results and our previous experiment differ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJOSV50b9Td"
      },
      "source": [
        "## Let's start as usual by setting up the dataset\n",
        "We will use the CIFAR-10 dataset for this experiment.\n",
        "The CIFAR-10 dataset is a widely used benchmark in machine learning for evaluating image classification algorithms. It consists of 60,000 color images, each sized 32x32 pixels, divided into 10 distinct classes such as airplanes, automobiles, birds, cats, and ships. The dataset is split into 50,000 training images and 10,000 test images, with an equal number of examples per class. Created by the Canadian Institute for Advanced Research, CIFAR-10 is challenging due to its low resolution and high intra-class variability. It serves as a foundational dataset for developing and comparing deep learning models, especially convolutional neural networks (CNNs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpsseJYsc0Y1"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/deepkit\n",
        "    !cd deepkit && git pull && uv build . --wheel && pip uninstall deepkit -y && pip install ./dist/* --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7bhT4GGb9Td"
      },
      "source": [
        "\n",
        "## Next, let's set up the model\n",
        "We'll be using the standard VGG-Net architecture as per the paper.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPSb_PkVb9Td"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "\n",
        "kernel_init = nnx.initializers.glorot_normal()\n",
        "\n",
        "\n",
        "class VGGBlock(nnx.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
        "        self.conv = nnx.Conv(in_features=in_features,\n",
        "                             out_features=out_features,\n",
        "                             kernel_size=(3, 3),\n",
        "                             kernel_init=kernel_init,\n",
        "                             padding='SAME',\n",
        "                             rngs=rngs)\n",
        "        self.bn = nnx.BatchNorm(num_features=out_features, momentum=0.90, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.conv(x)\n",
        "        conv_activation = x\n",
        "        x = self.bn(x)\n",
        "        x = nnx.relu(x)\n",
        "        return x, conv_activation\n",
        "\n",
        "class VGGNet(nnx.Module):\n",
        "    def __init__(self, rngs: nnx.Rngs):\n",
        "        self.convs = [\n",
        "            VGGBlock(in_features=3, out_features=64,  rngs=rngs),\n",
        "            VGGBlock(in_features=64, out_features=64,  rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=64, out_features=128, rngs=rngs),\n",
        "            VGGBlock(in_features=128, out_features=128, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=128, out_features=256, rngs=rngs),\n",
        "            VGGBlock(in_features=256, out_features=256, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=256, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "        ]\n",
        "\n",
        "        self.fc1 = nnx.Linear(in_features=512, out_features=96, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.fc2 = nnx.Linear(in_features=96, out_features=96, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.out = nnx.Linear(in_features=96, out_features=10,  kernel_init=kernel_init, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        activations = {}\n",
        "        max_pool_after = [1, 3, 5, 7, 9]\n",
        "        for conv_idx in range(len(self.convs)):\n",
        "            layer = self.convs[conv_idx]\n",
        "            x, act = layer(x)\n",
        "            activations[f\"conv_{conv_idx}\"] = act\n",
        "            if conv_idx in max_pool_after:\n",
        "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        x = x.squeeze()\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        activations[\"fc1\"] = x\n",
        "        x = nnx.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        activations[\"fc2\"] = x\n",
        "        x = nnx.relu(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        activations[\"out\"] = x\n",
        "        return x, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWUeygKCb9Te"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "candidate = VGGNet(rngs=rngs)\n",
        "graphdef, state = nnx.split(candidate)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLfMaFY4i0jj"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "baseline = VGGNet(rngs=rngs)\n",
        "\n",
        "class Dummy(nnx.Module):\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "# Remove the batchnorm layers\n",
        "for vgg in baseline.convs:\n",
        "  vgg.bn = lambda x: x\n",
        "graphdef, state = nnx.split(baseline)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWAvrpdSb9Te"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "baseline_opt = optax.chain(#optax.add_decayed_weights(5e-4),\n",
        "                           #optax.clip_by_global_norm(1.0),\n",
        "                           optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False)\n",
        "                           )\n",
        "candidate_opt = optax.chain(#optax.add_decayed_weights(5e-4),\n",
        "                            #optax.clip_by_global_norm(1.0),\n",
        "                            optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False)\n",
        "                            )\n",
        "\n",
        "baseline_optimizer = nnx.Optimizer(baseline, baseline_opt)\n",
        "candidate_optimizer = nnx.Optimizer(candidate, candidate_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkkhUv_6b9Te"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def loss_fn(model, batch, targets):\n",
        "    logits, activations = model(batch)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "    return loss, activations\n",
        "\n",
        "@nnx.jit\n",
        "def step_fn(model: nnx.Module, optimizer: nnx.Optimizer, batch: jax.Array, labels: jax.Array):\n",
        "    (loss, activations), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, batch, labels)\n",
        "    optimizer.update(grads)\n",
        "    return loss, activations, grads\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def accuracy(model: nnx.Module, batch: jax.Array, labels: jax.Array):\n",
        "    logits, _ = model(batch)\n",
        "    #probs = nnx.softmax(logits, axis=-1)\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    sum = jnp.sum(preds == labels)\n",
        "    acc = sum/logits.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def test_accuracy(model: nnx.Module, testloader):\n",
        "    acc, n = 0, 0\n",
        "    for batch, labels in testloader:\n",
        "        batch = jnp.array(batch)\n",
        "        labels = jnp.array(labels)\n",
        "        acc += accuracy(model, batch, labels)\n",
        "        n += 1\n",
        "    return acc/n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZasUa23U2JA"
      },
      "outputs": [],
      "source": [
        "def grad_norms(grads):\n",
        "  norms = jax.tree_util.tree_map(lambda x: jnp.linalg.norm(x), grads)\n",
        "  return norms\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def generate_random_code(length=6):\n",
        "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
        "\n",
        "class DiskLogger:\n",
        "    def __init__(self, name, save_dir=\"./logs\"):\n",
        "        self.run_code = generate_random_code()\n",
        "        self.save_dir = Path(save_dir) / self.run_code / name\n",
        "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.name = name\n",
        "        self.files = None\n",
        "\n",
        "    def log(self, step, x):\n",
        "        file_name = f\"{step:05d}.npz\"\n",
        "        file_path = self.save_dir / file_name\n",
        "        np.savez(file_path, **x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.save_dir))\n",
        "\n",
        "    def __call__(self):\n",
        "        if not self.files:\n",
        "          self.files = sorted([f for f in os.listdir(self.save_dir)])\n",
        "        for filename in self.files:\n",
        "          filepath = os.path.join(self.save_dir, filename)\n",
        "          data = np.load(filepath, allow_pickle=True)\n",
        "          yield data\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "       if not self.files:\n",
        "        self.files = sorted([f for f in os.listdir(self.save_dir)])\n",
        "       filepath = os.path.join(self.save_dir, self.files[i])\n",
        "       data = np.load(filepath, allow_pickle=True)\n",
        "       return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCGFto6Kb9Te"
      },
      "outputs": [],
      "source": [
        "from deepkit import load_CIFAR10\n",
        "\n",
        "num_epochs = 39\n",
        "\n",
        "train_loader, test_loader = load_CIFAR10(augment=False)\n",
        "num_steps = num_epochs*len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nwV6fYtb9Te"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "baseline.train()\n",
        "candidate.train()\n",
        "baseline_train_accs, candidate_train_accs = [], []\n",
        "baseline_test_accs, candidate_test_accs = [], []\n",
        "baseline_train_losses, candidate_train_losses = [], []\n",
        "#baseline_grad_norms, candidate_grad_norms = [], []\n",
        "#baseline_activations_log, candidate_activations_log = [], []\n",
        "\n",
        "baseline_activations_logger = DiskLogger(\"baseline_activations\")\n",
        "candidate_activations_logger = DiskLogger(\"candidate_activations\")\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch, labels in train_loader:\n",
        "          batch = jnp.array(batch)\n",
        "          labels = jnp.array(labels)\n",
        "          baseline.train()\n",
        "          candidate.train()\n",
        "          baseline_loss, baseline_activations, baseline_grads = step_fn(baseline, baseline_optimizer, batch, labels)\n",
        "          candidate_loss, candidate_activations, candidate_grads = step_fn(candidate, candidate_optimizer, batch, labels)\n",
        "          baseline_train_losses.append(baseline_loss)\n",
        "          candidate_train_losses.append(candidate_loss)\n",
        "          baseline.eval()\n",
        "          candidate.eval()\n",
        "          baseline_acc = accuracy(baseline, batch, labels)\n",
        "          candidate_acc = accuracy(candidate, batch, labels)\n",
        "          baseline_train_accs.append(baseline_acc)\n",
        "          candidate_train_accs.append(candidate_acc)\n",
        "          if i % 200 == 0:\n",
        "            baseline_test_acc = test_accuracy(baseline, test_loader)\n",
        "            candidate_test_acc = test_accuracy(candidate, test_loader)\n",
        "            baseline_test_accs.append(baseline_test_acc)\n",
        "            candidate_test_accs.append(candidate_test_acc)\n",
        "            baseline_activations_logger.log(i, baseline_activations)\n",
        "            candidate_activations_logger.log(i, candidate_activations)\n",
        "          if i % 20 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"iter: {i} | baseline loss: {baseline_loss:0.4f} | candidate loss: {candidate_loss:0.4f} | baseline train acc: {baseline_acc:0.2f} | candidate train acc: {candidate_acc:0.2f} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "            axes[0].plot(baseline_train_losses, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[0].plot(candidate_train_losses, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[0].set_title(\"Loss\")\n",
        "            axes[1].plot(baseline_train_accs, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[1].plot(candidate_train_accs, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[1].set_title(\"Train Accuracy\")\n",
        "            axes[2].plot(baseline_test_accs, label=\"Without Batchnorm\")\n",
        "            axes[2].plot(candidate_test_accs, label=\"With Batchnorm\")\n",
        "            axes[2].set_title(\"Test Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "          #print(f\"iter: {i} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "          i += 1\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unaL0Z6SeHw4"
      },
      "outputs": [],
      "source": [
        "from matplotlib import colormaps as cm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "layers = [f\"conv_{i}\" for i in range(10)] + [\"fc1\", \"fc2\", \"out\"]\n",
        "\n",
        "fig, axs = plt.subplots(13, 1, figsize=(5,24), constrained_layout=True)\n",
        "\n",
        "\n",
        "def update(frame):\n",
        "    baseline_activations = baseline_activations_logger[frame]\n",
        "    candidate_activations = candidate_activations_logger[frame]\n",
        "    for layer_idx, layer in enumerate(layers):\n",
        "        axs[layer_idx].cla()\n",
        "        layer_baseline_activations = baseline_activations[layer].flatten()\n",
        "        layer_baseline_mean = layer_baseline_activations.mean()\n",
        "        layer_baseline_std = layer_baseline_activations.std()\n",
        "        axs[layer_idx].hist(layer_baseline_activations, color=cm[\"Blues\"](50), bins=60, alpha=1.0)\n",
        "\n",
        "        layer_candidate_activations = candidate_activations[layer].flatten()\n",
        "        layer_candidate_mean = layer_candidate_activations.mean()\n",
        "        layer_candidate_std = layer_candidate_activations.std()\n",
        "        axs[layer_idx].hist(layer_candidate_activations, color=cm[\"Reds\"](90),  bins=60, alpha=0.5)\n",
        "        axs[layer_idx].set_title(f\"{layer} Outputs - Iteration:{frame}\")\n",
        "\n",
        "        axs[layer_idx].margins(x=0, y=0)\n",
        "        axs[layer_idx].set_xlim(-8, 8)\n",
        "        axs[layer_idx].legend([f\"Baseline: {layer_baseline_mean:0.2f} ± {layer_baseline_std:0.2f}\",\n",
        "                              f\"Candidate:{layer_candidate_mean:0.2f} ± {layer_candidate_std:0.2f}\"])\n",
        "\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(baseline_activations_logger), interval=300, repeat=True)\n",
        "plt.close(fig)\n",
        "video_html = ani.to_html5_video().replace('<video', '<video muted')\n",
        "HTML(video_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeXE_lRPeHw4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}