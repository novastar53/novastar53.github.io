{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0QZXjN-b9Tc"
      },
      "source": [
        "# How and Why BatchNorm Works - Part 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxG2sVcsb9Td"
      },
      "source": [
        "In Part 1, we showed that batch normalization (BN) can help train neural networks by reducing internal covariate shift (ICS) and smoothing gradient updates. ICS was defined in terms of activation distributions by Ioffe and Szegedy (2015). For our fully-connected network, we indeed observed that the activation distributions for the network with BN were comparatively stable compared to those for the network without BN.\n",
        "We also touched upon the idea that BN has a smoothing effect on the loss function making it more convex (residual connections have a similar effect). \n",
        "\n",
        "In their 2018 paper, Santukar et al. used a new definition of ICS and argued that BN has no substantial effect on it. In fact, BN improves training performance even when random noise is added to the activations to force covariate shift.  They argue that the major reason for improved performance loss surface smoothing.\n",
        "\n",
        "\n",
        "We define internal covariate shift (ICS) of activation i at time t to be the difference $||Gt,i − G′ t,i||^2$, where \n",
        "$$\n",
        "G_{t,i} = ∇W (t) i L(W (t) 1 , . . . , W (t) k ; x(t), y(t)) G′ t,i = ∇W (t) i L(W (t+1) 1 , . . . , W (t+1) i−1 , W (t) i , W (t) i+1, . . . , W (t) k ; x(t), y(t)).\n",
        "$$\n",
        "\n",
        "In this experiment, we will start by reproducing the results from Santukar et al. Then we will try and understand why their results and our previous experiment differ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJOSV50b9Td"
      },
      "source": [
        "## Let's start as usual by setting up the dataset\n",
        "We will use the CIFAR-10 dataset for this experiment.\n",
        "The CIFAR-10 dataset is a widely used benchmark in machine learning for evaluating image classification algorithms. It consists of 60,000 color images, each sized 32x32 pixels, divided into 10 distinct classes such as airplanes, automobiles, birds, cats, and ships. The dataset is split into 50,000 training images and 10,000 test images, with an equal number of examples per class. Created by the Canadian Institute for Advanced Research, CIFAR-10 is challenging due to its low resolution and high intra-class variability. It serves as a foundational dataset for developing and comparing deep learning models, especially convolutional neural networks (CNNs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpsseJYsc0Y1",
        "outputId": "a64992cb-8a31-420d-aa09-2277554484ea"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/deepkit\n",
        "    !cd deepkit && git pull && uv build . --wheel && pip uninstall deepkit -y && pip install ./dist/* --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfbo4k1b9Td",
        "outputId": "75a78d61-011c-4f69-a5fd-a90cdfc3e063"
      },
      "outputs": [],
      "source": [
        "from deepkit import load_CIFAR10\n",
        "train_loader, test_loader = load_CIFAR10(augment=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7bhT4GGb9Td"
      },
      "source": [
        "\n",
        "## Next, let's set up the model\n",
        "We'll be using the standard VGG-Net architecture as per the paper.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GPSb_PkVb9Td"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "\n",
        "kernel_init = nnx.initializers.glorot_normal()\n",
        "\n",
        "class VGGBlock(nnx.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
        "        self.conv = nnx.Conv(in_features=in_features,\n",
        "                             out_features=out_features,\n",
        "                             kernel_size=(3, 3),\n",
        "                             kernel_init=kernel_init,\n",
        "                             padding='SAME',\n",
        "                             rngs=rngs)\n",
        "        self.bn = nnx.BatchNorm(num_features=out_features, momentum=0.90, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.conv(x)\n",
        "        conv_activation = x\n",
        "        x = self.bn(x)\n",
        "        x = nnx.relu(x)\n",
        "        return x, conv_activation\n",
        "\n",
        "class VGGNet(nnx.Module):\n",
        "    def __init__(self, rngs: nnx.Rngs):\n",
        "        self.convs = [\n",
        "            VGGBlock(in_features=3, out_features=64,  rngs=rngs),\n",
        "            VGGBlock(in_features=64, out_features=64,  rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=64, out_features=128, rngs=rngs),\n",
        "            VGGBlock(in_features=128, out_features=128, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=128, out_features=256, rngs=rngs),\n",
        "            VGGBlock(in_features=256, out_features=256, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=256, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
        "        ]\n",
        "\n",
        "        self.fc1 = nnx.Linear(in_features=512, out_features=512, kernel_init=kernel_init, rngs=rngs)\n",
        "        self.out = nnx.Linear(in_features=512, out_features=10,  kernel_init=kernel_init, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        activations = {}\n",
        "        max_pool_after = [1, 3, 5, 7, 9]\n",
        "        for conv_idx in range(len(self.convs)):\n",
        "            layer = self.convs[conv_idx]\n",
        "            x, act = layer(x)\n",
        "            activations[f\"conv_{conv_idx}\"] = act\n",
        "            if conv_idx in max_pool_after:\n",
        "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        x = x.squeeze()\n",
        "        x = self.fc1(x)\n",
        "        activations[\"fc1\"] = x\n",
        "        x = nnx.relu(x)\n",
        "        x = self.out(x)\n",
        "        return x, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "AWUeygKCb9Te",
        "outputId": "ef550e04-49d3-40dc-f1f6-1af346a4d649"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "candidate = VGGNet(rngs=rngs)\n",
        "graphdef, state = nnx.split(candidate)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "kLfMaFY4i0jj",
        "outputId": "6a6126ae-67c9-48c4-f370-db3e2892b4d6"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.key(1337)\n",
        "rngs = nnx.Rngs(rng_key)\n",
        "baseline = VGGNet(rngs=rngs)\n",
        "\n",
        "class Dummy(nnx.Module):\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "for vgg in baseline.convs:\n",
        "  vgg.bn = Dummy()\n",
        "graphdef, state = nnx.split(baseline)\n",
        "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
        "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
        "nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZWAvrpdSb9Te"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "baseline_opt = optax.chain(optax.clip_by_global_norm(1.0), optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
        "candidate_opt = optax.chain(optax.clip_by_global_norm(1.0), optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
        "\n",
        "baseline_optimizer = nnx.Optimizer(baseline, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
        "candidate_optimizer = nnx.Optimizer(candidate, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VkkhUv_6b9Te"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def loss_fn(model, batch, targets):\n",
        "    logits, activations = model(batch)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "    return loss, activations\n",
        "\n",
        "@nnx.jit\n",
        "def step_fn(model: nnx.Module, optimizer: nnx.Optimizer, batch: jax.Array, labels: jax.Array):\n",
        "    (loss, activations), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, batch, labels)\n",
        "    optimizer.update(grads)\n",
        "    return loss, activations, grads\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def accuracy(model: nnx.Module, batch: jax.Array, labels: jax.Array):\n",
        "    logits, _ = model(batch)\n",
        "    #probs = nnx.softmax(logits, axis=-1)\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    sum = jnp.sum(preds == labels)\n",
        "    acc = sum/logits.shape[0]\n",
        "    return acc\n",
        "\n",
        "\n",
        "def test_accuracy(model: nnx.Module, testloader):\n",
        "    acc, n = 0, 0\n",
        "    for batch, labels in testloader:\n",
        "        batch = jnp.array(batch)\n",
        "        labels = jnp.array(labels)\n",
        "        acc += accuracy(model, batch, labels)\n",
        "        n += 1\n",
        "    return acc/n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZZasUa23U2JA"
      },
      "outputs": [],
      "source": [
        "def grad_norms(grads):\n",
        "  norms = jax.tree_util.tree_map(lambda x: jnp.linalg.norm(x), grads)\n",
        "  return norms\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def generate_random_code(length=6):\n",
        "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
        "\n",
        "class DiskLogger:\n",
        "    def __init__(self, name, save_dir=\"./logs\"):\n",
        "        self.run_code = generate_random_code()\n",
        "        self.save_dir = Path(save_dir) / self.run_code / name\n",
        "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.name = name\n",
        "\n",
        "    def log(self, step, x):\n",
        "        file_name = f\"{step:05d}.npz\"\n",
        "        file_path = self.save_dir / file_name\n",
        "        np.savez(file_path, **x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.save_dir))\n",
        "\n",
        "    def __call__(self):\n",
        "        files = sorted([f for f in os.listdir(self.save_dir)])\n",
        "        for filename in files:\n",
        "          print(f\"Reading {self.save_dir}/{filename}\")\n",
        "          filepath = os.path.join(self.save_dir, filename)\n",
        "          data = np.load(filepath, allow_pickle=True)\n",
        "          yield data\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "       files = sorted([f for f in os.listdir(self.save_dir)])\n",
        "       filepath = os.path.join(self.save_dir, files[i])\n",
        "       data = np.load(filepath, allow_pickle=True)\n",
        "       return data \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DCGFto6Kb9Te"
      },
      "outputs": [],
      "source": [
        "num_epochs = 39\n",
        "num_steps = num_epochs*len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "2nwV6fYtb9Te",
        "outputId": "52322844-e9ba-4b8b-f1ec-c41614fe77a8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "i = 0\n",
        "baseline.train()\n",
        "candidate.train()\n",
        "baseline_train_accs, candidate_train_accs = [], []\n",
        "baseline_test_accs, candidate_test_accs = [], []\n",
        "baseline_train_losses, candidate_train_losses = [], []\n",
        "#baseline_grad_norms, candidate_grad_norms = [], []\n",
        "#baseline_activations_log, candidate_activations_log = [], []\n",
        "\n",
        "baseline_activations_logger = DiskLogger(\"baseline_activations\")\n",
        "candidate_activations_logger = DiskLogger(\"candidate_activations\")\n",
        "\n",
        "try:\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch, labels in train_loader:\n",
        "          batch = jnp.array(batch)\n",
        "          labels = jnp.array(labels)\n",
        "          baseline.train()\n",
        "          candidate.train()\n",
        "          baseline_loss, baseline_activations, baseline_grads = step_fn(baseline, baseline_optimizer, batch, labels)\n",
        "          candidate_loss, candidate_activations, candidate_grads = step_fn(candidate, candidate_optimizer, batch, labels)\n",
        "          baseline_train_losses.append(baseline_loss)\n",
        "          candidate_train_losses.append(candidate_loss)\n",
        "          baseline.eval()\n",
        "          candidate.eval()\n",
        "          baseline_acc = accuracy(baseline, batch, labels)\n",
        "          candidate_acc = accuracy(candidate, batch, labels)\n",
        "          baseline_train_accs.append(baseline_acc)\n",
        "          candidate_train_accs.append(candidate_acc)\n",
        "          if i % 200 == 0:\n",
        "            baseline_test_acc = test_accuracy(baseline, test_loader)\n",
        "            candidate_test_acc = test_accuracy(candidate, test_loader)\n",
        "            baseline_test_accs.append(baseline_test_acc)\n",
        "            candidate_test_accs.append(candidate_test_acc)\n",
        "          if i % 20 == 0:\n",
        "            baseline_activations_logger.log(i, baseline_activations)\n",
        "            candidate_activations_logger.log(i, candidate_activations)\n",
        "            clear_output(wait=True)\n",
        "            print(f\"iter: {i} | baseline loss: {baseline_loss:0.4f} | candidate loss: {candidate_loss:0.4f} | baseline train acc: {baseline_acc:0.2f} | candidate train acc: {candidate_acc:0.2f} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "            axes[0].plot(baseline_train_losses, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[0].plot(candidate_train_losses, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[0].set_title(\"Loss\")\n",
        "            axes[1].plot(baseline_train_accs, alpha=0.9, label=\"Without BatchNorm\")\n",
        "            axes[1].plot(candidate_train_accs, alpha=0.5, label=\"With BatchNorm\")\n",
        "            axes[1].set_title(\"Train Accuracy\")\n",
        "            axes[2].plot(baseline_test_accs, label=\"Without Batchnorm\")\n",
        "            axes[2].plot(candidate_test_accs, label=\"With Batchnorm\")\n",
        "            axes[2].set_title(\"Test Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "          #print(f\"iter: {i} | baseline test acc: {baseline_test_acc: 0.2f} | candidate test acc: {candidate_test_acc: 0.2f}\")\n",
        "          i += 1\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "unaL0Z6SeHw4",
        "outputId": "978e2bec-ee8c-4c44-c9a6-3ad370eaa02a"
      },
      "outputs": [],
      "source": [
        "from matplotlib import colormaps as cm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "layers = [f\"conv_{i}\" for i in range(10)]\n",
        "\n",
        "fig, axs = plt.subplots(10, 1, figsize=(5,24), constrained_layout=True)\n",
        "\n",
        "\n",
        "def update(frame):\n",
        "    baseline_activations = baseline_activations_logger[frame]\n",
        "    candidate_activations = candidate_activations_logger[frame]\n",
        "    for layer_idx, layer in enumerate(layers):\n",
        "        axs[layer_idx].cla()\n",
        "        layer_baseline_activations = baseline_activations[layer].flatten()\n",
        "        axs[layer_idx].hist(layer_baseline_activations, color=cm[\"Blues\"](50), bins=60, alpha=1.0)\n",
        "        layer_candidate_activations = candidate_activations[layer].flatten()\n",
        "        axs[layer_idx].hist(layer_candidate_activations, color=cm[\"Reds\"](90),  bins=60, alpha=0.5)\n",
        "        axs[layer_idx].set_title(f\"{layer} Outputs - Iteration:{frame}\")\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.margins(x=0, y=0)\n",
        "        ax.set_xlim(-8, 8)\n",
        "        ax.legend([\"Baseline\", \"Candidate\"])\n",
        "\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(baseline_activations_logger), interval=300, repeat=True)\n",
        "plt.close(fig)\n",
        "video_html = ani.to_html5_video().replace('<video', '<video muted')\n",
        "HTML(video_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeXE_lRPeHw4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
