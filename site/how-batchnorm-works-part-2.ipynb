{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How and Why BatchNorm Works - Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, we showed how using batch normalization can help train neural networks by reducing internal covariate shift and smoothing gradient updates. We also touch upon the idea that it makes the loss surface smoother and more convex (residual connections have a similar effect). \n",
    "\n",
    "In their 2018 paper, Santukar et al. argue that internal covariate shift isn't substantially reduced at all. In fact, when additional noise is added to the activations to force covariate shift, batch normalization continues to improve training performance. This is despite no significant reduction in covariate shift. They argue that the major reason for improved performance is the smoother loss surface.\n",
    "\n",
    "However, we saw in our own experiments using a shallow neural network that the reduction in covariate shift is substantial. Let's get to the bottom of this.\n",
    "\n",
    "In this experiment, we will start by reproducing the results from Santukar et al. Then we will try and understand why their results and our previous experiment differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start as usual by setting up the dataset\n",
    "We will use the CIFAR-10 dataset for this experiment.\n",
    "The CIFAR-10 dataset is a widely used benchmark in machine learning for evaluating image classification algorithms. It consists of 60,000 color images, each sized 32x32 pixels, divided into 10 distinct classes such as airplanes, automobiles, birds, cats, and ships. The dataset is split into 50,000 training images and 10,000 test images, with an equal number of examples per class. Created by the Canadian Institute for Advanced Research, CIFAR-10 is challenging due to its low resolution and high intra-class variability. It serves as a foundational dataset for developing and comparing deep learning models, especially convolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepkit import load_CIFAR10\n",
    "train_loader, test_loader = load_CIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    batch = images\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, let's set up the model\n",
    "We'll be using the standard VGG-Net architecture as per the paper.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "\n",
    "class VGGBlock(nnx.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
    "        self.conv = nnx.Conv(in_features=in_features, \n",
    "                             out_features=out_features,\n",
    "                             kernel_size=(3, 3), \n",
    "                             padding='SAME', \n",
    "                             rngs=rngs)\n",
    "        self.bn = nnx.BatchNorm(num_features=out_features, use_running_average=False, \n",
    "                                rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, training: bool):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x, use_running_average=not training)\n",
    "        x = nnx.relu(x)\n",
    "        return x\n",
    "\n",
    "class VGGNet(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.convs = [\n",
    "            VGGBlock(in_features=3, out_features=64,  rngs=rngs),\n",
    "            VGGBlock(in_features=64, out_features=128, rngs=rngs),\n",
    "            VGGBlock(in_features=128, out_features=256, rngs=rngs),\n",
    "            VGGBlock(in_features=256, out_features=256, rngs=rngs),\n",
    "            VGGBlock(in_features=256, out_features=512, rngs=rngs),\n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs),\n",
    "        ]\n",
    "\n",
    "        self.fc1 = nnx.Linear(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.fc2 = nnx.Linear(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.out = nnx.Linear(in_features=512, out_features=10,  rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, training: bool = True):\n",
    "        max_pool_after = [0, 1, 3, 5, 7]\n",
    "        for layer in range(len(self.convs)):\n",
    "            x = self.convs[layer](x, training)\n",
    "            if layer in max_pool_after:\n",
    "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        x = nnx.relu(self.fc1(x))\n",
    "        x = nnx.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.key(1337)\n",
    "rngs = nnx.Rngs(rng_key)\n",
    "model = VGGNet(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "param_counts = sum(jax.tree_util.tree_leaves(jax.tree_util.tree_map(lambda x: x.size, state)))\n",
    "print(f\"Initialized model with {param_counts:,} parameters.\")\n",
    "nnx.display(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "tx = optax.sgd(learning_rate=0.01, momentum=0.9)\n",
    "optimizer = nnx.Optimizer(model, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def loss_fn(model, batch, targets):\n",
    "    logits = model(batch)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def step_fn(model: nnx.Module, optimizer: nnx.Optimizer, batch: jax.Array, labels: jax.Array):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, labels)\n",
    "    optimizer.update(grads)\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def accuracy(model: nnx.Module, batch: jax.Array, labels: jax.Array):\n",
    "    logits = model(batch)\n",
    "    #probs = nnx.softmax(logits, axis=-1)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    sum = jnp.sum(preds == labels)\n",
    "    acc = sum/logits.shape[0]\n",
    "    return acc\n",
    "    \n",
    "\n",
    "def test_accuracy(model: nnx.Module, testloader):\n",
    "    acc, n = 0, 0\n",
    "    for batch, labels in testloader: \n",
    "        batch = jnp.array(batch)\n",
    "        labels = jnp.array(labels)\n",
    "        acc += accuracy(model, batch, labels)\n",
    "        n += 1\n",
    "    return acc/n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_steps = num_epochs*len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch, labels in train_loader:\n",
    "        batch = jnp.array(batch)\n",
    "        labels = jnp.array(labels)\n",
    "        loss, grads = step_fn(model, optimizer, batch, labels)\n",
    "        acc = accuracy(model, batch, labels)\n",
    "        print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
