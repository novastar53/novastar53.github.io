<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>How and Why does Self Attention Work? &#8212; Vikram&#39;s Data Science Blog</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="icon" href="_static/starburst (200 x 200 px).png"/>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="how-and-why-does-self-attention-work">
<h1>How and Why does Self Attention Work?<a class="headerlink" href="#how-and-why-does-self-attention-work" title="Link to this heading">¶</a></h1>
<section id="stacking-non-linear-transformations">
<h2>Stacking Non-Linear Transformations<a class="headerlink" href="#stacking-non-linear-transformations" title="Link to this heading">¶</a></h2>
<p>The prevailing paradigm in machine learning is to repeatedly perform the same non-linear transformation, initially on the input data, and then on the outputs successively. Each of these transformations is called a layer. Deep architectures are so called because they have many such layers. For example, the GPT-3 model trained at OpenAI had 96 layers in total.</p>
<p>Many of the key breakthroughs in recent years have focused on resolving some of the problems that crop up while trying to train such architectures. I have covered a few of these in previous posts, such as <a class="reference internal" href="#..."><span class="xref myst">Batch Normalization</span></a> and <a class="reference internal" href="#..."><span class="xref myst">Dropout</span></a>.</p>
<p>The earliest deep-learning architecture was what is now called a Feed-Forward Network, which in its current mature formulation, consists of a linear operation followed by a non-linear activation function <span class="math notranslate nohighlight">\((\sigma)\)</span> such as ReLU (Rectified Linear Unit).</p>
<div class="math notranslate nohighlight">
\[
f(\bold{x}) = \sigma(\bold{Wx} + b)
\]</div>
<img src="nn.svg" alt="Feed Forward Network" style="width:500px;"/><p>Another popular architecture used primarily in computer vision is the Convolutional Neural Network (CNN), whose basic transformation is the convolution followed by an activation function. The idea here is that the model can learn to detect features in an image by performing non-linear transformations on small patches of the image. Then, just like the basic Feed-Forward network, the same operation can be performed on the outputs successively.</p>
<img src="cnn.svg" alt="Feed Forward Network" style="width:800px;"/>
<p>I won’t go into to much detail on these here.</p>
</section>
<section id="a-new-transformation">
<h2>A New Transformation<a class="headerlink" href="#a-new-transformation" title="Link to this heading">¶</a></h2>
<p>The idea of self-attention developed out of the sequence-to-sequence model architectures which were used primarily for machine translation. Here, the goal was to learn a single representation for an input sentence (“encode”), then use it to generate a translation (“decode”).</p>
<p>It however proved difficult to compress the information needed to translate a long sentence into a single representation.</p>
<p>Bahadanau et. al. (2014) introduced the concept of attention – the model could learn to use parts of the input sentence (“attend”) directly while decoding rather than rely solely on the learnt representation. Parikh et al. (2016) realized that the attention operation itself could be used for NLP tasks such as entailment. Lin et al. (2017) introduced the concept of self-attention to perform a variety of NLP tasks.</p>
<p>In my earlier post on <a class="reference internal" href="#..."><span class="xref myst">Nadaraya-Watson Regression</span></a>, we saw how this classic non-parametric technique can be interpreted as an early form of attention.</p>
<p>Vaswani el al. (2017) realized that the self-attention mechanism could be used as the basic non-linear transformation for sequences of variable length. This also allowed the model to process tokens in parallel by incorporating positional information (check out my post on positional embeddings <a class="reference internal" href="#..."><span class="xref myst">here</span></a>). By avoiding having to explicitly process each token in sequence, it became possible to train much deeper networks.</p>
</section>
<section id="so-what-exactly-is-self-attention">
<h2>So What Exactly is Self Attention?<a class="headerlink" href="#so-what-exactly-is-self-attention" title="Link to this heading">¶</a></h2>
</section>
<section id="let-s-see-if-it-works">
<h2>Let’s See If it Works<a class="headerlink" href="#let-s-see-if-it-works" title="Link to this heading">¶</a></h2>
<p>Let’s train a sentiment classifier. For our dataset, we will be using Yelp review dataset with GloVE embeddings. Our baseline model is a simple feedforward network that uses an average of the word embeddings. The candidate model will use an additional self-attention layer.</p>
<section id="let-s-start-by-loading-the-dataset">
<h3>Let’s start by loading the dataset<a class="headerlink" href="#let-s-start-by-loading-the-dataset" title="Link to this heading">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">flax.linen</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">flax.training</span> <span class="kn">import</span> <span class="n">train_state</span>

<span class="c1"># Load IMDb dataset from TensorFlow Datasets</span>
<span class="k">def</span> <span class="nf">load_imdb_data</span><span class="p">(</span><span class="n">as_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load the IMDB dataset and optionally convert to NumPy.&quot;&quot;&quot;</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;imdb_reviews&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">if</span> <span class="n">as_numpy</span><span class="p">:</span>
        <span class="n">train_data</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
        <span class="n">test_data</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded IMDb dataset with </span><span class="si">{</span><span class="n">info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span><span class="si">}</span><span class="s2"> train samples and &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">info</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">num_examples</span><span class="si">}</span><span class="s2"> test samples.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span>

<span class="c1"># Preprocessing function: Tokenize and pad sequences</span>
<span class="k">def</span> <span class="nf">preprocess_text_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tokenizes text data and pads sequences to a fixed length.&quot;&quot;&quot;</span>
    <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">encoded</span><span class="p">[:</span><span class="n">max_len</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)),</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>
        <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">texts</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Dummy tokenizer: Replace with your own tokenizer</span>
<span class="k">def</span> <span class="nf">dummy_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple tokenizer that converts characters to integers (for demo).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="o">%</span> <span class="mi">255</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>  <span class="c1"># Modulo to keep values in range</span>

<span class="c1"># Example usage</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">load_imdb_data</span><span class="p">()</span>

<span class="c1"># Tokenize and preprocess the data</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">preprocess_text_data</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">dummy_tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">preprocess_text_data</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">dummy_tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training data shape: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Labels shape: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test data shape: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Labels shape: </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loaded IMDb dataset with 25000 train samples and 25000 test samples.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-01-29 01:44:21.957458: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">44</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span> <span class="c1"># Tokenize and preprocess the data</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="n">max_len</span> <span class="o">=</span> <span class="mi">256</span>
<span class="ne">---&gt; </span><span class="mi">44</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">preprocess_text_data</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">dummy_tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">45</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">preprocess_text_data</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">dummy_tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training data shape: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Labels shape: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nn">Cell In[1], line 28,</span> in <span class="ni">preprocess_text_data</span><span class="nt">(data, tokenizer, max_len)</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span> <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span>     <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
<span class="ne">---&gt; </span><span class="mi">28</span>     <span class="n">padded</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">encoded</span><span class="p">[:</span><span class="n">max_len</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)),</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">29</span>     <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">30</span>     <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:4007,</span> in <span class="ni">pad</span><span class="nt">(array, pad_width, mode, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">3890</span> <span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">array</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">:</span> <span class="n">PadValueLike</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="n">Array</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span class="g g-Whitespace">   </span><span class="mi">3891</span>         <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">3892</span><span class="w">   </span><span class="sd">&quot;&quot;&quot;Add padding to an array.</span>
<span class="g g-Whitespace">   </span><span class="mi">3893</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">3894</span><span class="sd">   JAX implementation of :func:`numpy.pad`.</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">   </span><span class="mi">4004</span><span class="sd">     Array([-10, -10,   2,   3,   4,  10,  10], dtype=int32)</span>
<span class="g g-Whitespace">   </span><span class="mi">4005</span><span class="sd">   &quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">4007</span>   <span class="n">util</span><span class="o">.</span><span class="n">check_arraylike</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">array</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">4008</span>   <span class="n">pad_width</span> <span class="o">=</span> <span class="n">_broadcast_to_pairs</span><span class="p">(</span><span class="n">pad_width</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">array</span><span class="p">),</span> <span class="s2">&quot;pad_width&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">4009</span>   <span class="k">if</span> <span class="n">pad_width</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">is_dim</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="n">core</span><span class="o">.</span><span class="n">is_dim</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="g g-Whitespace">   </span><span class="mi">4010</span>                            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pad_width</span><span class="p">):</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/util.py:311,</span> in <span class="ni">check_arraylike</span><span class="nt">(fun_name, emit_warning, stacklevel, *args)</span>
<span class="g g-Whitespace">    </span><span class="mi">308</span>   <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span> <span class="o">+</span> <span class="s2">&quot; In a future JAX release this will be an error.&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">309</span>                 <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="n">stacklevel</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">310</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">311</span>   <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fun_name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">),</span> <span class="n">pos</span><span class="p">))</span>

<span class="ne">TypeError</span>: pad requires ndarray or scalar arguments, got &lt;class &#39;list&#39;&gt; at position 0.
</pre></div>
</div>
</div>
</div>
</section>
<section id="now-let-s-load-the-glove-embeddings">
<h3>Now, let’s load the GloVE Embeddings<a class="headerlink" href="#now-let-s-load-the-glove-embeddings" title="Link to this heading">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">load_glove_embeddings</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
            <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
    <span class="k">return</span> <span class="n">embeddings</span>

<span class="c1"># Load the 100-dimensional GloVe embeddings</span>
<span class="n">glove_embeddings</span> <span class="o">=</span> <span class="n">load_glove_embeddings</span><span class="p">(</span><span class="s1">&#39;datasets/glove.6B.100d.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">14</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>     <span class="k">return</span> <span class="n">embeddings</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="c1"># Load the 100-dimensional GloVe embeddings</span>
<span class="ne">---&gt; </span><span class="mi">14</span> <span class="n">glove_embeddings</span> <span class="o">=</span> <span class="n">load_glove_embeddings</span><span class="p">(</span><span class="s1">&#39;datasets/glove.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="nn">Cell In[2], line 5,</span> in <span class="ni">load_glove_embeddings</span><span class="nt">(filepath)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="k">def</span> <span class="nf">load_glove_embeddings</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="n">embeddings</span> <span class="o">=</span> <span class="p">{}</span>
<span class="ne">----&gt; </span><span class="mi">5</span>     <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>         <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>             <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324,</span> in <span class="ni">_modified_open</span><span class="nt">(file, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span> <span class="k">if</span> <span class="n">file</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">}:</span>
<span class="g g-Whitespace">    </span><span class="mi">318</span>     <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">319</span>         <span class="sa">f</span><span class="s2">&quot;IPython won&#39;t let you open fd=</span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s2"> by default &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span>         <span class="s2">&quot;as it is likely to crash IPython. If you know what you are doing, &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">321</span>         <span class="s2">&quot;you can use builtins&#39; open.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span>     <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">324</span> <span class="k">return</span> <span class="n">io_open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;datasets/glove.6B.100d.txt&#39;
</pre></div>
</div>
</div>
</div>
<p>Let’s build the embedding matrix</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_embedding_matrix</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">glove_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">embedding_dim</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">glove_embeddings</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">glove_embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">embedding_matrix</span>

<span class="c1"># Example vocabulary</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;the&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ran&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;fast&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;&lt;PAD&gt;&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;&lt;UNK&gt;&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>

<span class="c1"># Create the embedding matrix</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">build_embedding_matrix</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">glove_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">13</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="c1"># Create the embedding matrix</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="ne">---&gt; </span><span class="mi">13</span> <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">build_embedding_matrix</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">glove_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;glove_embeddings&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>Let’s define the baseline model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">flax.core.frozen_dict</span> <span class="kn">import</span> <span class="n">freeze</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embed</span><span class="p">(</span>
            <span class="n">num_embeddings</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">embedding_init</span><span class="o">=</span><span class="k">lambda</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embedded</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>

<span class="c1"># Example input batch (batch_size=2, sequence_length=3)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="c1"># Initialize and apply the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>

<span class="c1"># Since Flax models are functional, we need to initialize parameters</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">input_data</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">freeze</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="n">input_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># Output shape: (2, 3, 100)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">10</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="c1"># Since Flax models are functional, we need to initialize parameters</span>
<span class="ne">---&gt; </span><span class="mi">10</span> <span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">input_data</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">freeze</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="n">input_data</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># Output shape: (2, 3, 100)</span>

    <span class="p">[</span><span class="o">...</span> <span class="n">skipping</span> <span class="n">hidden</span> <span class="mi">9</span> <span class="n">frame</span><span class="p">]</span>

<span class="nn">Cell In[4], line 16,</span> in <span class="ni">MyModel.__call__</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>     <span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embed</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span>         <span class="n">num_embeddings</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span>         <span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>         <span class="n">embedding_init</span><span class="o">=</span><span class="k">lambda</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span>     <span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">16</span>     <span class="n">embedded</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span>     <span class="k">return</span> <span class="n">embedded</span>

    <span class="p">[</span><span class="o">...</span> <span class="n">skipping</span> <span class="n">hidden</span> <span class="mi">5</span> <span class="n">frame</span><span class="p">]</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/flax/linen/linear.py:1101,</span> in <span class="ni">Embed.setup</span><span class="nt">(self)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span> <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1101</span>   <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1102</span>     <span class="s1">&#39;embedding&#39;</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>     <span class="bp">self</span><span class="o">.</span><span class="n">embedding_init</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>     <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1105</span>     <span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1106</span>   <span class="p">)</span>

    <span class="p">[</span><span class="o">...</span> <span class="n">skipping</span> <span class="n">hidden</span> <span class="mi">1</span> <span class="n">frame</span><span class="p">]</span>

<span class="nn">File ~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/flax/core/scope.py:997,</span> in <span class="ni">Scope.param</span><span class="nt">(self, name, init_fn, unbox, *init_args, **init_kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">995</span>     <span class="k">raise</span> <span class="n">errors</span><span class="o">.</span><span class="n">ScopeCollectionNotFound</span><span class="p">(</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_text</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">996</span>   <span class="k">raise</span> <span class="n">errors</span><span class="o">.</span><span class="n">ScopeParamNotFoundError</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_text</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">997</span> <span class="n">value</span> <span class="o">=</span> <span class="n">init_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">make_rng</span><span class="p">(</span><span class="s1">&#39;params&#39;</span><span class="p">),</span> <span class="o">*</span><span class="n">init_args</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">998</span> <span class="bp">self</span><span class="o">.</span><span class="n">put_variable</span><span class="p">(</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">999</span> <span class="k">if</span> <span class="n">unbox</span><span class="p">:</span>

<span class="ne">TypeError</span>: MyModel.__call__.&lt;locals&gt;.&lt;lambda&gt;() takes 2 positional arguments but 3 were given
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">glove_embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="nb">list</span><span class="p">(</span><span class="n">glove_embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">1</span><span class="p">]</span>

<span class="ne">NameError</span>: name &#39;glove_embeddings&#39; is not defined
</pre></div>
</div>
</div>
</div>
</section>
<section id="references">
<h3>References:<a class="headerlink" href="#references" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Bahdanau et al. 2015</p></li>
<li><p>Rocktashel et al. 2016 Reasoning about Entailment with Neural Attention ()</p></li>
<li><p>Parikh, A Decomposable Attention Model for Natural Language Inference</p></li>
<li><p>GloVe, Pennington et al. (2014)</p></li>
<li><p>Lin et al. A Structured Self-Attentive Sentence Embedding</p></li>
<li><p>GloVe Embeddings</p></li>
</ol>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="intro.html">
              <img class="logo" src="_static/starburst-transparent.png" alt="Logo of Starburst Data Science Blog"/>
            </a></p>
<h1 class="logo"><a href="intro.html">Starburst Data Science Blog</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="how-residual-connections-work.html">How Do Residual Connections Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-positional-embeddings-work.html">How do Positional Embeddings Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-dropout-works.html">How and Why Does Dropout Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how-batchnorm-works.html">How and Why Does Batch Normalization Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="nadaraya-watson-kernel-regression.html">Nadaraya-Watson Regression</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="intro.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024 Vikram Pawar novastar53.github.io.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/how-self-attention-works.ipynb"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>