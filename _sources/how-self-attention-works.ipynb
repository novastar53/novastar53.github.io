{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How and Why does Self Attention Work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Non-Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prevailing paradigm in machine learning is to repeatedly perform the same non-linear transformation, initially on the input data, and then on the outputs successively. Each of these transformations is called a layer. Deep architectures are so called because they have many such layers. For example, the GPT-3 model trained at OpenAI had 96 layers in total. \n",
    "\n",
    "Many of the key breakthroughs in recent years have focused on resolving some of the problems that crop up while trying to train such architectures. I have covered a few of these in previous posts, such as [Batch Normalization](...) and [Dropout](...).\n",
    "\n",
    "The earliest deep-learning architecture was what is now called a Feed-Forward Network, which in its current mature formulation, consists of a linear operation followed by a non-linear activation function $(\\sigma)$ such as ReLU (Rectified Linear Unit).\n",
    "\n",
    "$$\n",
    "f(\\bold{x}) = \\sigma(\\bold{Wx} + b)\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"nn.svg\" alt=\"Feed Forward Network\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular architecture used primarily in computer vision is the Convolutional Neural Network (CNN), whose basic transformation is the convolution followed by an activation function. The idea here is that the model can learn to detect features in an image by performing non-linear transformations on small patches of the image. Then, just like the basic Feed-Forward network, the same operation can be performed on the outputs successively.\n",
    "\n",
    "\n",
    "<img src=\"cnn.svg\" alt=\"Feed Forward Network\" style=\"width:800px;\"/>\n",
    "\n",
    "I won't go into to much detail on these here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A New Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of self-attention developed out of the sequence-to-sequence model architectures which were used primarily for machine translation. Here, the goal was to learn a single representation for an input sentence (\"encode\"), then use it to generate a translation (\"decode\").\n",
    "\n",
    "It however proved difficult to compress the information needed to translate a long sentence into a single representation. \n",
    "\n",
    "Bahadanau et. al. (2014) introduced the concept of attention -- the model could learn to use parts of the input sentence (\"attend\") directly while decoding rather than rely solely on the learnt representation. Parikh et al. (2016) realized that the attention operation itself could be used for NLP tasks such as entailment. Lin et al. (2017) introduced the concept of self-attention to perform a variety of NLP tasks. \n",
    "\n",
    "In my earlier post on [Nadaraya-Watson Regression](...), we saw how this classic non-parametric technique can be interpreted as an early form of attention. \n",
    "\n",
    "Vaswani el al. (2017) realized that the self-attention mechanism could be used as the basic non-linear transformation for sequences of variable length. This also allowed the model to process tokens in parallel by incorporating positional information (check out my post on positional embeddings [here](...)). By avoiding having to explicitly process each token in sequence, it became possible to train much deeper networks. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What Exactly is Self Attention?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's See If it Works\n",
    "\n",
    "Let's train a sentiment classifier. For our dataset, we will be using Yelp review dataset with GloVE embeddings. Our baseline model is a simple feedforward network that uses an average of the word embeddings. The candidate model will use an additional self-attention layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:Descriptors cannot be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/__init__.py\", line 79, in <module>\n",
      "    from tensorflow_datasets import rlds  # pylint: disable=g-bad-import-order\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/rlds/__init__.py\", line 21, in <module>\n",
      "    from tensorflow_datasets.rlds import envlogger_reader\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/rlds/envlogger_reader.py\", line 21, in <module>\n",
      "    from tensorflow_datasets.core.utils.lazy_imports_utils import tree\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/__init__.py\", line 22, in <module>\n",
      "    from tensorflow_datasets.core import community\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/community/__init__.py\", line 19, in <module>\n",
      "    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/community/huggingface_wrapper.py\", line 31, in <module>\n",
      "    from tensorflow_datasets.core import dataset_builder\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 35, in <module>\n",
      "    from tensorflow_datasets.core import dataset_info\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_info.py\", line 52, in <module>\n",
      "    from tensorflow_datasets.core import splits as splits_lib\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/splits.py\", line 34, in <module>\n",
      "    from tensorflow_datasets.core import proto as proto_lib\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/proto/__init__.py\", line 18, in <module>\n",
      "    from tensorflow_datasets.core.proto import dataset_info_generated_pb2 as dataset_info_pb2  # pylint: disable=line-too-long\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_datasets/core/proto/dataset_info_generated_pb2.py\", line 32, in <module>\n",
      "    from tensorflow_metadata.proto.v0 import schema_pb2 as tensorflow__metadata_dot_proto_dot_v0_dot_schema__pb2\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/tensorflow_metadata/proto/v0/schema_pb2.py\", line 32, in <module>\n",
      "    _descriptor.EnumValueDescriptor(\n",
      "  File \"/Users/vikram/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/google/protobuf/descriptor.py\", line 789, in __new__\n",
      "    _message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_datasets' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mord\u001b[39m(char) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text]  \u001b[38;5;66;03m# Modulo to keep values in range\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_imdb_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Tokenize and preprocess the data\u001b[39;00m\n\u001b[1;32m     43\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mload_imdb_data\u001b[0;34m(as_numpy)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_imdb_data\u001b[39m(as_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the IMDB dataset and optionally convert to NumPy.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     data, info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimdb_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m as_numpy:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_datasets' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "# Load IMDb dataset from TensorFlow Datasets\n",
    "def load_imdb_data(as_numpy=True):\n",
    "    \"\"\"Load the IMDB dataset and optionally convert to NumPy.\"\"\"\n",
    "    data, info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "    train_data, test_data = data\n",
    "\n",
    "    if as_numpy:\n",
    "        train_data = tfds.as_numpy(train_data)\n",
    "        test_data = tfds.as_numpy(test_data)\n",
    "\n",
    "    print(f\"Loaded IMDb dataset with {info.splits['train'].num_examples} train samples and \"\n",
    "          f\"{info.splits['test'].num_examples} test samples.\")\n",
    "    return train_data, test_data\n",
    "\n",
    "# Preprocessing function: Tokenize and pad sequences\n",
    "def preprocess_text_data(data, tokenizer, max_len=256):\n",
    "    \"\"\"Tokenizes text data and pads sequences to a fixed length.\"\"\"\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for text, label in data:\n",
    "        encoded = tokenizer(text.decode('utf-8'))\n",
    "        padded = jnp.pad(encoded[:max_len], (0, max_len - len(encoded)), constant_values=0)\n",
    "        texts.append(padded)\n",
    "        labels.append(label)\n",
    "\n",
    "    return jnp.array(texts), jnp.array(labels)\n",
    "\n",
    "# Dummy tokenizer: Replace with your own tokenizer\n",
    "def dummy_tokenizer(text):\n",
    "    \"\"\"A simple tokenizer that converts characters to integers (for demo).\"\"\"\n",
    "    return [ord(char) % 255 for char in text]  # Modulo to keep values in range\n",
    "\n",
    "# Example usage\n",
    "train_data, test_data = load_imdb_data()\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "max_len = 256\n",
    "X_train, y_train = preprocess_text_data(train_data, dummy_tokenizer, max_len)\n",
    "X_test, y_test = preprocess_text_data(test_data, dummy_tokenizer, max_len)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load the GloVE Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load the 100-dimensional GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings('datasets/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, glove_embeddings, embedding_dim=100):\n",
    "    embedding_matrix = np.random.normal(size=(len(vocab), embedding_dim)).astype('float32')\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Example vocabulary\n",
    "vocab = {'the': 0, 'dog': 1, 'ran': 2, 'fast': 3, '<PAD>': 4, '<UNK>': 5}\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = build_embedding_matrix(vocab, glove_embeddings, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "from flax.core.frozen_dict import freeze\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    embedding_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        embedding_layer = nn.Embed(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            features=self.embedding_dim,\n",
    "            embedding_init=lambda shape, dtype: jnp.array(embedding_matrix)\n",
    "        )\n",
    "        embedded = embedding_layer(x)\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyModel.__call__.<locals>.<lambda>() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Since Flax models are functional, we need to initialize parameters\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(freeze(params), input_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)  \u001b[38;5;66;03m# Output shape: (2, 3, 100)\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     embedding_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbed(\n\u001b[1;32m     12\u001b[0m         num_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m     13\u001b[0m         features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim,\n\u001b[1;32m     14\u001b[0m         embedding_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m shape, dtype: jnp\u001b[38;5;241m.\u001b[39marray(embedding_matrix)\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/flax/linen/linear.py:1101\u001b[0m, in \u001b[0;36mEmbed.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/flax/core/scope.py:997\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    996\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[0;32m--> 997\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, value)\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unbox:\n",
      "\u001b[0;31mTypeError\u001b[0m: MyModel.__call__.<locals>.<lambda>() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# Example input batch (batch_size=2, sequence_length=3)\n",
    "input_data = jnp.array([[0, 1, 2], [1, 3, 4]])\n",
    "\n",
    "# Initialize and apply the model\n",
    "model = MyModel(vocab_size=len(vocab), embedding_dim=embedding_dim)\n",
    "\n",
    "# Since Flax models are functional, we need to initialize parameters\n",
    "params = model.init(jax.random.PRNGKey(42), input_data)\n",
    "output = model.apply(freeze(params), input_data)\n",
    "\n",
    "print(output)  # Output shape: (2, 3, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glove_embeddings.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. Bahdanau et al. 2015\n",
    "2. Rocktashel et al. 2016 Reasoning about Entailment with Neural Attention ()\n",
    "3. Parikh, A Decomposable Attention Model for Natural Language Inference\n",
    "4. GloVe, Pennington et al. (2014)\n",
    "5. Lin et al. A Structured Self-Attentive Sentence Embedding\n",
    "6. GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
