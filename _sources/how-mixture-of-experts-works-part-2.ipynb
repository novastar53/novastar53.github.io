{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf7e90a5",
      "metadata": {},
      "source": [
        "# How do Mixture of Experts Layers Work? Part 2\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In [Part 1](https://blog.vikrampawar.com/how-mixture-of-experts-works-part-1.html), we introduced the Mixtures of Experts layer and attempted a naive implementation, where we trained a simple neural network with an expert router and two experts. The model learned to route each datapoint to one of two regression models. We used a synthetic dataset tailored for our model. \n",
        "\n",
        "In this post, we'll build upon that basic design and explore how to scale MoE layers across multiple GPUs.\n",
        "\n",
        "## Why Mixture of Experts?\n",
        "\n",
        "The main reason that Mixture of Experts is used is because it can scale model parameters with sublinear compute scaling. In a dense model, every parameter is used for every input. Double the parameters, double the FLOPs. MoE breaks this relationship by only activating a subset of parameters for each input. In modern MoE models, each token is routed to K of N experts (typically K = 1 or 2). This means per-token FLOPs in the expert layers are reduced to K/N of what a dense network would require, without a large performance penalty.\n",
        "\n",
        "As a result, we can train much larger models while keeping the compute budget under control. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fmaro4xlnvj",
      "metadata": {},
      "source": [
        "## The Parallelism Challenge\n",
        "\n",
        "The parallelism challenge in MoE is about what happens when we try to distribute experts across multiple GPUs. In our naive implementation, we ran every token through every expert and then weighted the outputs. This is actually dense computation masquerading as MoE.\n",
        "\n",
        "To get actual sparse computation, we need to only send each token to its assigned K experts. But when experts live on different GPUs, this creates a coordination problem. Token A on GPU 0 might need Expert 2 on GPU 2. Token B on GPU 1 might need Expert 0 on GPU 0. Every GPU potentially needs to send tokens to every other GPU, creating a many-to-many communication pattern.\n",
        "\n",
        "![token-routing](token-routing.png)\n",
        "\n",
        "This introduces several challenges. First, all-to-all communication is expensive. Second, if the router sends many tokens to one expert, that GPU becomes a bottleneck while others sit idle. Third, each GPU needs memory to buffer incoming and outgoing tokens during the shuffle. Finally, we need two shuffles, one to dispatch tokens to experts, and another to combine results back to their original positions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffflamyc6rj",
      "metadata": {},
      "source": [
        "## Expert Parallelism Explained\n",
        "\n",
        "Expert parallelism solves the routing problem by distributing experts across GPUs and using all-to-all communication to shuffle tokens to their assigned experts. The sharding strategy has three components: expert weights are sharded on the expert dimension so that GPU i holds expert i, non-expert layers (attention, embeddings, normalization) are replicated across all GPUs, and data batches are sharded on the batch dimension so that GPU i processes batch slice i. This combines data parallelism with expert parallelism.\n",
        "\n",
        "**MoE Sharding Strategy**\n",
        "\n",
        "![moe-sharding-strategy](moe-sharding-strategy-v2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5gyvwvomjhd",
      "metadata": {},
      "source": [
        "## Token Routing - Dispatch and Combine\n",
        "\n",
        "The token flow works in four phases: \n",
        "First, each GPU gathers its tokens into expert-specific buffers. After this local gathering step, each GPU has organized its tokens which expert they are assigned. Second, during the all-to-all dispatch, the expert buffers are redistributed across GPUs so that each expert's tokens are collected on their respective GPU. GPU 0 receives all tokens destined for expert 0, GPU 1 receives all tokens for expert 1, and so on. Third, each GPU runs its local expert on the tokens it received. Fourth, during the all-to-all combine, the processed tokens are routed back to their original GPUs and reassembled in the correct sequence positions.\n",
        "\n",
        "![token-dispatch-combine](token-dispatch-combine.png)\n",
        "\n",
        "In JAX, the all-to-all communication emerges implicitly from sharding constraints. We specify how tensors should be partitioned (e.g., tokens sharded by batch, expert weights sharded by expert index), and XLA's compiler inserts the necessary collectives when an operation requires data that lives on another device. For MoE, this means the dispatch and combine shuffles happen automatically when the sharding layout changes between \"tokens grouped by batch\" and \"tokens grouped by expert.\"\n",
        "\n",
        "The dispatch and combine phases come with real costs. MoE trades compute savings (activating only K of N experts) for communication overhead. Each token must be sent to its assigned expert and the result returned, roughly `2 × num_tokens × hidden_dim` bytes moved per MoE layer. All-to-all is also a synchronization barrier: every GPU must wait for the slowest one to finish sending and receiving, so load imbalance amplifies latency. Finally, each GPU needs buffer memory to stage outgoing tokens (grouped by destination) and incoming tokens (from all other GPUs), increasing peak memory usage beyond what the expert weights alone require."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z00m5pj40qj",
      "metadata": {},
      "source": [
        "## Capacity Factor, Token Dropping and Load Balancing\n",
        "\n",
        "The router doesn't distribute tokens evenly; one expert might receive 80% of tokens while another gets 5%. To handle this, MoE implementations define an expert capacity: `capacity = (batch_tokens / num_experts) × capacity_factor`, where capacity factor (typically 1.0 to 2.0) controls how much slack each expert has. When more tokens are routed to an expert than its capacity allows, the excess tokens are dropped. \n",
        "\n",
        "The capacity factor controls a tradeoff: higher values mean fewer dropped tokens but more buffer memory and wasted compute on padding; lower values keep memory tight but risk dropping tokens. To minimize dropping without inflating capacity, MoE models use auxiliary load balancing losses that penalize uneven routing, pushing the router toward balanced assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbtydyy8u4",
      "metadata": {},
      "source": [
        "## Putting It Together: A Sparse MoE Layer\n",
        "\n",
        "The implementation below demonstrates sparse token routing without sharding. The `Experts` class holds all expert weights in a single tensor of shape `(n_experts, n_embed, n_embed)`, indexing into the appropriate slice when called. The `MOE` class implements the full dispatch-compute-combine pattern: a router produces per-token logits, `jax.lax.top_k` selects the top-k experts for each token, and a masked softmax computes the expert weights. Each expert then processes only its gathered tokens. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section_setup_6",
      "metadata": {},
      "source": [
        "### Setup and Imports\n",
        "\n",
        "First, we set up the environment and import all necessary libraries. Since we're training on a CPU, we can emulate multiple devices by setting some environment variables for the XLA compiler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "section_setup_7",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
        "\n",
        "import flax.nnx as nnx\n",
        "import optax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section_mesh_8",
      "metadata": {},
      "source": [
        "### Device Mesh and Sharding Configuration\n",
        "\n",
        "Here we create a device mesh and sharding specifications that will be used throughout the MoE implementation. Device mesh is a logical arrangement of devices along at least one axis. We'll use a single axis and shard both our data and our experts along this axis. The other model weights will be replicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "section_mesh_9",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Devices: [CpuDevice(id=0), CpuDevice(id=1)]\n",
            "Mesh('devices': 2, axis_types=(Auto,))\n"
          ]
        }
      ],
      "source": [
        "# Set up device mesh - all devices along a single \"devices\" axis\n",
        "print(f\"Devices: {jax.devices()}\")\n",
        "mesh = Mesh(jax.devices(), [\"devices\"])\n",
        "print(mesh)\n",
        "num_devices = len(jax.devices())\n",
        "\n",
        "# Sharding spec for expert-parallel tensors\n",
        "expert_spec = PartitionSpec(\"devices\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section_config_10",
      "metadata": {},
      "source": [
        "### Model Configuration\n",
        "\n",
        "The configuration defines all hyperparameters for our Mixture of Experts toy model. Key parameters:\n",
        "\n",
        "- **n_experts**: Number of experts\n",
        "- **top_k**: Number of experts each token can be routed to \n",
        "- **load_factor**: Controls buffer size for expert capacity \n",
        "- **load_balancing_loss_coeff**: Coefficient for the load-balancing loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "section_config_11",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "@dataclass(unsafe_hash=True)\n",
        "class Config():\n",
        "    name: str = \"MoE\"\n",
        "    dtype: jnp.dtype = jnp.float32\n",
        "    param_dtype: jnp.dtype = jnp.float32\n",
        "    top_k = 2\n",
        "    load_factor = 1.10\n",
        "    load_balancing_loss_coeff = 0.01\n",
        "    n_experts = 2\n",
        "    n_embed = 3\n",
        "    n_mlp_hidden = 6\n",
        "    mlp_bias = True\n",
        "    dtype = jax.numpy.float32\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section_experts_12",
      "metadata": {},
      "source": [
        "### Sharded Experts \n",
        "\n",
        "The `with_sharding_constraint` calls ensure tensors are properly distributed during forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "section_experts_13",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "class Experts(nnx.Module):\n",
        "    def __init__(self, config, rngs):\n",
        "        # Use sharding annotations for expert weights\n",
        "        init = nnx.with_partitioning(\n",
        "            nnx.initializers.normal(stddev=0.02),\n",
        "            sharding=expert_spec\n",
        "        )\n",
        "        self.w1 = nnx.Param(init(rngs.default(), (config.n_experts, config.n_embed, config.n_embed)))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: (n_experts, tokens_per_expert, n_embed)\n",
        "        # Apply sharding constraint to input\n",
        "        x = jax.lax.with_sharding_constraint(x, expert_spec)\n",
        "        # Each expert processes its slice: einsum over expert dimension\n",
        "        y = jnp.einsum('eti,eio->eto', x, self.w1.value)\n",
        "        y = jax.lax.with_sharding_constraint(y, expert_spec)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section_moe_14",
      "metadata": {},
      "source": [
        "### MoE Layer with Router and Load Balancing Loss\n",
        "\n",
        "The `MOE` class combines routing and expert computation. First, a linear layer (the router) predicts which experts should process each token. This is executed for every sharded batch independently. Then, the top-k experts are picked for each token along with the corresponding expert weights. The dispatch operation then performs and all-to-all and gathers each expert's tokens. Once each expert processes its respective set of tokens, they are scattered back to their original positions. \n",
        "\n",
        "The load balancing loss is computed as the coefficient of variation between two distributions:\n",
        "\n",
        "1. frac_tokens: The actual fraction of tokens sent to each expert \n",
        "2. frac_router_probs: The average router probabilities assigned to each expert\n",
        "\n",
        "The loss encourages these two distributions to match:\n",
        "```python\n",
        "load_balance_loss = sum(frac_tokens * frac_router_probs) * n_experts\n",
        "```\n",
        "This prevents any single expert from becoming a bottleneck and ensures all experts receive meaningful training signals. The coefficient `load_balance_loss_coeff` controls how strongly we penalize imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "section_moe_15",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "class MOE(nnx.Module):\n",
        "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
        "        # Router is replicated\n",
        "        self.router_gate = nnx.Linear(\n",
        "            config.n_embed,\n",
        "            config.n_experts,\n",
        "            kernel_init=nnx.initializers.normal(stddev=0.02),\n",
        "            bias_init=nnx.initializers.zeros,\n",
        "            use_bias=config.mlp_bias,\n",
        "            dtype=config.dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.experts = Experts(config, rngs)        \n",
        "        self.top_k = config.top_k\n",
        "        self.n_experts = config.n_experts\n",
        "        self.load_factor = config.load_factor\n",
        "        self.add_noise = False\n",
        "        self.load_balance_loss = False\n",
        "        self.rngs = rngs\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        x_flat = x.reshape(-1, C)\n",
        "        \n",
        "        # Router produces expert logits for each token\n",
        "        logits = self.router_gate(x_flat)  # (B*T, n_experts)\n",
        "        \n",
        "        # Select top-k experts per token\n",
        "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k)\n",
        "        \n",
        "        # Create sparse logits for softmax (only top-k positions have real values)\n",
        "        zeros = jnp.full_like(logits, float('-inf'))\n",
        "        sparse_logits = jnp.put_along_axis(\n",
        "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
        "        )\n",
        "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
        "\n",
        "        # Gather tokens into expert buffers\n",
        "        expert_capacity = int(self.load_factor * self.top_k * B * T // self.n_experts)\n",
        "        expert_inputs = jnp.zeros((self.n_experts, expert_capacity, C))\n",
        "        input_counters = jnp.zeros((self.n_experts,), dtype=jnp.int32)\n",
        "\n",
        "        def update_expert_inputs(i, carry):\n",
        "            expert_inputs, counters = carry\n",
        "            for j in range(self.top_k):\n",
        "                expert_idx = expert_indices[i, j]\n",
        "                token_pos = counters[expert_idx]\n",
        "                expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x_flat[i])\n",
        "                counters = counters.at[expert_idx].add(1)\n",
        "            return expert_inputs, counters\n",
        "\n",
        "        expert_inputs, input_counters = jax.lax.fori_loop(\n",
        "            0, B * T, update_expert_inputs, (expert_inputs, input_counters)\n",
        "        )\n",
        "\n",
        "        # Apply sharding constraint to trigger all-to-all dispatch\n",
        "        # This moves tokens from batch-sharded to expert-sharded layout\n",
        "        expert_inputs = jax.lax.with_sharding_constraint(expert_inputs, expert_spec)\n",
        "\n",
        "        # #ach expert processes its tokens\n",
        "        expert_outputs = self.experts(expert_inputs)\n",
        "\n",
        "        # Scatter results back to original positions\n",
        "        output_counters = jnp.zeros((self.n_experts,), dtype=jnp.int32)\n",
        "        y_pred = jnp.zeros_like(x_flat)\n",
        "\n",
        "        def update_expert_outputs(i, carry):\n",
        "            y_pred, output_counters = carry\n",
        "            for j in range(self.top_k):\n",
        "                expert_idx = expert_indices[i, j]\n",
        "                token_pos = output_counters[expert_idx]\n",
        "                y_pred = y_pred.at[i].add(\n",
        "                    expert_outputs[expert_idx, token_pos] * expert_weights[i, expert_idx]\n",
        "                )\n",
        "                output_counters = output_counters.at[expert_idx].add(1)\n",
        "            return y_pred, output_counters\n",
        "\n",
        "        y_pred, output_counters = jax.lax.fori_loop(\n",
        "            0, B * T, update_expert_outputs, (y_pred, output_counters)\n",
        "        )\n",
        "\n",
        "        # Apply sharding constraint to trigger all-to-all combine\n",
        "        # This moves results from expert-sharded back to batch-sharded layout\n",
        "        y_pred = jax.lax.with_sharding_constraint(y_pred, expert_spec)\n",
        "\n",
        "        y_pred = y_pred.reshape(B, T, C)\n",
        "        \n",
        "        # Calculate load balancing loss\n",
        "        # Calculate fraction of tokens assigned to each expert\n",
        "        frac_tokens = jnp.bincount(expert_indices.flatten(), length=self.n_experts) / (B * T * self.top_k)\n",
        "        # Calculate fraction of router probabilities assigned to each expert\n",
        "        frac_router_probs = jnp.sum(expert_weights, axis=(0, 1)) / (B * T)\n",
        "        # Load balancing loss encourages equal token distribution across experts\n",
        "        load_balance_loss = jnp.sum(frac_tokens * frac_router_probs) * self.n_experts\n",
        "        return y_pred, load_balance_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_balancing_explanation",
      "metadata": {},
      "source": [
        "\n",
        "### Training Setup\n",
        "\n",
        "Now we set up the trainer. Since this is a regression task, we used an MSE loss with Adam as the optimizer. \n",
        "The synthetic data creates a clear routing signal: positive inputs go to expert 0, negative inputs go to expert 1. This tests whether the MoE learns to route correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "section_training_20",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, x, y):\n",
        "    y_pred, load_balance_loss = model(x)\n",
        "    loss = jnp.mean((y - y_pred)**2)\n",
        "    loss += config.load_balancing_loss_coeff * load_balance_loss  \n",
        "    return loss\n",
        "\n",
        "@nnx.jit\n",
        "def step(state, model, x, y):\n",
        "    total_loss, grads = nnx.value_and_grad(loss_fn)(model, x, y)\n",
        "    state.update(model, grads)\n",
        "    return total_loss\n",
        "\n",
        "D, B, T, C = 1000, config.n_experts, 5, config.n_embed \n",
        "default = jax.random.key(42)\n",
        "rngs = nnx.Rngs(default=default)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section_execution_21",
      "metadata": {},
      "source": [
        "### Model Creation \n",
        "\n",
        "Next, we create the sharded model. The expert weights need to be sharded along the experts axis while the gate weights need to be replicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "88edb3cf",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expert Layer Sharding:\n",
            "PartitionSpec('devices',)\n",
            "NamedSharding(mesh=Mesh('devices': 2, axis_types=(Auto,)), spec=PartitionSpec('devices',), memory_kind=unpinned_host)\n",
            "Router Sharding:\n",
            "PartitionSpec()\n",
            "NamedSharding(mesh=Mesh('devices': 2, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n"
          ]
        }
      ],
      "source": [
        "# Create model within mesh context for proper sharding\n",
        "@nnx.jit(static_argnums=(0, 1))\n",
        "def create_sharded_model(Model, config, rngs):\n",
        "    model = Model(config=config, rngs=rngs)\n",
        "    state = nnx.state(model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = nnx.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(model, sharded_state)\n",
        "    return model\n",
        "\n",
        "with mesh:\n",
        "    model = create_sharded_model(MOE, config, rngs)\n",
        "    model.train()\n",
        "    tx = optax.adam(1e-2)\n",
        "    state = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
        "\n",
        "\n",
        "    print(\"Expert Layer Sharding:\")\n",
        "    print(model.experts.w1.value.sharding.spec)\n",
        "    print(model.experts.w1.value.sharding)\n",
        "\n",
        "    print(\"Router Sharding:\")\n",
        "    print(model.router_gate.kernel.value.sharding.spec)\n",
        "    print(model.router_gate.kernel.value.sharding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb9a845",
      "metadata": {},
      "source": [
        "### Dataset Generation\n",
        "\n",
        "We create a synthetic dataset where each sample has shape `(B, T, C)` representing a batch of sequences with `C`-dimensional embeddings. The target transformation depends on the sign of the first feature dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ffd0ba6a",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "x = jax.random.normal(jax.random.key(1000), (D, B, T, C))\n",
        "expert_ids = (x[:, :, :, 0] > 0).astype(jnp.int32)[..., None]\n",
        "t = [\n",
        "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
        "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
        "]\n",
        "\n",
        "def transform(xi, eid):\n",
        "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
        "\n",
        "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
        "\n",
        "# Training indices\n",
        "indices = list(range(D))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12588ca5",
      "metadata": {},
      "source": [
        "### Training\n",
        "\n",
        "Finally, let's overfit the model on our toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b261761b",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 2.7845\n",
            "Epoch: 1, Loss: 0.0999\n",
            "Epoch: 2, Loss: 0.0780\n",
            "Epoch: 3, Loss: 0.0624\n",
            "Epoch: 4, Loss: 0.0505\n",
            "Epoch: 5, Loss: 0.0419\n",
            "Epoch: 6, Loss: 0.0361\n",
            "Epoch: 7, Loss: 0.0325\n",
            "Epoch: 8, Loss: 0.0303\n",
            "Epoch: 9, Loss: 0.0290\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "with mesh:\n",
        "    # Create data sharding for batch dimension\n",
        "    data_sharding = NamedSharding(mesh, PartitionSpec(\"devices\",))\n",
        "    for e in range(10):\n",
        "        for i in indices:\n",
        "            # Shard data across devices\n",
        "            x_batch = jax.device_put(x[i], data_sharding)\n",
        "            y_batch = jax.device_put(y[i], data_sharding)\n",
        "            loss = step(state, model, x_batch, y_batch)\n",
        "            if i == indices[0]:\n",
        "                print(f\"Epoch: {e}, Loss: {loss:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c02287",
      "metadata": {},
      "source": [
        "The model was able to successfully overfit on our dataset. \n",
        "\n",
        "This basic design can be used to create and train much larger models. I trained a large scale transformer model with 30 layers and around 400mm paramters on 8GPUs [here](https://github.com/novastar53/tiny_moe). We will cover that in our next post on this topic. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de0i2khwux",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "1. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). *Adaptive Mixtures of Local Experts*. Neural Computation.\n",
        "2. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. ICLR.\n",
        "3. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2021). *GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding*. ICLR.\n",
        "4. Fedus, W., Zoph, B., & Shazeer, N. (2022). *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. JMLR.\n",
        "5. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Le, Q. V. (2022). *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. ICML.\n",
        "6. Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., ... & Houlsby, N. (2021). *Scaling Vision with Sparse Mixture of Experts*. NeurIPS.\n",
        "7. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., ... & Sayed, W. E. (2024). *Mixtral of Experts*. arXiv."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jaxpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
