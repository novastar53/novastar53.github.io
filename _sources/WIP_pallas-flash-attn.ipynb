{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Write a Flash Attention Kernel in Pallas\n",
    "## Introduction\n",
    "\n",
    "- Link back to the softmax post (this builds on online softmax)\n",
    "- Why attention is a bottleneck: O(N²) memory for the attention matrix\n",
    "- Flash attention's promise: O(N) memory, same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Attention\n",
    "\n",
    "- Mathematical definition: `softmax(QK^T / √d) @ V`\n",
    "- Naive JAX implementation with einsum\n",
    "- The memory problem: for sequence length N, we materialize an N×N matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@jax.jit\n",
    "def mha_reference(q, k, v):\n",
    "    \"\"\"Reference multi-head attention: softmax(Q @ K^T / sqrt(d)) @ V\"\"\"\n",
    "    d = q.shape[-1]\n",
    "    scale = 1.0 / jnp.sqrt(d)\n",
    "    logits = jnp.einsum('bhqd,bhkd->bhqk', q, k) * scale\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    o = jnp.einsum('bhqk,bhkd->bhqd', probs, v)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Flash Attention Algorithm\n",
    "\n",
    "- Key insight: we never need the full attention matrix\n",
    "- Combine online softmax with output accumulation\n",
    "- Walk through the algorithm:\n",
    "  - Tile Q (outer parallel loop)\n",
    "  - Tile K, V (inner sequential loop)\n",
    "  - Maintain running max `m`, sum `l`, and output accumulator `o`\n",
    "  - Correction factor when max changes\n",
    "  - Final normalization\n",
    "- Python reference implementation (like your `online_softmax` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Kernel\n",
    "\n",
    "- BlockSpec design: Q tiled, K/V full sequence for inner loop\n",
    "- The kernel implementation\n",
    "- Storing logsumexp for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": "from functools import partial\nimport math\n\nfrom jax.experimental import pallas as pl\nfrom jax.experimental.pallas import triton as plgpu\n\nINTERPRET_MODE = True  # Set to False on GPU\n\nBLOCK_R = 64  # Block size for rows (Q blocks)\nBLOCK_C = 64  # Block size for columns (KV blocks)\nNUM_WARPS = 4\nNUM_STAGES = 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": "def flash_attention_fwd_kernel(q_ref, k_ref, v_ref, o_ref, logsumexp_ref, *, scale, num_k_blocks):\n    \"\"\"Flash attention forward kernel.\"\"\"\n    q_reg = plgpu.load(q_ref.at[0, :, :]).astype(jnp.float32)\n    o_reg = jnp.zeros(q_reg.shape, jnp.float32)\n    max_reg = jnp.full((BLOCK_R,), -jnp.inf, dtype=jnp.float32)\n    l_reg = jnp.zeros((BLOCK_R,), dtype=jnp.float32)\n    logsumexp_reg = jnp.zeros((BLOCK_R,), dtype=jnp.float32)\n\n    def body(t, args):\n        max_reg, l_reg, o_reg = args\n        idx = pl.dslice(t * BLOCK_C, BLOCK_C)\n        k_blk = plgpu.load(k_ref.at[0, idx, :]).astype(jnp.float32)\n        v_blk = plgpu.load(v_ref.at[0, idx, :]).astype(jnp.float32)\n        s_blk = pl.dot(q_reg, k_blk, trans_b=True) / scale\n        max_blk = jnp.maximum(max_reg, jnp.max(s_blk, axis=-1))\n        s_blk = jnp.exp(s_blk - max_blk[:, None])\n        l_blk = jnp.sum(s_blk, axis=-1)\n        o_blk = pl.dot(s_blk, v_blk)\n        return (max_blk, \n                l_reg * jnp.exp(max_reg - max_blk) + l_blk, \n                o_reg * jnp.exp(max_reg - max_blk)[:, None] + o_blk)\n\n    max_reg, l_reg, o_reg = jax.lax.fori_loop(0, num_k_blocks, body, (max_reg, l_reg, o_reg))\n    logsumexp_reg = max_reg + jnp.log(l_reg)\n    o_reg = o_reg / l_reg[:, None]\n    plgpu.store(o_ref.at[0, :, :], o_reg.astype(o_ref.dtype))\n    plgpu.store(logsumexp_ref.at[0, :], logsumexp_reg.astype(logsumexp_ref.dtype))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": "@jax.jit\ndef flash_attention_fwd(q, k, v):\n    \"\"\"Flash attention forward pass.\"\"\"\n    B, H, T, C = q.shape\n    B_flat = B*H\n    q_flat = q.reshape(-1, T, C)\n    k_flat = k.reshape(-1, T, C)\n    v_flat = v.reshape(-1, T, C)\n    scale = math.sqrt(C)\n    num_k_blocks = pl.cdiv(T, BLOCK_C)\n    grid = (B_flat, pl.cdiv(T, BLOCK_R))\n\n    out_flat, logsumexp = pl.pallas_call(\n        partial(flash_attention_fwd_kernel, scale=scale, num_k_blocks=num_k_blocks),\n        out_shape=[\n            jax.ShapeDtypeStruct(q_flat.shape, q_flat.dtype),\n            jax.ShapeDtypeStruct((B*H, T), q_flat.dtype)\n        ],\n        grid=grid,\n        in_specs=[\n            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),\n            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0))\n        ],\n        out_specs=[\n            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n            pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t))\n        ],\n        interpret=INTERPRET_MODE,\n        compiler_params=plgpu.CompilerParams(\n            num_warps=NUM_WARPS,\n            num_stages=NUM_STAGES\n        )\n    )(q_flat, k_flat, v_flat)\n    out = out_flat.reshape(q.shape)\n    logsumexp = logsumexp.reshape(B, H, T)\n    return out, logsumexp"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Performance Comparison\n\nWe compare our Pallas flash attention implementation against:\n1. **JAX cuDNN**: `jax.nn.dot_product_attention(implementation='cudnn')` - NVIDIA's highly optimized implementation\n2. **Reference (materialized)**: Standard attention that materializes the full N×N attention matrix\n\nNote: The cuDNN implementation requires a GPU with cuDNN installed and uses float16 for optimal performance. Set `INTERPRET_MODE = False` to run on GPU."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, *args, iters=10):\n",
    "    for _ in range(3):  # warmup\n",
    "        result = fn(*args)\n",
    "        if isinstance(result, tuple):\n",
    "            result[0].block_until_ready()\n",
    "        else:\n",
    "            result.block_until_ready()\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        result = fn(*args)\n",
    "        if isinstance(result, tuple):\n",
    "            result[0].block_until_ready()\n",
    "        else:\n",
    "            result.block_until_ready()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    return sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Performance benchmark (requires GPU with cuDNN)\n# Skip this cell if running on CPU\n\ndef benchmark_attention():\n    \"\"\"Benchmark attention implementations.\"\"\"\n    import time\n    \n    # Use float16 for cuDNN compatibility\n    B, H, T, D = 4, 8, 1024, 64\n    key = jax.random.key(42)\n    keys = jax.random.split(key, 4)\n    \n    q = jax.random.normal(keys[0], (B, H, T, D), dtype=jnp.float16)\n    k = jax.random.normal(keys[1], (B, H, T, D), dtype=jnp.float16)\n    v = jax.random.normal(keys[2], (B, H, T, D), dtype=jnp.float16)\n    do = jax.random.normal(keys[3], (B, H, T, D), dtype=jnp.float16)\n    \n    print(f\"Benchmark shape: B={B}, H={H}, T={T}, D={D}, dtype=float16\")\n    print(\"=\" * 60)\n    \n    def bench_fwd(fn, q, k, v, iters=20):\n        # Warmup\n        for _ in range(3):\n            out = fn(q, k, v)\n            jax.block_until_ready(out)\n        # Bench\n        times = []\n        for _ in range(iters):\n            t0 = time.perf_counter()\n            out = fn(q, k, v)\n            jax.block_until_ready(out)\n            times.append(time.perf_counter() - t0)\n        return sum(times) / len(times) * 1000  # ms\n\n    def bench_bwd(fn, q, k, v, do, iters=20):\n        # Warmup\n        for _ in range(3):\n            grads = jax.grad(lambda q, k, v: jnp.sum(fn(q, k, v) * do), argnums=(0, 1, 2))(q, k, v)\n            jax.block_until_ready(grads)\n        # Bench\n        times = []\n        for _ in range(iters):\n            t0 = time.perf_counter()\n            grads = jax.grad(lambda q, k, v: jnp.sum(fn(q, k, v) * do), argnums=(0, 1, 2))(q, k, v)\n            jax.block_until_ready(grads)\n            times.append(time.perf_counter() - t0)\n        return sum(times) / len(times) * 1000  # ms\n\n    # JAX cuDNN (requires GPU)\n    @jax.jit\n    def jax_cudnn_attention(q, k, v):\n        # Transpose from (B, H, T, D) to (B, T, H, D) for jax.nn.dot_product_attention\n        q_t = jnp.transpose(q, (0, 2, 1, 3))\n        k_t = jnp.transpose(k, (0, 2, 1, 3))\n        v_t = jnp.transpose(v, (0, 2, 1, 3))\n        out = jax.nn.dot_product_attention(q_t, k_t, v_t, implementation='cudnn')\n        return jnp.transpose(out, (0, 2, 1, 3))\n\n    # Our Pallas implementation\n    @jax.jit\n    def pallas_attention(q, k, v):\n        return flash_attention(q, k, v)\n\n    # Reference (materialized attention matrix)\n    @jax.jit \n    def reference_attention(q, k, v):\n        return mha_reference(q, k, v)\n\n    print(\"\\nForward pass:\")\n    try:\n        t_cudnn = bench_fwd(jax_cudnn_attention, q, k, v)\n        print(f\"  JAX cuDNN:              {t_cudnn:.3f} ms\")\n    except Exception as e:\n        print(f\"  JAX cuDNN:              N/A (cuDNN not available)\")\n        t_cudnn = None\n    \n    t_pallas = bench_fwd(pallas_attention, q, k, v)\n    print(f\"  Our Pallas:             {t_pallas:.3f} ms\")\n    \n    t_ref = bench_fwd(reference_attention, q, k, v)\n    print(f\"  Reference (materialized): {t_ref:.3f} ms\")\n    \n    if t_cudnn:\n        print(f\"\\n  Pallas vs cuDNN: {t_pallas/t_cudnn:.2f}x slower\")\n\n    print(\"\\nBackward pass:\")\n    try:\n        t_cudnn_bwd = bench_bwd(jax_cudnn_attention, q, k, v, do)\n        print(f\"  JAX cuDNN:              {t_cudnn_bwd:.3f} ms\")\n    except Exception as e:\n        print(f\"  JAX cuDNN:              N/A (cuDNN not available)\")\n        t_cudnn_bwd = None\n    \n    t_pallas_bwd = bench_bwd(pallas_attention, q, k, v, do)\n    print(f\"  Our Pallas:             {t_pallas_bwd:.3f} ms\")\n    \n    t_ref_bwd = bench_bwd(reference_attention, q, k, v, do)\n    print(f\"  Reference (materialized): {t_ref_bwd:.3f} ms\")\n    \n    if t_cudnn_bwd:\n        print(f\"\\n  Pallas vs cuDNN: {t_pallas_bwd/t_cudnn_bwd:.2f}x slower\")\n\n# Uncomment to run benchmark (requires GPU):\n# benchmark_attention()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example Results (RTX 4000 Ada)\n\nWhen run on an NVIDIA RTX 4000 Ada GPU, typical results are:\n\n```\nBenchmark shape: B=4, H=8, T=1024, D=64, dtype=float16\n============================================================\n\nForward pass:\n  JAX cuDNN:                0.368 ms\n  Our Pallas:               0.433 ms\n  Reference (materialized): 1.647 ms\n\n  Pallas vs cuDNN: 1.18x slower\n\nBackward pass:\n  JAX cuDNN:                3.230 ms\n  Our Pallas:               5.728 ms\n  Reference (materialized): 6.339 ms\n\n  Pallas vs cuDNN: 1.77x slower\n```\n\n**Key observations:**\n- Our forward pass is ~18% slower than cuDNN\n- Our backward pass is ~77% slower than cuDNN (due to 3 separate kernel launches)\n- Both are significantly faster than materializing the full attention matrix",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Backward Pass\n\nThe backward pass computes gradients dQ, dK, dV given the upstream gradient dO. The key insight is that we can recompute the attention weights P from the stored logsumexp values rather than storing them:\n\n$$P = \\exp(QK^T / \\sqrt{d} - \\text{logsumexp})$$\n\nWe use three separate kernels to avoid atomic operations:\n1. **Preprocess**: Compute $D = \\text{rowsum}(O \\odot dO)$ which is used in the softmax backward\n2. **dK/dV kernel**: Outer loop over KV blocks, inner loop over Q blocks\n3. **dQ kernel**: Outer loop over Q blocks, inner loop over KV blocks\n\nThe gradient formulas are:\n- $dP = dO \\cdot V^T$\n- $dS = P \\odot (dP - D) / \\sqrt{d}$ (softmax backward with scaling)\n- $dQ = dS \\cdot K$\n- $dK = dS^T \\cdot Q$  \n- $dV = P^T \\cdot dO$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": "# Kernel 1: Preprocess - compute D = rowsum(O * dO)\ndef flash_attention_bwd_preprocess_kernel(o_ref, do_ref, d_ref):\n    \"\"\"Compute D = rowsum(O * dO) for backward pass.\"\"\"\n    o_reg = plgpu.load(o_ref).astype(jnp.float32)\n    do_reg = plgpu.load(do_ref).astype(jnp.float32)\n    d_reg = jnp.sum(o_reg * do_reg, axis=-1)\n    plgpu.store(d_ref, d_reg.astype(d_ref.dtype))\n\n\ndef flash_attention_bwd_preprocess(o_flat, do_flat):\n    \"\"\"Preprocess for backward: compute D = rowsum(O * dO).\"\"\"\n    B_flat, T, C = o_flat.shape\n    grid = (B_flat, pl.cdiv(T, BLOCK_R))\n\n    d_flat = pl.pallas_call(\n        flash_attention_bwd_preprocess_kernel,\n        out_shape=jax.ShapeDtypeStruct((B_flat, T), jnp.float32),\n        grid=grid,\n        in_specs=[\n            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n        ],\n        out_specs=pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t)),\n        interpret=INTERPRET_MODE,\n        compiler_params=plgpu.CompilerParams(num_warps=NUM_WARPS, num_stages=NUM_STAGES)\n    )(o_flat, do_flat)\n    return d_flat\n\n\n# Kernel 2: dK/dV - outer loop over KV blocks, inner loop over Q blocks\ndef flash_attention_bwd_dkv_kernel(\n    q_ref, k_ref, v_ref, do_ref, logsumexp_ref, d_ref,\n    dk_ref, dv_ref,\n    *, scale, num_q_blocks\n):\n    \"\"\"Compute dK and dV gradients.\"\"\"\n    k_reg = plgpu.load(k_ref.at[0, :, :]).astype(jnp.float32)\n    v_reg = plgpu.load(v_ref.at[0, :, :]).astype(jnp.float32)\n\n    dk_acc = jnp.zeros(dk_ref.shape, dtype=jnp.float32)\n    dv_acc = jnp.zeros(dv_ref.shape, dtype=jnp.float32)\n\n    def body(t, carry):\n        dk_acc, dv_acc = carry\n        idx = pl.dslice(t * BLOCK_R, BLOCK_R)\n        q_blk = plgpu.load(q_ref.at[0, idx, :]).astype(jnp.float32)\n        do_blk = plgpu.load(do_ref.at[0, idx, :]).astype(jnp.float32)\n        logsumexp_blk = plgpu.load(logsumexp_ref.at[0, idx]).astype(jnp.float32)\n        d_blk = plgpu.load(d_ref.at[0, idx]).astype(jnp.float32)\n        # Recompute P = softmax(Q @ K^T / scale)\n        s_blk = pl.dot(q_blk, k_reg, trans_b=True) / scale\n        p_blk = jnp.exp(s_blk - logsumexp_blk[..., None])\n        # dP = dO @ V^T, dS = P * (dP - D) / scale\n        dp_blk = pl.dot(do_blk, v_reg, trans_b=True)\n        ds_blk = p_blk * (dp_blk - d_blk[..., None]) / scale\n        # Accumulate: dV += P^T @ dO, dK += dS^T @ Q\n        dv_acc += pl.dot(p_blk, do_blk, trans_a=True)\n        dk_acc += pl.dot(ds_blk, q_blk, trans_a=True)\n        return dk_acc, dv_acc\n        \n    dk_acc, dv_acc = jax.lax.fori_loop(0, num_q_blocks, body, (dk_acc, dv_acc))\n    plgpu.store(dk_ref, dk_acc.astype(dk_ref.dtype))\n    plgpu.store(dv_ref, dv_acc.astype(dv_ref.dtype))\n\n\ndef flash_attention_bwd_dkv(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale):\n    \"\"\"Compute dK and dV using pallas_call.\"\"\"\n    B_flat, T, C = q_flat.shape\n    num_q_blocks = pl.cdiv(T, BLOCK_R)\n    grid = (B_flat, pl.cdiv(T, BLOCK_C))\n\n    dk_flat, dv_flat = pl.pallas_call(\n        partial(flash_attention_bwd_dkv_kernel, scale=scale, num_q_blocks=num_q_blocks),\n        out_shape=[\n            jax.ShapeDtypeStruct(k_flat.shape, k_flat.dtype),\n            jax.ShapeDtypeStruct(v_flat.shape, v_flat.dtype),\n        ],\n        grid=grid,\n        in_specs=[\n            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # q (full)\n            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)), # k (blocked)\n            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)), # v (blocked)\n            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # do (full)\n            pl.BlockSpec((1, T), lambda b, _: (b, 0)),             # logsumexp (full)\n            pl.BlockSpec((1, T), lambda b, _: (b, 0)),             # d (full)\n        ],\n        out_specs=[\n            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)),\n            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)),\n        ],\n        interpret=INTERPRET_MODE,\n        compiler_params=plgpu.CompilerParams(num_warps=NUM_WARPS, num_stages=NUM_STAGES)\n    )(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat)\n    return dk_flat, dv_flat\n\n\n# Kernel 3: dQ - outer loop over Q blocks, inner loop over KV blocks\ndef flash_attention_bwd_dq_kernel(\n    q_ref, k_ref, v_ref, do_ref, logsumexp_ref, d_ref,\n    dq_ref,\n    *, scale, num_kv_blocks\n):\n    \"\"\"Compute dQ gradient.\"\"\"\n    q_reg = plgpu.load(q_ref.at[0, :, :]).astype(jnp.float32)\n    do_reg = plgpu.load(do_ref.at[0, :, :]).astype(jnp.float32)\n    logsumexp_reg = plgpu.load(logsumexp_ref.at[0, :]).astype(jnp.float32)\n    d_reg = plgpu.load(d_ref.at[0, :]).astype(jnp.float32)\n    dq_acc = jnp.zeros(dq_ref.shape, dtype=jnp.float32)\n\n    def body(t, carry):\n        dq_acc = carry\n        idx = pl.dslice(t * BLOCK_C, BLOCK_C)\n        k_blk = plgpu.load(k_ref.at[0, idx, :]).astype(jnp.float32)\n        v_blk = plgpu.load(v_ref.at[0, idx, :]).astype(jnp.float32)\n        # Recompute P\n        s_blk = pl.dot(q_reg, k_blk, trans_b=True) / scale\n        p_blk = jnp.exp(s_blk - logsumexp_reg[..., None])\n        # dP = dO @ V^T, dS = P * (dP - D) / scale\n        dp_blk = pl.dot(do_reg, v_blk, trans_b=True)\n        ds_blk = p_blk * (dp_blk - d_reg[..., None]) / scale\n        # Accumulate: dQ += dS @ K\n        dq_acc += pl.dot(ds_blk, k_blk)\n        return dq_acc\n\n    dq_acc = jax.lax.fori_loop(0, num_kv_blocks, body, dq_acc)\n    plgpu.store(dq_ref, dq_acc.astype(dq_ref.dtype))\n\n\ndef flash_attention_bwd_dq(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale):\n    \"\"\"Compute dQ using pallas_call.\"\"\"\n    B_flat, T, C = q_flat.shape\n    num_kv_blocks = pl.cdiv(T, BLOCK_C)\n    grid = (B_flat, pl.cdiv(T, BLOCK_R))\n\n    dq_flat = pl.pallas_call(\n        partial(flash_attention_bwd_dq_kernel, scale=scale, num_kv_blocks=num_kv_blocks),\n        out_shape=jax.ShapeDtypeStruct(q_flat.shape, q_flat.dtype),\n        grid=grid,\n        in_specs=[\n            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)), # q (blocked)\n            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # k (full)\n            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # v (full)\n            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)), # do (blocked)\n            pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t)),       # logsumexp (blocked)\n            pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t)),       # d (blocked)\n        ],\n        out_specs=pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n        interpret=INTERPRET_MODE,\n        compiler_params=plgpu.CompilerParams(num_warps=NUM_WARPS, num_stages=NUM_STAGES)\n    )(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat)\n    return dq_flat\n\n\n@jax.jit\ndef flash_attention_bwd(q, k, v, o, logsumexp, do):\n    \"\"\"Flash attention backward pass using 3 separate kernels.\"\"\"\n    B, H, T, C = q.shape\n    scale = math.sqrt(C)\n\n    # Flatten batch and head dimensions\n    q_flat = q.reshape(-1, T, C)\n    k_flat = k.reshape(-1, T, C)\n    v_flat = v.reshape(-1, T, C)\n    o_flat = o.reshape(-1, T, C)\n    do_flat = do.reshape(-1, T, C)\n    logsumexp_flat = logsumexp.reshape(-1, T)\n\n    # Kernel 1: Preprocess - compute D = rowsum(O * dO)\n    d_flat = flash_attention_bwd_preprocess(o_flat, do_flat)\n\n    # Kernel 2: Compute dK, dV\n    dk_flat, dv_flat = flash_attention_bwd_dkv(\n        q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale\n    )\n\n    # Kernel 3: Compute dQ\n    dq_flat = flash_attention_bwd_dq(\n        q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale\n    )\n\n    return (\n        dq_flat.reshape(q.shape),\n        dk_flat.reshape(k.shape),\n        dv_flat.reshape(v.shape),\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom VJP Integration\n",
    "\n",
    "- Wiring up forward and backward with `jax.custom_vjp`\n",
    "- The residuals needed (Q, K, V, O, logsumexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": "@jax.custom_vjp\ndef flash_attention(q, k, v):\n    \"\"\"Flash attention with custom backward pass.\"\"\"\n    o, _ = flash_attention_fwd(q, k, v)\n    return o\n\n\ndef flash_attention_fwd_rule(q, k, v):\n    \"\"\"Forward rule for custom_vjp.\n    \n    Returns the output and residuals needed for backward pass.\n    \"\"\"\n    o, logsumexp = flash_attention_fwd(q, k, v)\n    return o, (q, k, v, o, logsumexp)\n\n\ndef flash_attention_bwd_rule(res, do):\n    \"\"\"Backward rule for custom_vjp.\n    \n    Takes residuals from forward and upstream gradient dO,\n    returns gradients (dQ, dK, dV).\n    \"\"\"\n    q, k, v, o, logsumexp = res\n    dq, dk, dv = flash_attention_bwd(q, k, v, o, logsumexp, do)\n    return dq, dk, dv\n\n\nflash_attention.defvjp(flash_attention_fwd_rule, flash_attention_bwd_rule)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- Gradient correctness check against JAX autodiff\n",
    "- Train a small transformer or attention layer to verify end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference output shape: (2, 4, 256, 64)\n",
      "Flash attention output shape: (2, 4, 256, 64)\n",
      "Forward pass matches: True\n"
     ]
    }
   ],
   "source": [
    "B, H, T, D = 2, 4, 256, 64\n",
    "key = jax.random.key(0)\n",
    "keys = jax.random.split(key, 4)\n",
    "\n",
    "q = jax.random.normal(keys[0], (B, H, T, D), dtype=jnp.float32)\n",
    "k = jax.random.normal(keys[1], (B, H, T, D), dtype=jnp.float32)\n",
    "v = jax.random.normal(keys[2], (B, H, T, D), dtype=jnp.float32)\n",
    "do = jax.random.normal(keys[3], (B, H, T, D), dtype=jnp.float32)\n",
    "\n",
    "# Forward check\n",
    "o_ref = mha_reference(q, k, v)\n",
    "print(f\"Reference output shape: {o_ref.shape}\")\n",
    "\n",
    "o_flash = flash_attention(q, k, v)\n",
    "print(f\"Flash attention output shape: {o_flash.shape}\")\n",
    "print(f\"Forward pass matches: {jnp.allclose(o_flash, o_ref, atol=1e-2, rtol=1e-2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": "# Backward check (reference)\ndef loss_ref(q, k, v):\n    return jnp.sum(mha_reference(q, k, v) * do)\n\ndq_ref, dk_ref, dv_ref = jax.grad(loss_ref, argnums=(0, 1, 2))(q, k, v)\nprint(f\"Reference gradient shapes: dq={dq_ref.shape}, dk={dk_ref.shape}, dv={dv_ref.shape}\")\n\n# Flash attention backward pass\ndef loss_flash(q, k, v):\n    return jnp.sum(flash_attention(q, k, v) * do)\n\ndq_flash, dk_flash, dv_flash = jax.grad(loss_flash, argnums=(0, 1, 2))(q, k, v)\nprint(f\"Flash attention gradient shapes: dq={dq_flash.shape}, dk={dk_flash.shape}, dv={dv_flash.shape}\")\n\nprint(f\"dQ matches: {jnp.allclose(dq_flash, dq_ref, atol=1e-2, rtol=1e-2)}\")\nprint(f\"dK matches: {jnp.allclose(dk_flash, dk_ref, atol=1e-2, rtol=1e-2)}\")\nprint(f\"dV matches: {jnp.allclose(dv_flash, dv_ref, atol=1e-2, rtol=1e-2)}\")\n\n# Print max differences for debugging\nprint(f\"\\nMax differences:\")\nprint(f\"  dQ: {jnp.max(jnp.abs(dq_flash - dq_ref)):.6f}\")\nprint(f\"  dK: {jnp.max(jnp.abs(dk_flash - dk_ref)):.6f}\")\nprint(f\"  dV: {jnp.max(jnp.abs(dv_flash - dv_ref)):.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nWe've implemented a complete Flash Attention kernel in JAX Pallas with both forward and backward passes. The key ideas are:\n\n1. **Online softmax**: Computing softmax in tiles without materializing the full N×N attention matrix\n2. **Correction factors**: Rescaling partial results when the running maximum changes\n3. **Recomputation**: Storing only logsumexp and recomputing attention weights in the backward pass\n4. **Three backward kernels**: Separate passes for D (preprocess), dK/dV, and dQ to avoid atomic operations\n\nThe implementation achieves correctness and demonstrates the core Flash Attention algorithm clearly. While it doesn't match cuDNN performance, it serves as an excellent educational resource for understanding how memory-efficient attention works."
  },
  {
   "cell_type": "markdown",
   "source": "## Limitations and Future Work\n\n### Performance Gap\n\nOur Pallas implementation achieves correctness but runs approximately 1.5-2x slower than NVIDIA's cuDNN flash attention on the forward pass, and the gap widens on the backward pass. The primary reasons for this performance gap are:\n\n1. **Lack of warp-level tiling**: FlashAttention-2 uses sophisticated warp-level parallelism where different warps within a thread block handle different portions of the K/V matrices. This reduces shared memory traffic and improves tensor core utilization.\n\n2. **Three separate backward kernels**: Our implementation uses three kernel launches (preprocess, dK/dV, dQ) to avoid atomic operations. Production implementations fuse these more aggressively with careful synchronization.\n\n3. **No causal masking optimization**: Causal attention can skip computation for masked positions, but our implementation computes the full attention matrix.\n\n### Pallas Limitations\n\nPallas provides a high-level abstraction for writing GPU kernels, but it doesn't expose certain low-level primitives needed for peak performance:\n\n- **No warp-level programming**: Pallas doesn't provide access to `warp_id` or warp shuffle operations (`__shfl_sync`). You can configure `num_warps` but cannot coordinate work between warps within a block.\n\n- **Limited shared memory control**: Pallas manages shared memory implicitly through `BlockSpec`. You cannot explicitly allocate shared memory or control synchronization barriers.\n\n- **No atomic operations**: Pallas on GPU doesn't expose `atomic_add` or similar primitives, requiring separate kernels for reductions.\n\n### Path to Better Performance\n\nTo close the gap with cuDNN, you would need to:\n\n1. **Switch to Triton**: Triton provides more control over memory access patterns, explicit masking with `tl.where`, and better autotuning. However, even Triton abstracts away some warp-level primitives.\n\n2. **Use CUDA C++**: For full control over warp-level tiling, shared memory, and synchronization, CUDA C++ remains necessary. This is what cuDNN and the original FlashAttention implementations use.\n\n3. **Just use the built-in**: For production workloads, `jax.nn.dot_product_attention(implementation='cudnn')` is the pragmatic choice. It's highly optimized and well-tested.\n\n### Educational Value\n\nDespite the performance gap, this Pallas implementation has significant educational value:\n\n- **Algorithm clarity**: The tiled computation with online softmax correction is clearly visible in the code\n- **Gradient derivation**: The backward pass shows exactly how gradients flow through attention\n- **Pallas patterns**: Demonstrates `BlockSpec`, `fori_loop`, and `custom_vjp` integration\n- **Debugging**: `INTERPRET_MODE=True` allows stepping through the algorithm on CPU\n\nFor learning how flash attention works, this implementation is arguably better than optimized CUDA code where the algorithm is obscured by performance tricks.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## References\n\n**Papers:**\n- Dao, T., Fu, D., Ermon, S., Rudra, A., & Re, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*. https://arxiv.org/abs/2205.14135\n- Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. *arXiv preprint arXiv:2307.08691*. https://arxiv.org/abs/2307.08691\n\n**Reference Implementations:**\n- JAX Official Flash Attention (TPU): https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py\n- JAX Official Fused Attention (GPU): https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/gpu/attention.py\n- Umar Jamil's Triton Flash Attention: https://github.com/hkproj/triton-flash-attention\n\n**Documentation:**\n- JAX Pallas Documentation: https://jax.readthedocs.io/en/latest/pallas/\n- Triton Documentation: https://triton-lang.org/"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}