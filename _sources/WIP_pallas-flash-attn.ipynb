{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Write a Flash Attention Kernel in Pallas\n",
    "## Introduction\n",
    "\n",
    "- Link back to the softmax post (this builds on online softmax)\n",
    "- Why attention is a bottleneck: O(N²) memory for the attention matrix\n",
    "- Flash attention's promise: O(N) memory, same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Attention\n",
    "\n",
    "- Mathematical definition: `softmax(QK^T / √d) @ V`\n",
    "- Naive JAX implementation with einsum\n",
    "- The memory problem: for sequence length N, we materialize an N×N matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@jax.jit\n",
    "def mha_reference(q, k, v):\n",
    "    \"\"\"Reference multi-head attention: softmax(Q @ K^T / sqrt(d)) @ V\"\"\"\n",
    "    d = q.shape[-1]\n",
    "    scale = 1.0 / jnp.sqrt(d)\n",
    "    logits = jnp.einsum('bhqd,bhkd->bhqk', q, k) * scale\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    o = jnp.einsum('bhqk,bhkd->bhqd', probs, v)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Flash Attention Algorithm\n",
    "\n",
    "- Key insight: we never need the full attention matrix\n",
    "- Combine online softmax with output accumulation\n",
    "- Walk through the algorithm:\n",
    "  - Tile Q (outer parallel loop)\n",
    "  - Tile K, V (inner sequential loop)\n",
    "  - Maintain running max `m`, sum `l`, and output accumulator `o`\n",
    "  - Correction factor when max changes\n",
    "  - Final normalization\n",
    "- Python reference implementation (like your `online_softmax` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Kernel\n",
    "\n",
    "- BlockSpec design: Q tiled, K/V full sequence for inner loop\n",
    "- The kernel implementation\n",
    "- Storing logsumexp for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import triton as plgpu\n",
    "\n",
    "INTERPRET_MODE = True  # Set to False on GPU\n",
    "\n",
    "BLOCK_T = 64\n",
    "NUM_WARPS = 4\n",
    "NUM_STAGES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def flash_attention_fwd_kernel(q_ref, k_ref, v_ref, o_ref, logsumexp_ref, *, scale, num_k_blocks):\n",
    "    \"\"\"Flash attention forward kernel.\"\"\"\n",
    "    q_reg = plgpu.load(q_ref.at[0, :, :]).astype(jnp.float32)\n",
    "    o_reg = jnp.zeros(q_reg.shape, jnp.float32)\n",
    "    max_reg = jnp.full((BLOCK_T,), -jnp.inf, dtype=jnp.float32)\n",
    "    l_reg = jnp.zeros((BLOCK_T,), dtype=jnp.float32)\n",
    "\n",
    "    def body(t, args):\n",
    "        max_reg, l_reg, o_reg = args\n",
    "        idx = pl.dslice(t * BLOCK_T, BLOCK_T)\n",
    "        k_blk = plgpu.load(k_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        v_blk = plgpu.load(v_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        s_blk = pl.dot(q_reg, k_blk, trans_b=True) / scale\n",
    "        max_blk = jnp.maximum(max_reg, jnp.max(s_blk, axis=-1))\n",
    "        s_blk = jnp.exp(s_blk - max_blk[:, None])\n",
    "        l_blk = jnp.sum(s_blk, axis=-1)\n",
    "        o_blk = pl.dot(s_blk, v_blk)\n",
    "        return (max_blk, \n",
    "                l_reg * jnp.exp(max_reg - max_blk) + l_blk, \n",
    "                o_reg * jnp.exp(max_reg - max_blk)[:, None] + o_blk)\n",
    "\n",
    "    max_reg, l_reg, o_reg = jax.lax.fori_loop(0, num_k_blocks, body, (max_reg, l_reg, o_reg))\n",
    "    o_reg = o_reg / l_reg[:, None]\n",
    "    logsumexp_reg = max_reg + jnp.log(l_reg)\n",
    "    plgpu.store(o_ref.at[0, :, :], o_reg.astype(o_ref.dtype))\n",
    "    plgpu.store(logsumexp_ref.at[0, :], logsumexp_reg.astype(logsumexp_ref.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def flash_attention_fwd(q, k, v):\n",
    "    \"\"\"Flash attention forward pass.\"\"\"\n",
    "    B, H, T, C = q.shape\n",
    "    B_flat = B*H\n",
    "    q_flat = q.reshape(-1, T, C)\n",
    "    k_flat = k.reshape(-1, T, C)\n",
    "    v_flat = v.reshape(-1, T, C)\n",
    "    scale = math.sqrt(C)\n",
    "    num_k_blocks = pl.cdiv(T, BLOCK_T)\n",
    "\n",
    "    grid = (B_flat, pl.cdiv(T, BLOCK_T))\n",
    "\n",
    "    out_flat, logsumexp = pl.pallas_call(\n",
    "        partial(flash_attention_fwd_kernel, scale=scale, num_k_blocks=num_k_blocks),\n",
    "        out_shape=[\n",
    "            jax.ShapeDtypeStruct(q_flat.shape, q_flat.dtype),\n",
    "            jax.ShapeDtypeStruct((B*H, T), q_flat.dtype)\n",
    "        ],\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_T, C), lambda b, t: (b, t, 0)),\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0))\n",
    "        ],\n",
    "        out_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_T, C), lambda b, t: (b, t, 0)),\n",
    "            pl.BlockSpec((1, BLOCK_T), lambda b, t: (b, t))\n",
    "        ],\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES\n",
    "        )\n",
    "    )(q_flat, k_flat, v_flat)\n",
    "    out = out_flat.reshape(q.shape)\n",
    "    logsumexp = logsumexp.reshape(B, H, T)\n",
    "    return out, logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "- Benchmark against JAX's standard attention\n",
    "- Memory comparison (if possible to demonstrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, *args, iters=10):\n",
    "    for _ in range(3):  # warmup\n",
    "        result = fn(*args)\n",
    "        if isinstance(result, tuple):\n",
    "            result[0].block_until_ready()\n",
    "        else:\n",
    "            result.block_until_ready()\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        result = fn(*args)\n",
    "        if isinstance(result, tuple):\n",
    "            result[0].block_until_ready()\n",
    "        else:\n",
    "            result.block_until_ready()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    return sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "- Mathematical derivation of gradients dQ, dK, dV\n",
    "- The recomputation strategy: recompute attention weights from logsumexp\n",
    "- Two-pass approach: one for dK/dV, one for dQ (as in the reference)\n",
    "- Kernel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def flash_attention_bwd_kernel(q_ref, k_ref, v_ref, o_ref, do_ref, logsumexp_ref,\n",
    "                                dq_ref, dk_ref, dv_ref, *, num_kv_blocks, scale):\n",
    "    \"\"\"Flash attention backward kernel.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def flash_attention_bwd(q, k, v, o, logsumexp, do):\n",
    "    \"\"\"Flash attention backward pass.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom VJP Integration\n",
    "\n",
    "- Wiring up forward and backward with `jax.custom_vjp`\n",
    "- The residuals needed (Q, K, V, O, logsumexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "@jax.custom_vjp\n",
    "def flash_attention(q, k, v):\n",
    "    \"\"\"Flash attention with custom backward pass.\"\"\"\n",
    "    o, logsumexp = flash_attention_fwd(q, k, v)\n",
    "    return o\n",
    "\n",
    "\n",
    "def flash_attention_fwd_rule(q, k, v):\n",
    "    \"\"\"Forward rule for custom_vjp.\"\"\"\n",
    "    o, logsumexp = flash_attention_fwd(q, k, v)\n",
    "    return o, (q, k, v, o, logsumexp)\n",
    "\n",
    "\n",
    "def flash_attention_bwd_rule(res, do):\n",
    "    \"\"\"Backward rule for custom_vjp.\"\"\"\n",
    "    q, k, v, o, logsumexp = res\n",
    "    dq, dk, dv = flash_attention_bwd(q, k, v, o, logsumexp, do)\n",
    "    return dq, dk, dv\n",
    "\n",
    "\n",
    "flash_attention.defvjp(flash_attention_fwd_rule, flash_attention_bwd_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- Gradient correctness check against JAX autodiff\n",
    "- Train a small transformer or attention layer to verify end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference output shape: (2, 4, 256, 64)\n",
      "Flash attention output shape: (2, 4, 256, 64)\n",
      "Forward pass matches: True\n"
     ]
    }
   ],
   "source": [
    "B, H, T, D = 2, 4, 256, 64\n",
    "key = jax.random.key(0)\n",
    "keys = jax.random.split(key, 4)\n",
    "\n",
    "q = jax.random.normal(keys[0], (B, H, T, D), dtype=jnp.float32)\n",
    "k = jax.random.normal(keys[1], (B, H, T, D), dtype=jnp.float32)\n",
    "v = jax.random.normal(keys[2], (B, H, T, D), dtype=jnp.float32)\n",
    "do = jax.random.normal(keys[3], (B, H, T, D), dtype=jnp.float32)\n",
    "\n",
    "# Forward check\n",
    "o_ref = mha_reference(q, k, v)\n",
    "print(f\"Reference output shape: {o_ref.shape}\")\n",
    "\n",
    "o_flash = flash_attention(q, k, v)\n",
    "print(f\"Flash attention output shape: {o_flash.shape}\")\n",
    "print(f\"Forward pass matches: {jnp.allclose(o_flash, o_ref, atol=1e-2, rtol=1e-2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference gradient shapes: dq=(2, 4, 256, 64), dk=(2, 4, 256, 64), dv=(2, 4, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "# Backward check (reference)\n",
    "def loss_ref(q, k, v):\n",
    "    return jnp.sum(mha_reference(q, k, v) * do)\n",
    "\n",
    "dq_ref, dk_ref, dv_ref = jax.grad(loss_ref, argnums=(0, 1, 2))(q, k, v)\n",
    "print(f\"Reference gradient shapes: dq={dq_ref.shape}, dk={dk_ref.shape}, dv={dv_ref.shape}\")\n",
    "\n",
    "# TODO: Uncomment once backward pass is implemented\n",
    "# def loss_flash(q, k, v):\n",
    "#     return jnp.sum(flash_attention(q, k, v) * do)\n",
    "#\n",
    "# dq_flash, dk_flash, dv_flash = jax.grad(loss_flash, argnums=(0, 1, 2))(q, k, v)\n",
    "# print(f\"Flash attention gradient shapes: dq={dq_flash.shape}, dk={dk_flash.shape}, dv={dv_flash.shape}\")\n",
    "# print(f\"dQ matches: {jnp.allclose(dq_flash, dq_ref, atol=1e-2, rtol=1e-2)}\")\n",
    "# print(f\"dK matches: {jnp.allclose(dk_flash, dk_ref, atol=1e-2, rtol=1e-2)}\")\n",
    "# print(f\"dV matches: {jnp.allclose(dv_flash, dv_ref, atol=1e-2, rtol=1e-2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Recap of key ideas\n",
    "- Potential extensions: causal masking, multi-query attention\n",
    "- Link to further resources (FlashAttention paper, JAX Pallas docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. *arXiv preprint arXiv:2307.08691*. https://arxiv.org/abs/2307.08691"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
