{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a86036",
   "metadata": {},
   "source": [
    "# How to Write a Softmax Kernel in Pallas\n",
    "\n",
    "<div style=\"display:flex; gap:16px; align-items:center; justify-content:center;\">\n",
    "    <img src=\"_images/softmax_2d.png\" alt=\"softmax-2d\" style=\"width:66%; height:auto;\"/>\n",
    "</div>\n",
    "\n",
    "The softmax function is fundamental to neural network training because it converts raw model outputs (logits) into valid probability distributions. This is essential for classification tasks where we need to interpret network predictions as probabilities over discrete classes. Additionally, softmax is differentiable, allowing gradients to flow effectively through the network during training, which is why it's the standard choice for the output layer in multi-class classification models.\n",
    "\n",
    "\n",
    "In the [previous post](https://vikrampawar.com/pallas-matmul.html), we wrote a GPU kernel in Pallas for performing efficient matrix multiplication. In this post, we'll build on this by writing a GPU kernel for the softmax function. We will also write the backward pass and test it with a neural network training run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1062b",
   "metadata": {},
   "source": [
    "## Softmax Operation\n",
    "\n",
    "Given an input vector $z = (z_1, ..., z_n) \\in R^n$, the softmax function $σ : R^n → (0,1)^n$ produces a probability distribution over the n entries:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^n \\exp(z_j)} \\quad\\text{for } i=1,\\dots,n.\n",
    "$$\n",
    "\n",
    "Softmax is invariant to shifts: $σ(z) = σ(z + c)$ for any scalar c. \n",
    "For numerical stability one commonly uses\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{\\exp(z_i - \\max_j z_j)}{\\sum_{k=1}^n \\exp(z_k - \\max_j z_j)}.\n",
    "$$\n",
    "\n",
    "let's start with a naive implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a642097",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def manual_softmax(logits):\n",
    "    m = jnp.max(logits, axis=-1)         # 1\n",
    "    s = jnp.exp(logits - m[..., None])   # 2\n",
    "    l = jnp.sum(s, axis=-1)              # 3\n",
    "    return s / l[..., None]              # 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3acb1",
   "metadata": {},
   "source": [
    "Now imagine trying to implement this on a GPU. For computing line #1, the entire logits tensor will have to be loaded into the GPU cache. This will be extremely slow for large matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00557c1",
   "metadata": {},
   "source": [
    "\n",
    "## Online Softmax\n",
    "\n",
    "Since the max and sum operations are across columns, it's easy to tile across the rows. However, the column axis might still cause a bottleneck. \n",
    "Since we cannot parallelize the row operations, we can calculate the max (m) and normalizing factor (l) across each block of columns in a loop and use a trick to correct both as we process each block.\n",
    "\n",
    "```\n",
    "new_m = max(old_m, block_max)\n",
    "```\n",
    "\n",
    "```\n",
    "l = l * exp(old_m - new_m) + sum(exp(x_block - new_m))\n",
    "```\n",
    "Let's update our implementation to reflect our new algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b8f4b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "BLOCK_M = 64 # These can be tuned for your GPU\n",
    "BLOCK_N = 64\n",
    "\n",
    "# Online softmax\n",
    "def online_softmax(logits):\n",
    "    out = jnp.zeros_like(logits)\n",
    "    m = jnp.full((logits.shape[0],), -jnp.inf)\n",
    "    l = jnp.zeros((logits.shape[0],))\n",
    "    for i in range(0, logits.shape[0], BLOCK_M):  # This axis can be tiled in parallel blocks.\n",
    "        for j in range(0, logits.shape[1], BLOCK_N):  # This axis cannot be tiled in parallel, so it is tiled sequentially\n",
    "            block = logits[i:i+BLOCK_M, j:j+BLOCK_N] # Load a block\n",
    "            block_max = jnp.max(block, axis=-1) # Get the max across the block\n",
    "            curr_max = m[i:i+BLOCK_M] # Retrieve the previous computed max for the rows\n",
    "            new_max = jnp.maximum(curr_max, block_max) # Update the max for all the rows\n",
    "            m = m.at[i:i+BLOCK_M].set(new_max)  \n",
    "            l_block = l[i:i+BLOCK_M] # Get the denominator for the rows in the block\n",
    "            l_block = l_block * jnp.exp(curr_max - new_max) + jnp.sum( # Correct and update the denominator based on the current block\n",
    "                jnp.exp(block - new_max[:, None]), axis=-1\n",
    "            )\n",
    "            l = l.at[i:i+BLOCK_M].set(l_block)\n",
    "        for j in range(0, logits.shape[1], BLOCK_N):  # Loop over the column blocks and generate the output values \n",
    "            out_block = jnp.exp(logits[i:i+BLOCK_M, j:j+BLOCK_N] - m[i:i+BLOCK_M][:, None]) / l[i:i+BLOCK_M][:, None]\n",
    "            out = out.at[i:i+BLOCK_M, j:j+BLOCK_N].set(out_block)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea77d7a",
   "metadata": {},
   "source": [
    "The next step is to convert this into an efficient GPU kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44070d44",
   "metadata": {},
   "source": [
    "\n",
    "## Let's Implement the Forward Pass Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b046b7",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.pallas as pl\n",
    "from jax.experimental.pallas import triton as plgpu\n",
    "\n",
    "INTERPRET_MODE = False # Set to False on GPU\n",
    "NUM_WARPS = 4\n",
    "NUM_STAGES = 3\n",
    "\n",
    "def softmax_kernel(x_ref, out_ref, *, n_col_blocks, n_rows, n_cols):\n",
    "    max_reg = jnp.full((BLOCK_M,), -jnp.inf, dtype=jnp.float32) \n",
    "    l_reg = jnp.zeros((BLOCK_M,), dtype=jnp.float32) \n",
    "    row_ids = pl.program_id(0) * BLOCK_M + jnp.arange(BLOCK_M)\n",
    "    row_mask = row_ids < n_rows\n",
    "\n",
    "    def stats_body(t, args):\n",
    "        max_reg, l_reg = args\n",
    "        idx = pl.dslice(t * BLOCK_N, BLOCK_N)\n",
    "        col_ids = t * BLOCK_N + jnp.arange(BLOCK_N)\n",
    "        cols_mask = col_ids < n_cols\n",
    "        mask = row_mask[:, None] & cols_mask[None, :]\n",
    "\n",
    "        x_tile = plgpu.load(\n",
    "            x_ref.at[:, idx],\n",
    "            mask=mask,\n",
    "            other=-jnp.inf,\n",
    "        ).astype(jnp.float32)\n",
    "        max_tile = jnp.max(x_tile, axis=-1)\n",
    "        max_new = jnp.maximum(max_reg, max_tile)\n",
    "        l_update = l_reg * jnp.exp(max_reg - max_new) + jnp.sum(\n",
    "            jnp.exp(x_tile - max_new[:, None]), axis=-1\n",
    "        )\n",
    "        return max_new, l_update\n",
    "        \n",
    "    max_reg, l_reg = jax.lax.fori_loop(0, n_col_blocks, stats_body, (max_reg, l_reg))\n",
    "\n",
    "    def out_body(t, _):\n",
    "        idx = pl.dslice(t * BLOCK_N, BLOCK_N)\n",
    "        col_ids = t * BLOCK_N + jnp.arange(BLOCK_N)\n",
    "        cols_mask = col_ids < n_cols\n",
    "        mask = row_mask[:, None] & cols_mask[None, :]\n",
    "\n",
    "        x_tile = plgpu.load(\n",
    "            x_ref.at[:, idx],\n",
    "            mask=mask,\n",
    "            other=-jnp.inf,\n",
    "        ).astype(jnp.float32)\n",
    "        out_tile = jnp.exp(x_tile - max_reg[:, None]) / l_reg[:, None]\n",
    "        plgpu.store(out_ref.at[:, idx], out_tile.astype(jnp.float32), mask=mask)\n",
    "\n",
    "    _ = jax.lax.fori_loop(0, n_col_blocks, out_body, None)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def softmax(logits):\n",
    "    n_row_blocks = pl.cdiv(logits.shape[0], BLOCK_M)\n",
    "    n_col_blocks = pl.cdiv(logits.shape[1], BLOCK_N)\n",
    "    return pl.pallas_call(\n",
    "        partial(softmax_kernel, n_col_blocks=n_col_blocks, n_rows=logits.shape[0], n_cols=logits.shape[1]),\n",
    "        out_shape=jax.ShapeDtypeStruct(logits.shape, jnp.float32),\n",
    "        grid=(n_row_blocks,),\n",
    "        in_specs=[pl.BlockSpec((BLOCK_M, logits.shape[1]), lambda i: (i, 0))],\n",
    "        out_specs=pl.BlockSpec((BLOCK_M, logits.shape[1]), lambda i: (i, 0)),\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES,\n",
    "        ),\n",
    "    )(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cad5e1",
   "metadata": {},
   "source": [
    "\n",
    "## Performance\n",
    "\n",
    "Let's compare our performance with the out-of-the-box softmax implementation provided by Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4057a8",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 10:44:50.336112    3714 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0107 10:44:50.340067    3436 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Softmax: 0.30 ms\n",
      "Pallas Softmax: 0.33 ms\n",
      "Speedup (jax / pallas): 0.93x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, *args, iters=10):\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        out = fn(*args)\n",
    "        out.block_until_ready()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    times.sort()\n",
    "    return times[len(times) // 2]\n",
    "\n",
    "\n",
    "d = 1024\n",
    "key = jax.random.key(0)\n",
    "logits = jax.random.normal(shape=(d, d), key=key)\n",
    "\n",
    "out_jax = jax.nn.softmax(logits)\n",
    "out_online = online_softmax(logits)\n",
    "out_pallas = softmax(logits)\n",
    "\n",
    "assert jnp.allclose(jnp.squeeze(out_jax), out_pallas)\n",
    "assert jnp.allclose(jnp.squeeze(out_jax), out_online)\n",
    "\n",
    "softmax_jit = jax.jit(jax.nn.softmax)\n",
    "\n",
    "_ = softmax_jit(logits).block_until_ready()\n",
    "_ = softmax(logits).block_until_ready()\n",
    "\n",
    "t_jax = bench(softmax_jit, logits)\n",
    "t_pallas = bench(softmax, logits)\n",
    "\n",
    "print(f\"Jax Softmax: {t_jax*1e3:.2f} ms\")\n",
    "print(f\"Pallas Softmax: {t_pallas*1e3:.2f} ms\")\n",
    "print(f\"Speedup (jax / pallas): {t_jax / t_pallas:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33ef43",
   "metadata": {},
   "source": [
    "Our softmax kernel is around 7% slower than the Jax implementation. This is not surprising considering that Jax uses Nvidia's CUDA kernels under the hood, which are highly tuned. Now, let's implement the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3c7f2",
   "metadata": {},
   "source": [
    "\n",
    "## Let's Implement the Backward Pass Kernel\n",
    "\n",
    "Let's first derive the expression for the backward pass.\n",
    "\n",
    "Let $x\\in\\mathbb{R}^n, y=\\mathrm{softmax}(x)$ with\n",
    "$y_i=\\frac{e^{x_i}}{\\sum_{k=1}^n e^{x_k}}.$\n",
    "Assume an upstream gradient $g=\\frac{\\partial L}{\\partial y}$ is given, and we want $\\frac{\\partial L}{\\partial x}$.\n",
    "\n",
    "First compute the Jacobian of softmax. Let $S=\\sum_k e^{x_k}$, so $y_i=e^{x_i}/S$.\n",
    "\n",
    "Differentiate $y_i$ w.r.t. $x_j$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j}\n",
    "=\\frac{\\partial}{\\partial x_j}\\left(\\frac{e^{x_i}}{S}\\right)\n",
    "=\\frac{\\delta_{ij}e^{x_i}\\,S - e^{x_i}\\,\\frac{\\partial S}{\\partial x_j}}{S^2}$$\n",
    "\n",
    "But $\\frac{\\partial S}{\\partial x_j}=e^{x_j}$\n",
    "Substitute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j}\n",
    "=\\frac{\\delta_{ij}e^{x_i}S - e^{x_i}e^{x_j}}{S^2}\n",
    "=\\delta_{ij}\\frac{e^{x_i}}{S} - \\frac{e^{x_i}}{S}\\frac{e^{x_j}}{S}\n",
    "=\\delta_{ij}y_i - y_i y_j\n",
    "$$\n",
    "\n",
    "\n",
    "Now apply the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j}\n",
    "=\\sum_{i=1}^n \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial x_j}\n",
    "=\\sum_i g_i\\left(\\delta_{ij}y_i - y_i y_j\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= g_j y_j - y_j\\sum_i g_i y_i\n",
    "= y_j\\left(g_j - \\sum_i g_i y_i\\right)\n",
    "$$\n",
    "\n",
    "Finally, in vector form:\n",
    "\n",
    "$$\n",
    "\\;\\frac{\\partial L}{\\partial x} = y \\odot (g - \\langle g, y\\rangle)\\;\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e804ae9",
   "metadata": {},
   "source": [
    "The kernel for the backward pass can be implemented in two steps. First, we can compute the inner product $ \\langle g, y\\rangle $, then an elementwise operation to compute the final expression. Since this is a binary classifier, both the upstream gradient $g$ and the output $y$ will be of shape (B, C) where B is the batch size and C is the number of classes. Since C = 2, we only need to tile our kernel along the B axis, simplifying our implementation greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c430de14",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def softmax_backward_kernel(y_ref, dy_ref, dx_ref):\n",
    "    # compute the inner product <g_ref, y_ref>\n",
    "    dy_reg = plgpu.load(dy_ref)\n",
    "    y_reg = plgpu.load(y_ref)\n",
    "    g_dot_y = jnp.sum(dy_reg * y_reg, axis=1)\n",
    "\n",
    "    # Compute the output block\n",
    "    output_reg = y_reg * ( dy_reg - g_dot_y[:, None] )\n",
    "    plgpu.store(dx_ref, output_reg)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def softmax_backward(y, dy):\n",
    "    M, N = y.shape\n",
    "\n",
    "    grid = (pl.cdiv(M, BLOCK_M),)\n",
    "    out_shape = jax.ShapeDtypeStruct((M, N), y.dtype)\n",
    "\n",
    "    return pl.pallas_call(\n",
    "        softmax_backward_kernel,\n",
    "        out_shape=out_shape,\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((BLOCK_M, N), lambda i: (i, 0)),  # y\n",
    "            pl.BlockSpec((BLOCK_M, N), lambda i: (i, 0)),  # dy \n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((BLOCK_M, N), lambda i: (i, 0)),  # dx\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES,\n",
    "        ),\n",
    "    )(y, dy)\n",
    "\n",
    "\n",
    "@jax.custom_vjp\n",
    "def softmax_pallas(x):\n",
    "    return softmax(x)\n",
    "\n",
    "\n",
    "def softmax_fwd(x):\n",
    "    y = softmax(x)\n",
    "    return y, y\n",
    "\n",
    "\n",
    "def softmax_bwd(saved_y, dy):\n",
    "    (y,) = (saved_y,)\n",
    "    dx = softmax_backward(y, dy)\n",
    "    return (dx,)\n",
    "\n",
    "\n",
    "softmax_pallas.defvjp(softmax_fwd, softmax_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c84ce",
   "metadata": {},
   "source": [
    "\n",
    "## Let's Evaluate our Kernel\n",
    "\n",
    "We will attempt to train a binary classifier model on some synthetic data. Let's start by generating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fca27ab",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "B, E = 256, 24 # (batch size, number of features)\n",
    "x = jax.random.normal(jax.random.key(1000), (B, E))\n",
    "class_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "y = class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec04b83",
   "metadata": {},
   "source": [
    "Next, let's define our binary classifier and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac92b8c8",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import flax.nnx as nnx\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    in_dim: int\n",
    "    hidden_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "class Model(nnx.Module):\n",
    "    def __init__(self, config: ModelConfig, rngs: nnx.Rngs):\n",
    "        self.config = config\n",
    "        self.l1 = nnx.Linear(config.in_dim, config.hidden_dim, rngs=rngs)\n",
    "        self.l2 = nnx.Linear(config.hidden_dim, config.out_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        x = self.l1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    probs = softmax_pallas(logits)\n",
    "    labels = y.reshape(-1)\n",
    "    one_hot = jax.nn.one_hot(labels, probs.shape[-1], dtype=probs.dtype)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * jnp.log(probs + 1e-9), axis=-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d8e26",
   "metadata": {},
   "source": [
    "Before we train the model, let's first test if our backward pass kernel is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e1116b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def loss_from_logits_pallas(logits, y):\n",
    "    probs = softmax_pallas(logits)\n",
    "    labels = y.reshape(-1)\n",
    "    one_hot = jax.nn.one_hot(labels, probs.shape[-1], dtype=probs.dtype)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * jnp.log(probs + 1e-9), axis=-1))\n",
    "    return loss\n",
    "\n",
    "def loss_from_logits_gt(logits, y):\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    labels = y.reshape(-1)\n",
    "    one_hot = jax.nn.one_hot(labels, probs.shape[-1], dtype=probs.dtype)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * jnp.log(probs + 1e-9), axis=-1))\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def verify(model, x, y):\n",
    "    logits = model(x)\n",
    "    d_logits_pallas = jax.grad(loss_from_logits_pallas)(logits, y)\n",
    "    d_logits_gt = jax.grad(loss_from_logits_gt)(logits, y)\n",
    "    return jnp.allclose(d_logits_pallas, d_logits_gt)\n",
    "\n",
    "\n",
    "default = jax.random.key(69)\n",
    "rngs = nnx.Rngs(default=default) \n",
    "\n",
    "config = ModelConfig(in_dim=E, hidden_dim=E * 4, out_dim=2)\n",
    "model = Model(config, rngs)\n",
    "model.train()\n",
    "\n",
    "print(verify(model, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1764a",
   "metadata": {},
   "source": [
    "Excellent! Looks like our backward pass kernel works correctly. Finally, let's overfit the model on our toy dataset using our softmax kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d2d606",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss=0.7163699865341187\n",
      "iter 1: loss=0.578166127204895\n",
      "iter 2: loss=1.0504496097564697\n",
      "iter 3: loss=0.13622123003005981\n",
      "iter 4: loss=0.3550783097743988\n",
      "iter 5: loss=0.1289454996585846\n",
      "iter 6: loss=0.06486515700817108\n",
      "iter 7: loss=0.056444257497787476\n",
      "iter 8: loss=0.05389563366770744\n",
      "iter 9: loss=0.03236997872591019\n",
      "iter 10: loss=0.016692688688635826\n",
      "iter 11: loss=0.0100052859634161\n",
      "iter 12: loss=0.003909544087946415\n",
      "iter 13: loss=0.0019604917615652084\n",
      "iter 14: loss=0.0014198491116985679\n"
     ]
    }
   ],
   "source": [
    "import optax \n",
    "\n",
    "@nnx.jit\n",
    "def step(model, state, x, y):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, x, y)\n",
    "    state.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "tx = optax.adam(1e-1)\n",
    "state = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "\n",
    "iters = 15\n",
    "for i in range(iters):\n",
    "    loss = step(model, state, x, y)\n",
    "    print(f\"iter {i}: loss={loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ab243",
   "metadata": {},
   "source": [
    "We were able to successfully overfit our toy dataset using our softmax implementation. In the next one, we'll build on this and implement a custom Pallas kernel for computing self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3793b",
   "metadata": {},
   "source": [
    "<script src=\"https://utteranc.es/client.js\"\n",
    "        repo=\"novastar53/novastar53.github.io\"\n",
    "        issue-term=\"pathname\"\n",
    "        theme=\"github-light\"\n",
    "        crossorigin=\"anonymous\"\n",
    "        async>\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6f22f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
