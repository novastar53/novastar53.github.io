{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Positional Embeddings Work? Part 2\n",
    "\n",
    "\n",
    "The attention mechanism computes dot products between projections of embeddings. \n",
    "\n",
    "$$\n",
    "\\sum_i W_i cos(\\theta_{t + k} - \\theta{t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 15\n",
    "embedding_length = 9\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def pos(t, i):\n",
    "\n",
    "    x = t / (100.0**(i/embedding_length))\n",
    "    return x, jnp.where(i%2 == 0, jnp.sin(x), jnp.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "def generate_embeddings(rng, shape=(sequence_length, embedding_length), min_val=-0.1, max_val=0.1):\n",
    "    embeddings = jax.random.uniform(rng, shape, minval=min_val, maxval=max_val)\n",
    "    return embeddings\n",
    "\n",
    "sem_embeddings = generate_embeddings(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def generate_pos_embeddings(shape=(sequence_length, embedding_length)):\n",
    "\n",
    "    embeddings = jnp.empty(shape)\n",
    "    for row in range(shape[0]):\n",
    "        for col in range(shape[1]):\n",
    "            embeddings = embeddings.at[(row,col)].set(pos(row,col)[1])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "pos_embeddings = generate_pos_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_samples, dataset):\n",
    "\n",
    "    i_values = jax.random.randint(rng, (num_samples,), 0, 10)\n",
    "    j_values = i_values + jax.random.randint(rng + 1, (num_samples,), 0, 10 - i_values)\n",
    "    j_values = jnp.clip(j_values, 0, 9)\n",
    "\n",
    "    emb_i = dataset[i_values,:]\n",
    "    emb_j = dataset[j_values, :]\n",
    "\n",
    "    diff = j_values - i_values\n",
    "\n",
    "    return jnp.stack([emb_i, emb_j], axis=1), diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul input operand 0 must have ndim at least 1, but it has ndim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(x.shape)\n\u001b[32m     24\u001b[39m attnblk = AttnBlock(embedding_length, rngs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m x = \u001b[43mattnblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mAttnBlock.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m v = \u001b[38;5;28mself\u001b[39m.v(x)\n\u001b[32m     15\u001b[39m att = q @ k / jnp.sqrt(x.shape[-\u001b[32m1\u001b[39m]) \u001b[38;5;66;03m# B x T x T \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m x = \u001b[43matt\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m \u001b[38;5;66;03m# B x T x E\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:579\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    577\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[31m[... skipping hidden 15 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vikrampawar.com/.venv/lib/python3.12/site-packages/jax/_src/numpy/tensor_contractions.py:192\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, precision, preferred_element_type)\u001b[39m\n\u001b[32m    189\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m np.ndim(x) < \u001b[32m1\u001b[39m:\n\u001b[32m    190\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatmul input operand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must have ndim at least 1, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut it has ndim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.ndim(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preferred_element_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    194\u001b[39m   preferred_element_type, output_weak_type = dtypes.result_type(a, b, return_weak_type_flag=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: matmul input operand 0 must have ndim at least 1, but it has ndim 0"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import flax.nnx as nnx\n",
    "\n",
    "class AttnBlock(nnx.Module):\n",
    "    def __init__(self, h: int, rngs: nnx.Rngs):\n",
    "        self.q = nnx.Linear(h, h, rngs=rngs)\n",
    "        self.k = nnx.Linear(h, h, rngs=rngs)\n",
    "        self.v = nnx.Linear(h, h, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        q = self.q(x) # B x T x E\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        att = q @ k / jnp.sqrt(x.shape[-1]) # B x T x T \n",
    "        x = att @ v # B x T x E\n",
    "        return x\n",
    "\n",
    "k = jax.random.key(42)\n",
    "rngs = nnx.Rngs(k)\n",
    "\n",
    "x = pos_embeddings[:3, :]\n",
    "attnblk = AttnBlock(embedding_length, rngs)\n",
    "x = attnblk(x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 11],\n",
      "        [22, 20]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "src = torch.tensor([[10, 11, 12, 13],\n",
    "                    [20, 21, 22, 23]])\n",
    "# shape = (2 rows, 4 columns)\n",
    "\n",
    "idx = torch.tensor([[3, 1],   # for output row 0, cols → [src[0,3], src[0,1]]\n",
    "                    [2, 0]])  # for output row 1, cols → [src[1,2], src[1,0]]\n",
    "# shape = (2 rows, 2 “gathered” columns)\n",
    "\n",
    "out = torch.gather(src, dim=1, index=idx)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
