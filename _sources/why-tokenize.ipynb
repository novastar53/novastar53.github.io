{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Tokenize?\n",
    "\n",
    "What if we split tokens into words? \n",
    "\n",
    "What if we split tokens into characters or bytes? \n",
    "\n",
    "If we split tokens too much, the sequence length increases, slowing down training. \n",
    "Hence we want to compress the sequence as much as possible while preserving the meaning of the text.\n",
    "\n",
    "If we split tokens too little, then the model might not see enough rare words. \n",
    "\n",
    "Ideally, we want to keep commonly seen words and split up rare words.\n",
    "\n",
    "## Old School Tokenizers\n",
    "\n",
    "Old-School Tokenizers (NLTK, spaCy, etc.)\n",
    "\n",
    "These tokenizers follow linguistic rules or heuristics to split text into words, subwords, or sentences. They rely on whitespace, punctuation, and grammar rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "The first modern tokenizer used in BERT. Discarded all the old rule-based and linguistic methods for a statistical approach.\n",
    "\n",
    "1. Pre-tokenization using whitespace.\n",
    "2. Iteratively merge the most frequently byte-pairs in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece\n",
    "\n",
    "Similar to BPE, but uses a maximum likelihood approach to merge byte-pairs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentencepiecea\n",
    "\n",
    "Works directly on raw text (no need for pre-tokenization). Can include spaces in tokens, which helps for Asian languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
